{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Posts","text":""},{"location":"#posts","title":"Posts","text":""},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tag:frc","title":"FRC","text":"<ul> <li>            Congrats to 6399 for winning Engineering Inspiration Award at 2024 World Robot Contest Championships Beijing          </li> <li>            Congrats to 8011 for winning Championship at 2020 WE RoboStar League          </li> <li>            Congrats to 8011 for winning Rookie Game Changer Award at 2021 INFINITE RECHARGE At Home Challenge          </li> <li>            Congrats to 8214 for being Division Finalists Captain and winning Industrial Design Award at 2025 Galileo Division          </li> <li>            Congrats to 8811 for winning 3rd Prize at 2023 Indiemicro Robotics Competition Exchange Event          </li> <li>            Congrats to both 9635 and 8214 for advancing to 2025 FIRST Championship          </li> </ul>"},{"location":"tags/#tag:bilibili","title":"bilibili","text":"<ul> <li>            EKF, UKF &amp; Partical Filter          </li> <li>            Factor Graph for Pose Estimation          </li> <li>            Java \u9762\u5411\u5bf9\u8c61\u8fdc\u5f81          </li> <li>            KKT Conditions and Optimization Techniques          </li> <li>            Kalman Filter in 3 Ways          </li> <li>            Learn Like a Kindergartener          </li> <li>            Limit-memory BFGS Method          </li> <li>            Linear Quadratic Regulator in 3 Ways          </li> <li>            PHR Conic Augmented Lagrangian Method          </li> <li>            RL as an Adaptive Optimal Control          </li> <li>            The Duality and the Failure of LQG Control          </li> </ul>"},{"location":"tags/#tag:control","title":"control","text":"<ul> <li>            Classic Control Quick Notes          </li> <li>            CoTiMo Planner Report          </li> <li>            Linear Quadratic Regulator          </li> <li>            Linear Quadratic Regulator in 3 Ways          </li> <li>            The Duality and the Failure of LQG Control          </li> <li>            \u4ece\u300a\u7cfb\u7edf\u4e4b\u7f8e\u300b\u5230\u7ecf\u5178\u63a7\u5236\u8bba          </li> </ul>"},{"location":"tags/#tag:estimation","title":"estimation","text":"<ul> <li>            EKF, UKF &amp; Partical Filter          </li> <li>            Kalman Filter in 3 Ways          </li> <li>            Kalman Filter in 3 Ways          </li> <li>            Optimal Estimation Theory          </li> </ul>"},{"location":"tags/#tag:hkust","title":"hkust","text":"<ul> <li>            Classic Control Quick Notes          </li> <li>            Discrete Math Quick Notes          </li> <li>            Kalman Filter in 3 Ways          </li> <li>            Linear Algebra &amp; System Theory          </li> <li>            Linear Quadratic Regulator          </li> <li>            Optimal Estimation Theory          </li> </ul>"},{"location":"tags/#tag:java","title":"java","text":"<ul> <li>            Java \u9762\u5411\u5bf9\u8c61\u8fdc\u5f81          </li> </ul>"},{"location":"tags/#tag:math","title":"math","text":"<ul> <li>            Derivation of On-Manifold IMU Preintegration          </li> <li>            Discrete Math Quick Notes          </li> <li>            Linear Algebra &amp; System Theory          </li> <li>            Rotation Representations in Robotics          </li> <li>            Sparsity Extended Information Filter SLAM          </li> <li>            \u4ece\u300a\u7cfb\u7edf\u4e4b\u7f8e\u300b\u5230\u7ecf\u5178\u63a7\u5236\u8bba          </li> </ul>"},{"location":"tags/#tag:oop","title":"oop","text":"<ul> <li>            Java \u9762\u5411\u5bf9\u8c61\u8fdc\u5f81          </li> </ul>"},{"location":"tags/#tag:optimization","title":"optimization","text":"<ul> <li>            KKT Conditions and Optimization Techniques          </li> <li>            Limit-memory BFGS Method          </li> <li>            PHR Conic Augmented Lagrangian Method          </li> </ul>"},{"location":"tags/#tag:rl","title":"rl","text":"<ul> <li>            RL as an Adaptive Optimal Control          </li> </ul>"},{"location":"tags/#tag:robotics","title":"robotics","text":"<ul> <li>            Rotation Representations in Robotics          </li> </ul>"},{"location":"tags/#tag:slam","title":"slam","text":"<ul> <li>            A Continuous-time VINS          </li> <li>            Classic Visual Feature Descriptors          </li> <li>            Derivation of On-Manifold IMU Preintegration          </li> <li>            Factor Graph for Pose Estimation          </li> <li>            Handling Gauge Freedom in VINS          </li> <li>            Robust Initialization of VINS          </li> <li>            Sparsity Extended Information Filter SLAM          </li> </ul>"},{"location":"2024/12/01/discrete-math-quick-notes/","title":"Discrete Math Quick Notes","text":"","tags":["hkust","math"]},{"location":"2024/12/01/discrete-math-quick-notes/#discrete-math-quick-notes","title":"Discrete Math Quick Notes","text":"<p>This is the review notes for key concepts and theorems in discrete mathematics, particularly focusing on modular arithmetic, divisibility, and the properties of integers.</p>","tags":["hkust","math"]},{"location":"2024/12/01/discrete-math-quick-notes/#modular-notation","title":"Modular Notation","text":"","tags":["hkust","math"]},{"location":"2024/12/01/discrete-math-quick-notes/#divisibility-notation","title":"Divisibility Notation","text":"<ul> <li>\\(3\\mid12\\) (3 divides 12)</li> <li>\\(3\\nmid11\\) (3 does not divide 11)</li> </ul>","tags":["hkust","math"]},{"location":"2024/12/01/discrete-math-quick-notes/#euclids-division-theorem","title":"Euclid\u2019s Division Theorem","text":"<p>For integers \\(a\\) and \\(d\\):</p> \\[ a=dq+r \\] Note Name Note Name Note Name Note Name \\(d\\) divisor \\(a\\) dividend \\(q\\) quotient \\(r\\) remainder","tags":["hkust","math"]},{"location":"2024/12/01/discrete-math-quick-notes/#quotient-ring","title":"Quotient Ring","text":"<p>The set of integers modulo \\(m\\) is defined as:</p> \\[ {\\mathbf Z}_m=\\set{0,1,\\cdots,m-1} \\] <p>Definition</p> <p>For any \\(a,b\\in\\mathbf Z_m\\):</p> \\[ \\forall a,b\\in{\\mathbf Z}_m \\] \\[ a+_mb=(a+b)\\bmod{m} \\] \\[ a\\cdot_mb=(a\\cdot b)\\bmod m \\] <p>Properties</p> <ul> <li>Closure: \\(\\forall a,b\\in{\\mathbf Z}_m, a+_mb\\in{\\mathbf Z}_m,a\\cdot_mb\\in{\\mathbf Z}_m\\)</li> <li>Associativity</li> <li>\\((a+_mb)+_mc=a+_m(b+_mc)\\)</li> <li>\\((a\\cdot_mb)\\cdot_mc=a\\cdot_m(b\\cdot_mc)\\)</li> <li>Commutativity:</li> <li>\\(a+_mb=b+_ma\\)</li> <li>\\(a\\cdot_mb=b\\cdot_ma\\)</li> <li>Distributivity</li> <li>\\(a\\cdot_m(b+_mc)=(a\\cdot_mb)+_m(a\\cdot_mc)\\)</li> <li>Inverse: \\(\\forall a\\in{\\mathbf Z}_m,\\exists!b\\in{\\mathbf Z}_m,a+_mb=0,a\\cdot_mb=1\\)</li> </ul>","tags":["hkust","math"]},{"location":"2024/12/01/discrete-math-quick-notes/#congruences","title":"Congruences","text":"<p>If</p> \\[ a\\equiv b\\bmod m, c\\in \\bf Z \\] <p>then:</p> \\[ a+c\\equiv b+c\\bmod m \\] \\[ ac\\equiv bc\\bmod m \\]","tags":["hkust","math"]},{"location":"2024/12/01/discrete-math-quick-notes/#greatest-common-divisor","title":"Greatest Common Divisor","text":"<p>The greatest common divisor is defined as:</p> \\[ \\gcd(a,b)=\\max(\\set{d\\in{\\mathbf Z} |\\ d\\mid a, d\\mid b}) \\]","tags":["hkust","math"]},{"location":"2024/12/01/discrete-math-quick-notes/#euclidean-algorithm","title":"Euclidean Algorithm","text":"<p>The process is as follows:</p> \\[ a=bq+r \\] <p>Then:</p> \\[ \\gcd(a,b)=\\gcd(b,r) \\]","tags":["hkust","math"]},{"location":"2024/12/01/discrete-math-quick-notes/#bezouts-theorem","title":"B\u00e9zout\u2019s Theorem","text":"<p>For all \\(a,b\\in\\mathbf Z^+\\), there exist integers \\(s\\) and \\(t\\) such that:</p> \\[ \\gcd(a,b)=sa+tb \\]","tags":["hkust","math"]},{"location":"2024/12/01/discrete-math-quick-notes/#extended-euclidean-algorithm","title":"Extended Euclidean Algorithm","text":"<p>Example: express \\(\\gcd(123,111)\\) as a linear combination of \\(123\\) and \\(111\\)</p> <p>Step 1: find <code>gcd()</code></p> \\[123=1\\cdot111+12\\] \\[111=9\\cdot12+3\\] \\[12=4\\cdot3\\] \\[\\gcd(123,111)=3\\] <p>Step 2: Rewrite</p> \\[12=123-1\\cdot111\\] \\[3=111-9\\cdot12\\] <p>Step 3: Substitute</p> \\[\\begin{aligned}3&amp;=111-9\\cdot12\\\\&amp;=111-9\\cdot(123-1\\cdot11)\\\\&amp;=-9\\cdot123+10\\cdot111\\end{aligned}\\]","tags":["hkust","math"]},{"location":"2024/12/01/discrete-math-quick-notes/#multiplicative-inverses","title":"Multiplicative Inverses","text":"<p>For all \\(a\\in\\mathbf Z_m,m&gt;1\\) if \\(\\gcd(a,m)=1\\), then:</p> \\[ \\exists! b,ab\\equiv1\\bmod m \\] <p>Example: \\(a=4,m=9\\)</p> <p>Step 1: Linear Combination</p> \\[1=9-2\\cdot4\\] <p>Step 2:</p> \\[-2\\equiv7\\bmod9\\]","tags":["hkust","math"]},{"location":"2024/12/01/discrete-math-quick-notes/#linear-congruences","title":"Linear Congruences","text":"<p>For \\(ax\\equiv b\\bmod m\\) with \\(\\gcd(a,m)=1\\):</p> \\[ a^{-1}ax\\equiv a^{-1}b\\bmod m \\] <p>If \\(\\gcd(a,m)\\neq1\\), there may be multiple solutions or no solution.</p>","tags":["hkust","math"]},{"location":"2024/12/01/discrete-math-quick-notes/#chinese-remainder-theorem","title":"Chinese Remainder Theorem","text":"<p>For \\(x,y\\in\\set{m_1,m_2,\\cdots,m_n}\\) such that \\(\\gcd(x,y)=1\\):</p> \\[ \\begin{aligned} x\\equiv a_1&amp;\\bmod m_1 \\\\ x\\equiv a_2&amp;\\bmod m_2 \\\\ \\vdots \\\\ x\\equiv a_n&amp;\\bmod m_n \\end{aligned} \\] <p>Let \\(m=m_1m_2\\cdots m_n\\) and define:</p> \\[ M_k=\\frac m{m_k}, k=1,2,\\dots,n \\] <p>If \\(\\gcd(m_k,M_k)=1\\), then:</p> \\[y_kM_k\\equiv1\\bmod m_k\\] <p>Finally, compute:</p> \\[ x=a_1M_1y_1+a_2M_2y_2+\\cdots+a_nM_ny_n \\] \\[ x\\equiv a_1\\cdot m_1\\cdot m_1^{-1}+a_2\\cdot m_2\\cdot m_2^{-1}+\\cdots+a_n\\cdot m_n\\cdot m_n^{-1} \\bmod m \\]","tags":["hkust","math"]},{"location":"2024/12/01/discrete-math-quick-notes/#repeated-squaring-method","title":"Repeated Squaring Method","text":"<p>Example: evaluate \\(2048^{13}\\bmod2050\\)</p> \\[\\begin{aligned}2048^{2^0}\\bmod2050&amp;=2048\\\\2048^{2^1}\\bmod2050&amp;=4\\\\2048^{2^2}\\bmod2050&amp;=16\\\\2048^{2^3}\\bmod2050&amp;=256\\\\\\end{aligned}\\] \\[13=2^0+2^2+2^3\\] \\[2048^{13}\\equiv2048^{2^3}\\cdot2048^{2^2}\\cdot2048^{2^0}\\equiv2048\\cdot16\\cdot256\\equiv8\\bmod2050\\]","tags":["hkust","math"]},{"location":"2024/12/01/discrete-math-quick-notes/#fermat-little-theorem","title":"Fermat Little Theorem","text":"<p>For any prime \\(p\\) and integer \\(a\\) such that \\(p\\nmid a\\):</p> \\[ a^{p-1}\\equiv1\\bmod p \\]","tags":["hkust","math"]},{"location":"2024/11/24/cotimo-planner-report/","title":"CoTiMo Planner Report","text":"","tags":["control"]},{"location":"2024/11/24/cotimo-planner-report/#cotimo-planner-report","title":"CoTiMo Planner Report","text":"<p>CoTiMo represents Collision-Free Smooth Path Generation, Time-Optimal Path Parameterization, and Model Predictive Control Planner. It is a comprehensive solution for robotic motion planning.</p> <p></p> <p>This report provides a detailed explanation of the planner's core algorithmic implementations, covering key modules such as path generation, time parameterization, and model predictive control.</p>","tags":["control"]},{"location":"2024/11/24/cotimo-planner-report/#collision-free-smooth-path-generation","title":"Collision-Free Smooth Path Generation","text":"","tags":["control"]},{"location":"2024/11/24/cotimo-planner-report/#cubic-spline","title":"Cubic Spline","text":"<p>For a group of certain points \\((x_{0,1},x_{0,2},\\cdots,x_{0,m}),(x_{1,1},x_{1,2},\\cdots,x_{1,m}), (x_{2,1},x_{2,2},\\cdots,x_{2,m}), \\cdots, (x_{n,1},x_{n,2},\\cdots,x_{n,m})\\), we want to find a smooth enough curve that passes all these points to achieve the interpolation goal. To do so, we can use piecewise cubic functions to fitting the points.</p> <p>Let's talk about a specific dimention \\(k\\) first. To simplify the denotation, we let \\(x_i=x_{i,k}\\). For a set of \\(n+1\\) points \\((x_{0,k},x_{1,k},x_{2,k},\\cdots,x_{n,k})\\) and for the \\(i\\)-th piece of the spline, we can represent it by</p> \\[ q_i(s)=a_is^3+b_is^2+c_is+d_i, \\ \\ \\ \\ s\\in[0,1],\\ i=0,1,\\cdots,n-1 \\] <p>To ensure the smooth of the curve, we write down the constraints,</p> \\[ \\begin{cases} q_{i-1}(1)=q_i(0)=x_{i} \\\\ q_{i-1}'(1)=q_i'(0) \\\\ q_{i-1}''(1)=q_i''(0) \\end{cases} \\] <p>Apply \\(a_i,b_i,c_i,d_i\\) to the constraints, we get</p> \\[ \\begin{cases} a_{i-1}+b_{i-1}+c_{i-1}+d_{i-1}=d_i \\\\ 3a_{i-1}+2b_{i-1}+c_{i-1}=c_i \\\\ 6a_{i-1}+2b_{i-1}=2b_i \\\\ \\end{cases} \\] <p>So we can get the following expressions</p> \\[ \\begin{cases} a_i=x_{i+2}-2x_{i+1}+x_{i} \\\\ b_i=-x_{i+2}+2x_{i+1,k}-x_{i} \\\\ c_{i}=x_{i+1}-x_{i} \\\\ d_i=x_{i} \\end{cases}\\ ,\\ i=1,2,\\cdots,n-1 \\] <p>In open loop case, assume stationary boundary \\(q_0'(0)=0,q_{n-1}'(1)=0\\), which is \\(c_0=0, 3a_{n-1}+2b_{n-1}+c_{n-1}=0\\).</p> <p>You can also regard \\(x_{n+2}\\) as \\(x_{n+1}\\), so that \\(x_{n+2}-2x_{n+1}+x_n=-x_{n+1}+x_n\\)</p> <p>Convert the expressions to vector form, we have</p> \\[ \\left[\\begin{matrix} a_0\\\\a_1\\\\a_2\\\\a_3\\\\\\vdots\\\\a_{n-2}\\\\a_{n-1}\\\\ \\end{matrix}\\right]= \\left[\\begin{matrix} 2&amp;-3&amp;1&amp;0&amp;\\cdots&amp;0&amp;0&amp;0\\\\ 0&amp;1&amp;-2&amp;1&amp;\\cdots&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;1&amp;-2&amp;\\cdots&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;1&amp;\\cdots&amp;0&amp;0&amp;0\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ 0&amp;0&amp;0&amp;0&amp;\\cdots&amp;1&amp;-2&amp;1\\\\ 0&amp;0&amp;0&amp;0&amp;\\cdots&amp;0&amp;1&amp;-1\\\\ \\end{matrix}\\right] \\left[\\begin{matrix} x_0\\\\x_1\\\\x_2\\\\x_3\\\\\\vdots\\\\x_{n-2}\\\\x_{n-1}\\\\x_{n} \\end{matrix}\\right] \\] \\[ \\left[\\begin{matrix} b_0\\\\b_1\\\\b_2\\\\b_3\\\\\\vdots\\\\b_{n-2}\\\\b_{n-1}\\\\ \\end{matrix}\\right]= \\left[\\begin{matrix} -3&amp;4&amp;-1&amp;0&amp;\\cdots&amp;0&amp;0&amp;0\\\\ 0&amp;-1&amp;2&amp;-1&amp;\\cdots&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;-1&amp;2&amp;\\cdots&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;-1&amp;\\cdots&amp;0&amp;0&amp;0\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ 0&amp;0&amp;0&amp;0&amp;\\cdots&amp;-1&amp;2&amp;-1\\\\ 0&amp;0&amp;0&amp;0&amp;\\cdots&amp;0&amp;-1&amp;1\\\\ \\end{matrix}\\right] \\left[\\begin{matrix} x_0\\\\x_1\\\\x_2\\\\x_3\\\\\\vdots\\\\x_{n-2}\\\\x_{n-1}\\\\x_{n} \\end{matrix}\\right] \\] \\[ \\left[\\begin{matrix} c_0\\\\c_1\\\\c_2\\\\c_3\\\\\\vdots\\\\c_{n-2}\\\\c_{n-1}\\\\ \\end{matrix}\\right]= \\left[\\begin{matrix} 0&amp;0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0&amp;0\\\\ 0&amp;-1&amp;1&amp;0&amp;\\cdots&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;-1&amp;1&amp;\\cdots&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;-1&amp;\\cdots&amp;0&amp;0&amp;0\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ 0&amp;0&amp;0&amp;0&amp;\\cdots&amp;-1&amp;1&amp;0\\\\ 0&amp;0&amp;0&amp;0&amp;\\cdots&amp;0&amp;-1&amp;1\\\\ \\end{matrix}\\right] \\left[\\begin{matrix} x_0\\\\x_1\\\\x_2\\\\x_3\\\\\\vdots\\\\x_{n-2}\\\\x_{n-1}\\\\x_{n} \\end{matrix}\\right] \\] \\[ \\left[\\begin{matrix} d_0\\\\d_1\\\\d_2\\\\d_3\\\\\\vdots\\\\d_{n-2}\\\\d_{n-1}\\\\ \\end{matrix}\\right]= \\left[\\begin{matrix} 1&amp;0&amp;0&amp;0&amp;\\cdots&amp;0&amp;0&amp;0\\\\ 0&amp;1&amp;0&amp;0&amp;\\cdots&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;1&amp;0&amp;\\cdots&amp;0&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;1&amp;\\cdots&amp;0&amp;0&amp;0\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ 0&amp;0&amp;0&amp;0&amp;\\cdots&amp;1&amp;0&amp;0\\\\ 0&amp;0&amp;0&amp;0&amp;\\cdots&amp;0&amp;1&amp;0\\\\ \\end{matrix}\\right] \\left[\\begin{matrix} x_0\\\\x_1\\\\x_2\\\\x_3\\\\\\vdots\\\\x_{n-2}\\\\x_{n-1}\\\\x_{n} \\end{matrix}\\right] \\] <p>For every dimension in \\(x\\), we would obtain four vector \\(a, b, c, d\\)</p>","tags":["control"]},{"location":"2024/11/24/cotimo-planner-report/#minimize-stretch-energy","title":"Minimize Stretch Energy","text":"<p>Then, we can define energy function by acceleration</p> \\[ Energy(x_0,x_1,\\cdots,x_n)=\\sum_{i=1}^{n-1}\\int_0^1||q_i''(s)||_1{\\rm d}s \\] <p>But in practice, I use the following formula</p> \\[ Energy(x_0,x_1,\\cdots,x_n)=\\sum_{i=1}^{n-1}||x_{i-1}-2x_i+x_{i+1}||^2 \\]","tags":["control"]},{"location":"2024/11/24/cotimo-planner-report/#collision-free","title":"Collision Free","text":"<p>We want to avoid our robot get crashed into the obstacle. So that we also need to define a potential function.</p> <p>First, we need to calculate the distance between the robot and the obstacle. Which is find a point \\(o\\) in obstacle (\\(Ao\\leq b\\)), to minimize the distance from robot \\(x\\) to point \\(o\\)</p> \\[ \\begin{aligned} \\min\\ &amp; ||o-x||^2 \\\\ \\text{s.t.}\\ &amp;Ao\\leq b \\end{aligned} \\] \\[ \\begin{aligned} \\min\\ &amp; o^To-2x^To+x^Tx \\\\ \\text{s.t.}\\ &amp;Ao\\leq \\text{b} \\end{aligned} \\] <p>By SOCP, we can obtain the distance vector</p> \\[ d(x)=\\left[\\begin{matrix} d(x,o_1),d(x,o_2),\\cdots,d(x,o_m) \\end{matrix}\\right]^T \\] <p>And the potential function would be defined as below</p> \\[ Potential(x_0,x_1,\\cdots,x_n) = \\sum_{i=0}^{n}\\exp(||d(x_i)||_\\infty) \\]","tags":["control"]},{"location":"2024/11/24/cotimo-planner-report/#loss-function","title":"Loss Function","text":"\\[ Loss(x_0,x_1,\\cdots,x_n)=\\eta\\cdot Energy(x_0,x_1,\\cdots,x_n)+(1-\\eta)\\cdot Potential(x_0,x_1,\\cdots,x_n) \\] <p>Finally, we convert the problem to an optimization problem, we can use Line Search, Quasi Newton, LBFGS, or Newton-CG to solve it.</p> \\[ \\min_{x_0,x_1,\\cdots,x_n}Loss(x_0,x_1,\\cdots,x_n) \\] <p>In practice, I add some trick to make the optimizer more stable. Additional to five different \\(\\eta\\), I introduce sigmoid function in update step.</p> \\[ \\sigma(x)=\\frac1{1+e^{-x}}=\\frac{e^x}{1+e^x}=1-\\sigma(-x) \\] \\[ x^{k+1}=x^{k}-\\eta_k\\left[2\\cdot\\sigma\\big(\\nabla Loss(x)\\big)-1\\right] \\]","tags":["control"]},{"location":"2024/11/24/cotimo-planner-report/#time-optimal-path-parameterization","title":"Time Optimal Path Parameterization","text":"","tags":["control"]},{"location":"2024/11/24/cotimo-planner-report/#continuous-case","title":"Continuous Case","text":"<p>After optimizing the control points \\(x_0, x_1, \\cdots, x_n\\), we get a spline \\(q\\) uniquely determined by arc-length \\(s\\).</p> <p>Then the optimal time \\(T\\) is integrated by</p> \\[ T = \\int_0^T 1\\ {\\rm d}t = \\int_0^L\\frac1{\\frac{{\\rm d}s}{{\\rm d}t}}{\\rm d}s \\] <p>Additional to \\(q(s)\\), \\(\\frac{{\\rm d}q}{{\\rm d}s}\\) and \\(\\frac{{\\rm d}^2q}{{\\rm d}s^2}\\) are also known. Because the cubic spline is second order differentiable.</p> <p>We also have velocity constraint,</p> \\[ -v_{max}\\leq v\\leq v_{max} \\] <p>acceleration constraint,</p> \\[ -a_{max}\\leq a\\leq a_{max} \\] <p>and always move forward constraint</p> \\[ \\frac{{\\rm d}s}{{\\rm d}t}&gt;0 \\] <p>The true velocity is</p> \\[ \\frac{{\\rm d}q}{{\\rm d}t} = \\frac{{\\rm d}q}{{\\rm d}s}\\cdot\\frac{{\\rm d}s}{{\\rm d}t} \\] <p>The true acceleration is</p> \\[ \\frac{{\\rm d}^2q}{{\\rm d}t^2}=\\frac{{\\rm d}^2q}{{\\rm d}s^2}\\cdot\\frac{{\\rm d}s}{{\\rm d}t}+\\frac{{\\rm d}q}{{\\rm d}s}\\cdot\\frac{{\\rm d}^2s}{{\\rm d}t^2} \\] <p>Denote</p> \\[ a(s) = \\frac{{\\rm d}^2s}{{\\rm d}t^2},\\ b(s)=\\bigg(\\frac{{\\rm d}s}{{\\rm d}t}\\bigg)^2 \\] <p>Then the problem is described by</p> \\[ \\begin{aligned} \\min_{a(s),b(s)}\\ &amp; \\int_0^L\\frac1{\\sqrt{b(s)}}{\\rm d}s \\\\ \\text{s.t.}\\ &amp; b'(s)=2a(s) \\\\ &amp; q'(0)\\sqrt{b(0)} = v_0 \\\\ &amp; q'(L)\\sqrt{b(L)} = v_L  \\\\ &amp; b(s) &gt; 0\\ \\ \\  (\\geq) \\\\ &amp; ||q'(s)\\sqrt{b(s)}||_\\infty\\le v_\\max \\\\ &amp; ||q''(s)b(s) + q'(s)a(s) ||_\\infty \\le a_\\max \\\\ \\end{aligned} \\]","tags":["control"]},{"location":"2024/11/24/cotimo-planner-report/#discrete-case","title":"Discrete Case","text":"<p>Try to obtain the discretized form \\(a\\in{\\mathbb R}^K,s,b\\in{\\mathbb R}^{K+1}\\). \\(K\\) is different from the number of curves or control points \\(N\\) and is the result of resampling the curve. Usually \\(K\\) is greater than \\(N\\).</p> \\[ \\begin{aligned} \\min_{a,b} &amp; \\sum_{k=1}^{K}\\frac{2(s^{k+1}-s^k)}{\\sqrt{b^{k+1}}+\\sqrt{b^k}} \\\\ {\\text s.t.}\\ &amp; \\frac{b^{k+1}-b^k}{s^{k+1}-s^k}=2a^k, &amp;\\forall k\\in[1,K]\\\\ &amp; q'(s^0)\\sqrt{b^0} = v_0 \\\\ &amp; q'(s^{K+1})\\sqrt{b^{K+1}}=v_L \\\\ &amp; b^k \\ge 0, &amp;\\forall k\\in[1,K+1] \\\\ &amp; ||q'(s^k)\\sqrt{b^k}||_\\infty\\le v_\\max, &amp;\\forall k\\in[1,K+1] \\\\ &amp; ||q''(s^k)b^k + q'(s^k)a^k ||_\\infty \\le a_\\max, &amp;\\forall k\\in[1,K]\\\\ \\end{aligned} \\]","tags":["control"]},{"location":"2024/11/24/cotimo-planner-report/#second-order-cones","title":"Second-Order Cones","text":"<p>To rewrite the problem to a second-order conic programming form, we first try to bound nonlinear term \\(\\sqrt{b^k}\\) with \\(c^k\\), such that \\(\\sqrt{b^k}\\ge c^k,k\\in[1,K+1]\\). Going a step further, we introduce one more slack variable \\(d^k\\), which satisfies \\(\\frac1{c^{k+1}+c^k}\\le d^k,k\\in[1,K]\\).</p> \\[ \\min_{a,b} \\sum_{k=1}^{K}\\frac{2(s^{k+1}-s^k)}{\\sqrt{b^{k+1}}+\\sqrt{b^k}}  \\Leftrightarrow\\ \\begin{aligned} \\min_{a,b,c,d} &amp;\\sum_{k=1}^K 2d^k(s^{k+1}-s^k) \\\\ {\\text s.t.}\\ &amp; \\left|\\left|\\begin{matrix}c^k+c^{k+1}-d^k\\\\2\\end{matrix}\\right|\\right|_2\\le c^{k}+c^{k+1}+d^k, &amp;\\forall k\\in[1,K] \\\\ &amp; \\left|\\left|\\begin{matrix}b^k-1\\\\2c^k\\end{matrix}\\right|\\right|_2\\le b^k+1, &amp;\\forall k\\in[1,K+1] \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\left|\\left|\\begin{matrix}c^k+c^{k+1}-d^k\\\\2\\end{matrix}\\right|\\right|_2\\le c^{k}+c^{k+1}+d^k &amp; \\Leftrightarrow \\begin{bmatrix}c^k+c^{k+1}+d^k\\\\c^k+c^{k+1}-d^k\\\\2\\end{bmatrix}\\in{\\mathcal Q^3} \\\\ \\left|\\left|\\begin{matrix}b^k-1\\\\2c^k\\end{matrix}\\right|\\right|_2\\le b^k+1 &amp; \\Leftrightarrow \\begin{bmatrix}b^k+1\\\\b^k-1\\\\2c^k\\end{bmatrix}\\in{\\mathcal Q^3} \\\\ \\end{aligned} \\] <p>Rewriting the constraints of the optimization problem, especially the nonlinear terms \\(\\sqrt{b_k}\\), we get the final objective function (2D case).</p> \\[ \\begin{aligned} \\min_{a,b,c,d}&amp; \\sum_{k=1}^K 2(s^{k+1}-s^k)d^k \\\\ {\\text s.t.}&amp; \\begin{flalign*} \\begin{bmatrix}c^k+c^{k+1}+d^k\\\\c^k+c^{k+1}-d^k\\\\2\\end{bmatrix} &amp;\\in {\\mathcal Q^3}, &amp;&amp;\\forall k\\in[1,K] \\\\ \\begin{bmatrix}b^k+1\\\\b^k-1\\\\2c^k\\end{bmatrix} &amp;\\in {\\mathcal Q^3}, &amp;&amp;\\forall k\\in[1,K+1] \\\\ 2(s^{k+1}-s^k)a^k+b^k-b^{k+1} &amp;=0, &amp;&amp;\\forall k\\in[1,K] \\\\ \\left\\{\\left[q'_x(s^0)\\right]^2+\\left[q_y'(s^0)\\right]^2\\right\\}(b^0)^2 &amp;= v_0^2 \\\\ \\left\\{\\left[q'_x(s^{K+1})\\right]^2+\\left[q_y'(s^{K+1})\\right]^2\\right\\}(b^{K+1})^2 &amp;= v_L^2 \\\\ -b^k &amp;\\le 0, &amp;&amp;\\forall k\\in[1,K+1] \\\\ \\left[q'_x(s^k)\\right]^2b^k &amp;\\le v^2_\\max, &amp;&amp;\\forall k\\in[1,K+1] \\\\ \\left[q'_y(s^k)\\right]^2b^k &amp;\\le v^2_\\max, &amp;&amp;\\forall k\\in[1,K+1] \\\\ \\left[q''_x(s^k)\\right]^2b^k+q'_x(s^k)a^k &amp;\\le a_\\max, &amp;&amp;\\forall k\\in[1,K] \\\\ \\left[q''_y(s^k)\\right]^2b^k+q'_y(s^k)a^k &amp;\\le a_\\max, &amp;&amp;\\forall k\\in[1,K] \\\\ -\\left[q''_x(s^k)\\right]^2b^k-q'_x(s^k)a^k &amp;\\le a_\\max, &amp;&amp;\\forall k\\in[1,K] \\\\ -\\left[q''_y(s^k)\\right]^2b^k-q'_y(s^k)a^k &amp;\\le a_\\max, &amp;&amp;\\forall k\\in[1,K] \\\\ \\end{flalign*} \\end{aligned} \\]","tags":["control"]},{"location":"2024/11/24/cotimo-planner-report/#phr-alm-for-symmetric-cones","title":"PHR ALM for Symmetric Cones","text":"<p>All constraints are divided into second-order cone constraints, equality constraints and inequality constraints.</p> \\[ \\begin{aligned} \\min_x\\ &amp; c^Tx \\\\ {\\text s.t.}\\ &amp; A_i+b_i\\in K_i \\\\ &amp; Gx=h \\\\ &amp; Px\\le q \\\\ \\end{aligned} \\] <p>Therefore, we can reformulate the problem using the Augmented Lagrangian method for Symmetric Cones.</p> \\[ \\begin{aligned} {\\mathcal L}_\\rho(x,\\mu,\\lambda,\\eta) = &amp; c^Tx + \\\\ &amp; \\frac\\rho2\\sum_{i=1}^m ||P_{K_i}(\\frac{\\mu_i}\\rho-A_ix-b_i)||^2 + \\\\ &amp; \\frac\\rho2||Gx-h-\\frac\\lambda\\rho||^2 + \\\\ &amp; \\frac\\rho2||\\max[Px-Q+\\frac\\eta\\rho,0]||^2 \\end{aligned} \\] <p>\\(P_{\\mathcal K}\\) is the projection of a vector on a symmetric cone</p> \\[ P_{\\mathcal K}(v)=\\arg\\min_{x\\in\\mathcal K}||v-x||^2 \\] <p>In second order cone case,</p> \\[ P_{\\mathcal K=\\mathcal Q^n}(v)=\\begin{cases} 0, &amp;v_0\\le-||v_1||_2 \\\\ \\frac{v_0+||v_1||_2}{2||v_1||_2}(||v_1||_2,v_1)^T, &amp;|v_0|&lt;||v_1||_2 \\\\ v, &amp;v_0\\ge||v_1||_2 \\end{cases} \\] <p>And the gradient of the augmented Lagrangian function is given by</p> \\[ \\begin{aligned} \\nabla{\\mathcal L}_\\rho(x,\\mu,\\lambda,\\eta) = &amp; c + \\rho\\sum_{i=1}^m A_i^TP_{K_i}(\\frac{\\mu_i}\\rho-A_ix-b_i) + \\\\ &amp; \\rho G^T(Gx-h-\\frac\\lambda\\rho) + \\\\ &amp; \\rho P\\{\\max[Px-Q+\\frac\\eta\\rho,0]\\} \\end{aligned} \\] <p>So we can use a L-BFGS method to solve this convex and unconstrained function.</p> \\[ \\begin{aligned} x &amp;\\leftarrow \\arg\\min_x \\mathcal L_\\rho(x,\\mu,\\lambda,\\eta) \\\\ \\mu_i &amp;\\leftarrow P_{\\mathcal K_i}(\\mu_i-\\rho(A_ix+b_i)) \\\\ \\lambda &amp;\\leftarrow \\lambda + \\rho(Gx-h) \\\\ \\eta &amp;\\leftarrow \\max[\\eta+\\rho(Px-q),0] \\\\ \\rho &amp;\\leftarrow \\min[(1+\\gamma)\\rho,\\beta] \\\\ \\end{aligned} \\] <p>\\(\\gamma\\) is the growth rate of \\(\\rho\\) and \\(\\beta\\) is the upper bound of \\(\\rho\\), which is typically <code>1e3</code></p>","tags":["control"]},{"location":"2024/11/24/cotimo-planner-report/#model-predictive-control","title":"Model Predictive Control","text":"","tags":["control"]},{"location":"2024/11/24/cotimo-planner-report/#swerve-kinematic-model","title":"Swerve Kinematic Model","text":"<p>A simple omnidirectional platform is mecanum wheel chassis. But for a FRC player, a swerve chassis is what we need.</p> <p>Consider only the translation case. Let's simply denote \\((x,y)\\) for position, \\(v\\) for velocity, \\(a\\) for acceleration, \\(\\theta\\) for angle, \\(\\omega\\) for angular velocity, \\(\\alpha\\) for angular acceleration.</p> \\[ \\begin{bmatrix} \\dot x \\\\ \\dot y \\\\ \\dot v \\\\ \\dot \\theta \\\\ \\dot \\omega \\\\ \\end{bmatrix} = \\begin{bmatrix} v\\cos\\theta \\\\ v\\sin\\theta \\\\ a \\\\ \\omega \\\\ \\alpha \\\\ \\end{bmatrix} \\] <p>So that, we can use the following equation to update the states in discrete time domian.</p> \\[ \\begin{cases} v_{k+1} = v_k +a\\cdot\\Delta t \\\\ \\omega_{k+1} = \\omega_k + \\alpha\\cdot\\Delta t \\\\ \\theta_{k+1} = \\theta_k + \\frac12(\\omega_k+\\omega_{k+1})\\cdot\\Delta t \\\\ x_{k+1} = x_k+\\frac12(v_k\\cos\\theta_k+v_{k+1}\\cos\\theta_{k+1})\\cdot\\Delta t \\\\ y_{k+1} = y_k+\\frac12(v_k\\sin\\theta_k+v_{k+1}\\sin\\theta_{k+1})\\cdot\\Delta t \\\\ \\end{cases},\\ \\ u_k=\\begin{bmatrix}a\\\\\\alpha\\end{bmatrix} \\]","tags":["control"]},{"location":"2024/11/24/cotimo-planner-report/#objective-function","title":"Objective Function","text":"<p>Denote the number of control points of optimal control is \\(M\\), and \\(\\bar u_k=\\begin{bmatrix}u_{k}^T&amp;u_{k+1}^T&amp;\\cdots u_{k+M-1}^T\\end{bmatrix}^T\\in\\mathbb R^{2M}\\)</p> \\[ {\\text Loss}(\\bar u_k)=\\sum_{i=0}^{M-1}\\left[ \\begin{aligned} &amp;(x_{k+i}-\\hat x_{k+1})^2+ \\\\ &amp;(y_{k+i}-\\hat y_{k+1})^2+ \\\\ &amp;\\omega_a(a_{k+i-1}-a_{k+i})^2+\\\\ &amp;\\omega_\\alpha(\\alpha_{k+i-1}-\\alpha_{k+1})^2 \\end{aligned}\\right] \\] \\[ \\begin{aligned} \\min_{\\bar u_k}\\ &amp; {\\text Loss}(\\bar u_k) \\\\ {\\text s.t.}\\ &amp; a_\\min\\le a_i\\le a_\\max, &amp;\\forall i\\in[k,k+M-1] \\\\ &amp; \\alpha_\\min\\le \\alpha_i\\le \\alpha_\\max, &amp;\\forall i\\in[k,k+M-1] \\\\ \\end{aligned} \\] <p>The method to solve this objective function is similar to the PHR ALM method mentioned above, except that the terms related to the symmetric cone are removed.</p>","tags":["control"]},{"location":"2024/11/26/classic-control-quick-notes/","title":"Classic Control Quick Notes","text":"","tags":["hkust","control"]},{"location":"2024/11/26/classic-control-quick-notes/#classic-control-quick-notes","title":"Classic Control Quick Notes","text":"<p>This cheat sheet summarizes key concepts in System Modeling, Analysis and Control.</p>","tags":["hkust","control"]},{"location":"2024/11/26/classic-control-quick-notes/#transfer-function","title":"Transfer Function","text":"<p>For an LTI system, we can take the Laplace transform with zero initial conditions</p> \\[ \\begin{aligned} sX(s) &amp;= AX(s) + bU(s) \\\\ Y(s) &amp;= cX(s) + dU(s) \\end{aligned} \\] <p>Using linear algebra, we can express \\(X(s)\\) as:</p> \\[ X(s) = (sI-A)^{-1} b U(s) \\] <p>The transfer function of an LTI system is defined as the ratio of the Laplace transform of the output to that of the input when the initial conditions are zero:</p> \\[ G(s) = \\frac{Y(s)}{U(s)} = c(sI-A)^{-1} b + d \\] <p>Different state space representations can be derived for a particular system due to the infinite number of linear transformations, but the transfer function remains unique.</p>","tags":["hkust","control"]},{"location":"2024/11/26/classic-control-quick-notes/#routh-stability-criterion","title":"Routh Stability Criterion","text":"<p>Given the characteristic polynomial:</p> \\[ c(s)=1s^5+2s^4+3s^3+4s^2+5s^1+6s^0 \\] \\(s^5\\) \\(1\\) \\(3\\) \\(5\\) \\(s^4\\) \\(2\\) \\(4\\) \\(6\\) \\(s^3\\) \\(\\begin{aligned}&amp;-\\frac12\\begin{vmatrix}1&amp;3\\\\2&amp;4\\end{vmatrix}\\\\=&amp;-\\frac12(1\\times4-2\\times3)\\\\=&amp;3-\\frac{1\\times4}2\\\\=&amp;1\\end{aligned}\\) \\(\\begin{aligned}&amp;-\\frac12\\begin{vmatrix}1&amp;5\\\\2&amp;6\\end{vmatrix}\\\\=&amp;-\\frac12(1\\times6-2\\times5)\\\\=&amp;5-\\frac{1\\times6}2\\\\=&amp;2\\end{aligned}\\) \\(s^2\\) \\(4-\\frac{2\\times2}1=0\\) \\(6\\) \\(s^1\\) \\(2\\) \\(s^0\\) \\(6\\)","tags":["hkust","control"]},{"location":"2024/11/26/classic-control-quick-notes/#kharitonov-theorem","title":"Kharitonov Theorem","text":"<p>Let</p> \\[ {\\mathcal P}=\\set{a(s)=a_0s^n+a_1s^{n-1}+\\cdots+a_{n-1}s+a_n\\ |\\ a_i\\in[\\underline{a_i}, \\overline{a_i}]} \\] <p>All members of \\(\\mathcal P\\) are stable if and only if the following four polynomials are stable</p> \\[ \\begin{aligned} a_1(s)=\\underline{a_0}s^n+\\underline{a_1}s^{n-1}+\\overline{a_2}s^{n-2}+\\overline{a_3}s^{n-3}+\\underline{a_4}s^{n-4}+\\cdots\\\\ a_2(s)=\\underline{a_0}s^n+\\overline{a_1}s^{n-1}+\\overline{a_2}s^{n-2}+\\underline{a_3}s^{n-3}+\\underline{a_4}s^{n-4}+\\cdots\\\\ a_3(s)=\\overline{a_0}s^n+\\overline{a_1}s^{n-1}+\\underline{a_2}s^{n-2}+\\underline{a_3}s^{n-3}+\\overline{a_4}s^{n-4}+\\cdots\\\\ a_4(s)=\\overline{a_0}s^n+\\underline{a_1}s^{n-1}+\\underline{a_2}s^{n-2}+\\overline{a_3}s^{n-3}+\\overline{a_4}s^{n-4}+\\cdots\\\\ \\end{aligned} \\]","tags":["hkust","control"]},{"location":"2024/11/26/classic-control-quick-notes/#prototype-2nd-order-system","title":"Prototype 2nd-Order System","text":"<p>The transfer function for a second-order system is given by:</p> \\[ H(s)=\\frac{k\\omega_n^2}{s^2+2\\zeta\\omega_ns+\\omega_n^2}=\\frac{k(\\sigma^2+\\omega_d^2)}{(s+\\sigma)^2+\\omega_d^2} \\]","tags":["hkust","control"]},{"location":"2024/11/26/classic-control-quick-notes/#time-constants","title":"Time Constants","text":"<p>The following approximations hold:</p> \\[ \\begin{cases} t_r\\approx\\frac{1.8}{\\omega_n} \\\\ t_p=\\frac\\pi{\\omega_n\\sqrt{1-\\zeta^2}} = \\frac\\pi{\\omega_d} \\\\ t_s\\approx\\frac3{\\zeta\\omega_n} = \\frac3\\sigma \\end{cases} \\]","tags":["hkust","control"]},{"location":"2024/11/26/classic-control-quick-notes/#percent-overshoot","title":"Percent Overshoot","text":"<p>The percent overshoot (PO) is defined as:</p> \\[ PO=\\left(\\frac{y(t_p)}{y(\\infty)}-1\\right)\\times100\\%=\\exp\\left(-\\frac{\\zeta\\pi}{\\sqrt{1-\\zeta^2}}\\right) \\]","tags":["hkust","control"]},{"location":"2024/11/26/classic-control-quick-notes/#final-value-theorem","title":"Final Value Theorem","text":"<p>The final value theorem states that:</p> \\[ FVT=H(0)=k \\]","tags":["hkust","control"]},{"location":"2024/11/26/classic-control-quick-notes/#bodes-sensitivity","title":"Bode's Sensitivity","text":"<p>In the nominal situation, we have the motor with DC gain = \\(A\\), and the overall transfer function, either open- or closed-loop, has some other DC gain (call it \\(T\\)).</p>","tags":["hkust","control"]},{"location":"2024/11/26/classic-control-quick-notes/#perturbations","title":"Perturbations","text":"<p>Let:</p> \\[ \\begin{aligned} \\hat A=A+\\delta A \\hat T=T+\\delta T \\end{aligned} \\] <p>Then, the sensitivity can be approximated as:</p> \\[ \\delta T\\approx\\frac{{\\rm d}T}{{\\rm d}A}\\delta A \\]","tags":["hkust","control"]},{"location":"2024/11/26/classic-control-quick-notes/#sensitivity-function","title":"Sensitivity Function","text":"<p>The sensitivity \\(\\mathcal S\\) is given by:</p> \\[ {\\mathcal S}=\\frac{\\frac{\\delta T}T}{\\frac{\\delta A}A}=\\frac{\\delta T\\cdot A}{\\delta A\\cdot T}\\approx\\frac{{\\rm d}T}{{\\rm d}A}\\cdot\\frac AT \\]","tags":["hkust","control"]},{"location":"2024/11/26/classic-control-quick-notes/#root-locus","title":"Root Locus","text":"<p>The standard form of the root locus is:</p> \\[ 1+KL(s)=0 \\]","tags":["hkust","control"]},{"location":"2024/11/26/classic-control-quick-notes/#transformation-to-standard-form","title":"Transformation to Standard Form","text":"<p>Change to standard form:</p> \\[ a(s)+Kb(s)=0 \\] <p>This can be expressed as:</p> \\[ 1+K\\cdot\\frac{b(s)}{a(s)}=0 \\]","tags":["hkust","control"]},{"location":"2024/11/26/classic-control-quick-notes/#root-locus-rules","title":"Root Locus Rules","text":"Rule A <code>n</code> branches Rule B starts at <code>s = x, x, ...</code> Rule C ends at <code>s = x, x, ...</code> Rule D Real locus: <code>(-xx,-xx) U (-xx,-xx)</code> Rule E <code>n - m =xx</code>, <code>l = 0,1,...,xx-1</code>Asymptotes = <code>xxx\u00b0, xxx\u00b0</code> Rule F <code>a(s)+Kb(s)=0</code>Routh Table =&gt; <code>K\u2208(xx,xx)</code><code>j\u00b7w?</code>, <code>w=?</code>","tags":["hkust","control"]},{"location":"2024/09/17/rotation-representations-in-robotics/","title":"Rotation Representations in Robotics","text":"","tags":["math","robotics"]},{"location":"2024/09/17/rotation-representations-in-robotics/#rotation-representations-in-robotics","title":"Rotation Representations in Robotics","text":"<p>\u4ecb\u7ecd\u4e86\u56db\u79cd\u59ff\u6001\u8868\u793a\u65b9\u5f0f\uff1a\u65cb\u8f6c\u77e9\u9635\u3001\u6b27\u62c9\u89d2\u3001\u8f74\u89d2 \u548c \u56db\u5143\u6570\uff0c\u4ee5\u53ca\u5b83\u4eec\u4e4b\u95f4\u76f8\u4e92\u8f6c\u6362\u5173\u7cfb\u7684\u63a8\u5bfc</p>","tags":["math","robotics"]},{"location":"2024/09/17/rotation-representations-in-robotics/#rotation-matrix","title":"\u65cb\u8f6c\u77e9\u9635 Rotation Matrix","text":"<p>\u5bf9\u4e8e\u7269\u4f53\u5750\u6807\u7cfb \\(\\{O\\}\\) \u4e0b\u4e09\u4e2a\u4e3b\u8f74\u7684\u5355\u4f4d\u77e2\u91cf \\(\\hat X_O\\)\u3001\\(\\hat Y_O\\) \u548c \\(\\hat Z_O\\) \u5728\u4e16\u754c\u5750\u6807\u7cfb \\(\\{W\\}\\) \u4e0a\u7684\u6295\u5f71 \\(^W\\hat X_O\\)\u3001\\(^W\\hat Y_O\\) \u548c \\(^W\\hat Z_O\\)\uff0c\u53ef\u4ee5\u5229\u7528\u65cb\u8f6c\u77e9\u9635 \\(^W_OR\\) \u6765\u8868\u793a\u7269\u4f53\u5750\u6807\u7cfb\u7684\u59ff\u6001</p> \\[ \\begin{aligned} ^W_OR = \\begin{pmatrix}^W\\hat X_O &amp; ^W\\hat Y_O &amp; ^W\\hat Z_O\\end{pmatrix} = \\begin{pmatrix} \\hat X_O \\cdot \\hat X_W &amp; \\hat Y_O\\cdot\\hat X_W &amp; \\hat Z_O\\cdot\\hat X_W \\\\ \\hat X_O \\cdot \\hat Y_W &amp; \\hat Y_O\\cdot\\hat Y_W &amp; \\hat Z_O\\cdot\\hat Y_W \\\\ \\hat X_O \\cdot \\hat Z_W &amp; \\hat Y_O\\cdot\\hat Z_W &amp; \\hat Z_O\\cdot\\hat Z_W \\\\ \\end{pmatrix} = \\begin{pmatrix}^O\\hat X_W^T \\\\ ^O\\hat Y_W^T \\\\ ^O\\hat Z_W^T\\end{pmatrix} \\end{aligned} \\] <p>\u663e\u7136\uff0c\u8fd9\u662f\u4e00\u4e2a\u6b63\u4ea4\u77e9\u9635</p> \\[ \\begin{aligned} ^W_OR^T\\ ^W_OR = \\begin{pmatrix}^W\\hat X_O^T \\\\ ^W\\hat Y_O^T \\\\ ^W\\hat Z_O^T\\end{pmatrix} \\begin{pmatrix}^W\\hat X_O &amp; ^W\\hat Y_O &amp; ^W\\hat Z_O\\end{pmatrix} = I_3 \\end{aligned} \\] <p>\u8fdb\u4e00\u6b65\uff0c\u7531\u4e8e\u65cb\u8f6c\u77e9\u9635\u7684\u6240\u6709\u884c\u5411\u91cf\u7684\u6a21\u957f\u5747\u4e3a 1\uff0c\u4e14\u884c\u5411\u91cf\u4e4b\u95f4\u4e24\u4e24\u6b63\u4ea4\uff0c\u6709 \\(\\det(R)=1\\)</p> <p>\u66f4\u666e\u904d\u5730\uff0c\u6211\u4eec\u7528 \\(R = \\begin{pmatrix}r_{11} &amp; r_{12} &amp; r_{13} \\\\ r_{21} &amp; r_{22} &amp; r_{23} \\\\ r_{31} &amp; r_{32} &amp; r_{33} \\\\ \\end{pmatrix}\\) \u6765\u8868\u793a\u4e00\u4e2a\u65cb\u8f6c\u77e9\u9635</p> <p>\u63a5\u4e0b\u6765\u6211\u4eec\u63a8\u5bfc\u7ed5\u4efb\u4e00\u8f74\u65cb\u8f6c\u7684\u65cb\u8f6c\u77e9\u9635\u7684\u8868\u8fbe\u5f62\u5f0f</p> <p>\u8bb0\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u4e00\u4efb\u610f\u5411\u91cf \\(\\bf v\\) \u7ed5\u65cb\u8f6c\u8f74 \\(\\bf n=\\begin{pmatrix}n_x\\\\n_y\\\\n_z\\end{pmatrix}\\) \u65cb\u8f6c \\(\\theta\\) \u5f97\u5230\u65b0\u5411\u91cf \\(\\bf v_{rot}\\)\uff0c\u5176\u4e2d \\(\\bf n\\) \u4e3a\u5355\u4f4d\u5411\u91cf\uff0c\\(n_x^2+n_y^2+n_z^2=1\\)</p> <p>\u5219 \\(\\bf v\\) \u53ef\u4ee5\u6839\u636e\u5e73\u884c\u548c\u6b63\u4ea4\u4e8e \\(\\bf n\\) \u5206\u4e3a\u4e24\u90e8\u5206 \\(\\bf v_\\parallel\\) \u548c \\(\\bf v_\\perp\\)\uff0c\u6ee1\u8db3 \\(\\bf v=\\bf v_\\parallel+\\bf v_\\perp\\)</p> <p>\u5176\u4e2d</p> \\[ \\begin{cases} \\bf v_\\parallel = (\\bf v\\cdot\\bf n)n \\\\ \\bf v_\\perp = \\bf v-(\\bf v\\cdot\\bf n)n = -\\bf n\\times(\\bf n\\times\\bf v) \\end{cases} \\] <p>\u800c</p> \\[ \\begin{aligned} \\bf v_{rot} &amp;= \\bf v_{\\parallel rot} + \\bf v_{\\perp rot} \\\\ &amp;= \\bf v_\\parallel + \\cos\\theta\\bf v_\\perp + \\sin\\theta\\bf n\\times\\bf v_\\perp \\\\ &amp;= (\\bf v\\cdot\\bf n)n + \\cos\\theta\\bf v_\\perp + \\sin\\theta\\bf n\\times\\bf v \\\\ &amp;= \\cos\\theta\\bf v+(1-\\cos\\theta)(\\bf n\\cdot\\bf v)\\bf n + \\sin\\theta\\bf n\\times\\bf v \\end{aligned} \\] <p>\u4e0a\u5f0f\u5c31\u662f \u7f57\u5fb7\u91cc\u683c\u65af\u65cb\u8f6c\u516c\u5f0f Rodrigues' Rotation Formula\uff0c\u8fdb\u4e00\u6b65\u6211\u4eec\u4ee4 \\(\\bf v\\) \u5206\u522b\u4e3a \\(\\begin{pmatrix}1&amp;0&amp;0\\end{pmatrix}^T\\)\uff0c\\(\\begin{pmatrix}0&amp;1&amp;0\\end{pmatrix}^T\\) \u548c \\(\\begin{pmatrix}0&amp;0&amp;1\\end{pmatrix}^T\\)</p> <p>\u89e3\u5f97</p> \\[ \\begin{aligned} \\bf v_{rot} &amp;= \\cos\\theta\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}+(1-\\cos\\theta)\\cdot n_x\\begin{pmatrix}n_x\\\\n_y\\\\n_z\\end{pmatrix}+\\sin\\theta\\begin{pmatrix}0\\\\n_z\\\\-n_y\\end{pmatrix} \\\\ &amp;=\\begin{pmatrix}(1-\\cos\\theta)n_x^2+\\cos\\theta\\\\(1-\\cos\\theta)n_xn_y+\\sin\\theta n_z\\\\(1-\\cos\\theta)n_xn_z-\\sin\\theta n_y\\end{pmatrix} \\end{aligned} \\] \\[ \\bf v_{rot}=\\begin{pmatrix}(1-\\cos\\theta)n_yn_x-\\sin\\theta n_z\\\\(1-\\cos\\theta)n_y^2+\\cos\\theta\\\\(1-\\cos\\theta)n_yn_z+\\sin\\theta n_x\\end{pmatrix} \\] \\[ \\bf v_{rot}=\\begin{pmatrix}(1-\\cos\\theta)n_zn_x+\\sin\\theta n_y\\\\(1-\\cos\\theta)n_zn_y-\\sin\\theta n_x\\\\(1-\\cos\\theta)n_z^2+\\cos\\theta\\end{pmatrix} \\] <p>\u7efc\u4e0a\u5f97\u5230\u7ed5\u4efb\u610f\u8f74\u7684\u65cb\u8f6c\u77e9\u9635</p> \\[ R=\\begin{pmatrix}n_x^2(1-\\cos\\theta)+\\cos\\theta &amp; n_xn_y(1-\\cos\\theta)-n_z\\sin\\theta &amp; n_xn_z(1-\\cos\\theta)+n_y\\sin\\theta \\\\ n_yn_x(1-\\cos\\theta)+n_z\\sin\\theta &amp; n_y^2(1-\\cos\\theta)+\\cos\\theta &amp; n_yn_z(1-\\cos\\theta)-n_x\\sin\\theta \\\\ n_zn_x(1-\\cos\\theta)-n_y\\sin\\theta &amp; n_zn_y(1-\\cos\\theta)+n_x\\sin\\theta &amp; n_z^2(1-\\cos\\theta)+\\cos\\theta\\end{pmatrix} \\]","tags":["math","robotics"]},{"location":"2024/09/17/rotation-representations-in-robotics/#euler-angles","title":"\u6b27\u62c9\u89d2 Euler Angles","text":"<p>\u5b9a\u4e49\uff1a</p> <ul> <li>\u987a\u89c4\uff1a\u5408\u6cd5\u7684\u6b27\u62c9\u89d2\u7ec4\u4e2d\uff0c\u4efb\u4f55\u4e24\u4e2a\u8fde\u7eed\u7684\u65cb\u8f6c\uff0c\u5fc5\u987b\u7ed5\u7740\u4e0d\u540c\u7684\u8f6c\u52a8\u8f74\u65cb\u8f6c\uff0c\u56e0\u6b64\u4e00\u5171\u6709 12 \u79cd\u987a\u89c4\uff0c\u88ab\u5212\u5206\u4e3a\u4e24\u5927\u7c7b\uff1a</li> <li>\u7ecf\u5178\u6b27\u62c9\u89d2 Proper Euler Angles: <code>zxz</code>, <code>xyx</code>, <code>yzy</code>, <code>zyz</code>, <code>xzx</code>, <code>yxy</code></li> <li>\u6cf0\u7279-\u5e03\u83b1\u6069\u89d2 Tait-Bryan Angles: <code>xyz</code>, <code>yzx</code>, <code>zxy</code>, <code>xzy</code>, <code>zyx</code>, <code>yxz</code> \u5176\u4e2d\u6309<code>zyx</code>\u987a\u5e8f\u65cb\u8f6c\u7684\u60c5\u51b5\u53c8\u88ab\u79f0\u4e3a RPY \u89d2\u6216 XYZ \u56fa\u5b9a\u89d2</li> <li>\u5185\u5916\u65cb\uff1a\u6839\u636e\u7ed5\u65cb\u8f6c\u540e\u7684\u65b0\u8f74\u65cb\u8f6c\u8fd8\u662f\u7ed5\u4e16\u754c\u5750\u6807\u7cfb\u4e2d\u56fa\u5b9a\u4e0d\u52a8\u7684\u8f74\u65cb\u8f6c\uff0c\u6b27\u62c9\u89d2\u88ab\u5206\u4e3a\u5185\u65cb\u548c\u5916\u65cb\uff1a</li> <li>\u5185\u65cb Intrisic Rotation, \u53c8\u79f0\u4e3a\u52a8\u6001\u65cb\u8f6c\uff0c\u7ed5\u7269\u4f53\u5750\u6807\u7cfb\u65cb\u8f6c</li> <li>\u5916\u65cb Extrinsic Rotation, \u53c8\u79f0\u4e3a\u9759\u6001\u65cb\u8f6c\uff0c\u7ed5\u4e16\u754c\u5750\u6807\u7cfb\u65cb\u8f6c</li> <li>\u5185\u65cb\u4e0e\u5916\u65cb\u5177\u6709\u7b49\u4ef7\u6027</li> </ul>","tags":["math","robotics"]},{"location":"2024/09/17/rotation-representations-in-robotics/#2","title":"\u6b27\u62c9\u89d2 2 \u65cb\u8f6c\u77e9\u9635","text":"\\[ \\begin{aligned} R &amp;= R_Z(\\gamma)R_Y(\\beta)R_X(\\alpha)= \\begin{pmatrix} \\cos\\gamma &amp; -\\sin\\gamma &amp; 0 \\\\ \\sin\\gamma &amp; \\cos\\gamma &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\end{pmatrix} \\begin{pmatrix} \\cos\\beta &amp; 0 &amp; \\sin\\beta \\\\ 0 &amp; 1 &amp; 0 \\\\ -\\sin\\beta &amp; 0 &amp; \\cos\\beta \\\\ \\end{pmatrix} \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; \\cos\\alpha &amp; -\\sin\\alpha \\\\ 0 &amp; \\sin\\alpha &amp; \\cos\\alpha \\\\ \\end{pmatrix} \\\\ \\\\ &amp;=R_X^{-1}(\\gamma)R_Y^{-1}(\\beta)R_Z^{-1}(\\alpha)= \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; \\cos\\alpha &amp; \\sin\\alpha \\\\ 0 &amp; -\\sin\\alpha &amp; \\cos\\alpha \\\\ \\end{pmatrix} \\begin{pmatrix} \\cos\\beta &amp; 0 &amp; -\\sin\\beta \\\\ 0 &amp; 1 &amp; 0 \\\\ \\sin\\beta &amp; 0 &amp; \\cos\\beta \\\\ \\end{pmatrix} \\begin{pmatrix} \\cos\\gamma &amp; \\sin\\gamma &amp; 0 \\\\ -\\sin\\gamma &amp; \\cos\\gamma &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\end{pmatrix} \\end{aligned} \\]","tags":["math","robotics"]},{"location":"2024/09/17/rotation-representations-in-robotics/#2_1","title":"\u65cb\u8f6c\u77e9\u9635 2 \u6b27\u62c9\u89d2","text":"<p>\u4ee5\u65cb\u8f6c\u987a\u5e8f\u4e3a <code>zyx</code> \u7684\u5185\u65cb\u6b27\u62c9\u89d2\u4e3a\u4f8b</p> \\[ \\begin{aligned} R = R_Z(\\gamma)R_Y(\\beta)R_X(\\alpha) &amp;= \\begin{pmatrix} \\cos\\gamma &amp; -\\sin\\gamma &amp; 0 \\\\ \\sin\\gamma &amp; \\cos\\gamma &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\end{pmatrix} \\begin{pmatrix} \\cos\\beta &amp; 0 &amp; \\sin\\beta \\\\ 0 &amp; 1 &amp; 0 \\\\ -\\sin\\beta &amp; 0 &amp; \\cos\\beta \\\\ \\end{pmatrix} \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; \\cos\\alpha &amp; -\\sin\\alpha \\\\ 0 &amp; \\sin\\alpha &amp; \\cos\\alpha \\\\ \\end{pmatrix} \\\\ \\\\&amp;= \\begin{pmatrix} \\cos\\gamma\\cos\\beta &amp; -\\sin\\gamma\\cos\\alpha+\\cos\\gamma\\sin\\beta\\sin\\alpha &amp; \\sin\\gamma\\sin\\alpha+\\cos\\gamma\\sin\\beta\\cos\\alpha \\\\ \\sin\\gamma\\cos\\beta &amp; \\cos\\gamma\\cos\\alpha+\\sin\\gamma\\sin\\beta\\sin\\alpha &amp; -\\cos\\gamma\\sin\\alpha+\\sin\\gamma\\sin\\beta\\cos\\alpha \\\\ -\\sin\\beta &amp; \\cos\\beta\\sin\\alpha &amp; \\cos\\beta\\cos\\alpha \\end{pmatrix} \\end{aligned} \\] <p>\u7531\u6b64</p> \\[ \\begin{cases} \\alpha = \\arctan(\\frac{r_{32}}{r_{33}}) \\\\ \\beta = \\arcsin(-r_{31}) \\\\ \\gamma = \\arctan(\\frac{r_{21}}{r_{11}}) \\\\ \\end{cases} \\] <p>\u4f46\u5f53 \\(\\beta=\\frac\\pi2\\) \u65f6\uff0c\\(R=\\begin{pmatrix}0&amp;-\\sin(\\gamma-\\alpha)&amp;cos(\\gamma-\\alpha)\\\\0&amp;\\cos(\\gamma-\\alpha)&amp;\\sin(\\gamma-\\alpha)\\\\1&amp;0&amp;0\\end{pmatrix}\\)\uff0c\u65cb\u8f6c\u77e9\u9635\u9000\u5316\uff0c\u65e0\u6cd5\u89e3\u51fa\u552f\u4e00 \\(\\gamma\\) \u548c\\(\\alpha\\)\uff0c\u8fd9\u79cd\u60c5\u51b5\u88ab\u79f0\u4e3a \u5947\u5f02\u6027 Singularity \u6216 \u4e07\u5411\u8282\u6b7b\u9501 Glimbal Lock\uff0c\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e09\u7ef4\u7a7a\u95f4\u7684\u7269\u4f53\u59ff\u6001\u65e0\u6cd5\u901a\u8fc7\u4e09\u4e2a\u5b9e\u6570\u8fdb\u884c\u5b8c\u5168\u8868\u8fbe\u3002\u5728\u5b9e\u8df5\u4e2d\uff0c\u53ef\u4ee5\u9884\u5206\u6790\u673a\u68b0\u6700\u96be\u51fa\u73b0\u7684\u59ff\u6001\uff0c\u5e76\u901a\u8fc7\u6709\u610f\u5730\u8bbe\u8ba1\u6b27\u62c9\u89d2\u987a\u89c4\uff0c\u5c3d\u53ef\u80fd\u907f\u514d\u51fa\u73b0\u4e07\u5411\u8282\u6b7b\u9501\u7684\u60c5\u51b5\u3002</p>","tags":["math","robotics"]},{"location":"2024/09/17/rotation-representations-in-robotics/#axis-angle","title":"\u8f74\u89d2 Axis Angle","text":"<p>\u8f74\u89d2\u901a\u8fc7\u4e24\u4e2a\u53c2\u6570\u63cf\u8ff0\u4e00\u4e2a\u65cb\u8f6c\uff1a\u4e00\u6761\u8f74\u548c\u63cf\u8ff0\u7ed5\u8fd9\u4e2a\u8f74\u7684\u65cb\u8f6c\u91cf\u7684\u89d2\u5ea6\uff0c\u8bb0\u4e3a \\(&lt;axis,angle&gt;=\\begin{pmatrix}\\left[a_x\\\\a_y\\\\a_z\\right]^T,\\theta\\end{pmatrix}\\)</p>","tags":["math","robotics"]},{"location":"2024/09/17/rotation-representations-in-robotics/#2_2","title":"\u8f74\u89d2 2 \u65cb\u8f6c\u77e9\u9635","text":"<p>\u7531\u7f57\u5fb7\u91cc\u683c\u65af\u65cb\u8f6c\u516c\u5f0f\u53ef\u63a8\u51fa\u7ed5\u4efb\u610f\u8f74\u65cb\u8f6c\u7684\u65cb\u8f6c\u77e9\u9635\uff0c\u5177\u4f53\u63a8\u5bfc\u89c1</p> \\[ R=\\begin{pmatrix}a_x^2(1-\\cos\\theta)+\\cos\\theta &amp; a_xa_y(1-\\cos\\theta)-a_z\\sin\\theta &amp; a_xa_z(1-\\cos\\theta)+a_y\\sin\\theta \\\\ a_ya_x(1-\\cos\\theta)+a_z\\sin\\theta &amp; a_y^2(1-\\cos\\theta)+\\cos\\theta &amp; a_ya_z(1-\\cos\\theta)-a_x\\sin\\theta \\\\ a_za_x(1-\\cos\\theta)-a_y\\sin\\theta &amp; a_za_y(1-\\cos\\theta)+a_x\\sin\\theta &amp; a_z^2(1-\\cos\\theta)+\\cos\\theta\\end{pmatrix} \\]","tags":["math","robotics"]},{"location":"2024/09/17/rotation-representations-in-robotics/#2_3","title":"\u65cb\u8f6c\u77e9\u9635 2 \u8f74\u89d2","text":"<p>\u89c2\u5bdf\u4e0a\u5f0f\uff0c\u5bf9\u7ed9\u5b9a\u65cb\u8f6c\u77e9\u9635\uff0c\u6709</p> \\[ \\begin{cases} \\theta = \\arccos(\\frac12(r_{11}+r_{22}+r_{33}-1)) \\\\ a_x = \\frac{r_{32}-r_{23}}{\\sqrt{(r_{32}-r_{23})^2+(r_{13}-r_{31})^2+(r_{21}-r_{12})^2}} \\\\ a_y = \\frac{r_{13}-r_{31}}{\\sqrt{(r_{32}-r_{23})^2+(r_{13}-r_{31})^2+(r_{21}-r_{12})^2}} \\\\ a_z = \\frac{r_{21}-r_{12}}{\\sqrt{(r_{32}-r_{23})^2+(r_{13}-r_{31})^2+(r_{21}-r_{12})^2}} \\\\ \\end{cases} \\]","tags":["math","robotics"]},{"location":"2024/09/17/rotation-representations-in-robotics/#2_4","title":"\u6b27\u62c9\u89d2 2 \u8f74\u89d2","text":"<p>\u6109\u5feb\u5730\uff0c\u6211\u4eec\u53ef\u4ee5\u5229\u7528 \u6b27\u62c9\u89d2 --&gt; \u56db\u5143\u6570 --&gt; \u8f74\u89d2 \u7684\u65b9\u5f0f\u5f97\u5230</p> \\[ \\begin{cases} \\theta=2*\\arccos(\\cos\\gamma\\cos\\beta\\cos\\alpha-\\sin\\gamma\\sin\\beta\\sin\\alpha) \\\\ a_x=\\sin\\gamma\\sin\\beta\\cos\\alpha+\\cos\\gamma\\cos\\beta\\sin \\alpha\\\\ a_y=\\sin\\gamma\\cos\\beta\\cos\\alpha+\\cos\\gamma\\sin\\beta\\sin\\alpha\\\\ a_z=\\cos\\gamma\\sin\\beta\\cos\\alpha-\\sin\\gamma\\cos\\beta\\sin\\alpha \\end{cases} \\]","tags":["math","robotics"]},{"location":"2024/09/17/rotation-representations-in-robotics/#2_5","title":"\u8f74\u89d2 2 \u6b27\u62c9\u89d2","text":"<p>\u76f4\u63a5\u4ece\u8f74\u89d2\u8f6c\u4e3a\u6b27\u62c9\u89d2\u6bd4\u8f83\u56f0\u96be\uff0c\u6211\u4eec\u53ef\u4ee5\u5229\u7528 \u8f74\u89d2 --&gt; \u65cb\u8f6c\u77e9\u9635 --&gt; \u6b27\u62c9\u89d2 \u7684\u95f4\u63a5\u8f6c\u5316\u65b9\u5f0f\u51cf\u5c11\u8ba1\u7b97\u96be\u5ea6</p> \\[ \\begin{cases} \\alpha = \\arctan(\\frac{a_ya_z(1-\\cos\\theta)-a_x\\sin\\theta}{a_z^2(1-\\cos\\theta)+\\cos\\theta}) \\\\ \\beta = \\arcsin(a_y\\sin\\theta-a_xa_z(1-\\cos\\theta)) \\\\ \\gamma = \\arctan(\\frac{a_xa_y(1-\\cos\\theta)-a_z\\sin\\theta}{a_x^2(1-\\cos\\theta)+\\cos\\theta}) \\\\ \\end{cases} \\] <p>\u4f7f\u7528\u6b27\u62c9\u89d2\u8fdb\u884c\u8868\u793a\u65f6\uff0c\u67d0\u4e9b\u60c5\u51b5\u4f1a\u51fa\u73b0\u4e07\u5411\u8282\u9501\u548c\u5947\u5f02\u7684\u73b0\u8c61\uff0c\u9700\u8981\u7279\u6b8a\u5904\u7406</p>","tags":["math","robotics"]},{"location":"2024/09/17/rotation-representations-in-robotics/#quaterions","title":"\u56db\u5143\u6570 Quaterions","text":"<p>\u4e0d\u59a8\u8bb0\u56db\u5143\u6570\u4e3a \\(q=w+xi+yj+zk\\)\uff0c\u5176\u4e2d \\(i^2=j^2=k^2=ijk=-1\\)\uff0c\u800c\u5355\u4f4d\u56db\u5143\u6570\u6ee1\u8db3 \\(w^2+x^2+y^2+z^2=1\\)</p> <p>\u7ed5\u4efb\u610f\u8f74 \\(n\\) \u65cb\u8f6c \\(\\theta\\) \u7684\u56db\u5143\u6570\uff0c\u8868\u793a\u4e3a \\(q=[w,x,y,z]=[\\cos\\frac\\theta2,\\sin\\frac\\theta2n_x,\\sin\\frac\\theta2n_y,\\sin\\frac\\theta2n_z]\\)</p>","tags":["math","robotics"]},{"location":"2024/09/17/rotation-representations-in-robotics/#2_6","title":"\u8f74\u89d2 2 \u56db\u5143\u6570","text":"\\[ \\begin{cases} w = \\cos\\frac\\theta2 \\\\ x = a_x\\sin\\frac\\theta2 \\\\ y = a_y\\sin\\frac\\theta2 \\\\ z = a_z\\sin\\frac\\theta2 \\\\ \\end{cases} \\]","tags":["math","robotics"]},{"location":"2024/09/17/rotation-representations-in-robotics/#2_7","title":"\u56db\u5143\u6570 2 \u8f74\u89d2","text":"\\[ \\begin{cases} \\theta = 2*\\arccos(w)\\\\ a_x=\\frac{x}{\\sqrt{1-w^2}}\\\\ a_y=\\frac{y}{\\sqrt{1-w^2}}\\\\ a_z=\\frac{z}{\\sqrt{1-w^2}}\\\\ \\end{cases} \\]","tags":["math","robotics"]},{"location":"2024/09/17/rotation-representations-in-robotics/#2_8","title":"\u56db\u5143\u6570 2 \u65cb\u8f6c\u77e9\u9635","text":"<p>\u7531\u7f57\u5fb7\u91cc\u683c\u65af\u65cb\u8f6c\u516c\u5f0f\u53ef\u63a8\u51fa\u7ed5\u4efb\u610f\u8f74\u65cb\u8f6c\u7684\u65cb\u8f6c\u77e9\u9635</p> \\[ R=\\begin{pmatrix}n_x^2(1-\\cos\\theta)+\\cos\\theta &amp; n_xn_y(1-\\cos\\theta)-n_z\\sin\\theta &amp; n_xn_z(1-\\cos\\theta)+n_y\\sin\\theta \\\\ n_yn_x(1-\\cos\\theta)+n_z\\sin\\theta &amp; n_y^2(1-\\cos\\theta)+\\cos\\theta &amp; n_yn_z(1-\\cos\\theta)-n_x\\sin\\theta \\\\ n_zn_x(1-\\cos\\theta)-n_y\\sin\\theta &amp; n_zn_y(1-\\cos\\theta)+n_x\\sin\\theta &amp; n_z^2(1-\\cos\\theta)+\\cos\\theta\\end{pmatrix} \\] <p>\u5c06 \\(n_x,n_y,n_z,\\cos\\theta,\\sin\\theta\\) \u7528 \\(w,x,y,z\\) \u8868\u793a</p> \\[ \\begin{cases} \\cos\\theta = 1-2\\cos^2\\frac\\theta2 \\\\ \\sin\\theta = 2\\sin\\frac\\theta2\\cos\\frac\\theta2 \\end{cases} \\] <p>\u53ef\u4ee5\u63a8\u51fa\u4ee5\u4e0b\u8868\u8fbe</p> \\[ \\begin{aligned} R_q = \\begin{pmatrix} 1-2y^2-2z^2 &amp; 2xy-2wz &amp; 2xz+2wy \\\\ 2xy+2wz &amp; 1-2x^2-2z^2 &amp; 2yz-2wx \\\\ 2xz-2wy &amp; 2yz+2wx &amp; 1-2x^2-2y^2 \\\\ \\end{pmatrix} \\end{aligned} \\]","tags":["math","robotics"]},{"location":"2024/09/17/rotation-representations-in-robotics/#2_9","title":"\u65cb\u8f6c\u77e9\u9635 2 \u56db\u5143\u6570","text":"<p>\u5f53\u7ed9\u5b9a\u65cb\u8f6c\u77e9\u9635\u540e\uff0c\u89c2\u5bdf\u5f97\u5230</p> \\[ \\begin{aligned} r_{32}-r_{23}=(2yz+2wx)-(2yz-2wx)=4wx \\\\ r_{13}-r_{31}=(2xz+2wy)-(2xz-2wy)=4wy \\\\ r_{21}-r_{12}=(2xy+2wz)-(2xy-2wz)=4wz \\\\ \\end{aligned} \\] <p>\u53c8\u6709</p> \\[ r_{11}+r_{22}+r_{33}=3-4x^2-4y^2-4z^2=4w^2-1 \\] <p>\u8fdb\u4e00\u6b65\uff0c\u53ef\u5f97</p> \\[ \\begin{cases} w=\\frac12\\sqrt{1+r_{11}+r_{22}+r_{33}} \\\\ x=\\frac{r_{32}-r_{23}}{4w} \\\\ y=\\frac{r_{13}-r_{31}}{4w} \\\\ z=\\frac{r_{21}-r_{12}}{4w} \\\\ \\end{cases} \\]","tags":["math","robotics"]},{"location":"2024/09/17/rotation-representations-in-robotics/#2_10","title":"\u6b27\u62c9\u89d2 2 \u56db\u5143\u6570","text":"<p>\u5bf9\u4e8e\u6b27\u62c9\u89d2\u8f6c\u56db\u5143\u6570\uff0c\u6211\u4eec\u53ef\u4ee5\u6839\u636e\u56db\u5143\u6570\u7684\u5b9a\u4e49\u5199\u51fa\u4ee5\u4e0b\u8868\u8fbe</p> \\[ \\begin{aligned} q(\\gamma, \\beta, \\alpha) = q_z(\\gamma)\\ q_y(\\beta)\\ q_x(\\alpha) = \\begin{pmatrix} \\cos\\frac\\gamma2 \\\\ 0 \\\\ 0 \\\\ \\sin\\frac\\gamma2 \\end{pmatrix} \\begin{pmatrix} \\cos\\frac\\beta2 \\\\ 0 \\\\ \\sin\\frac\\beta2 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} \\cos\\frac\\alpha2 \\\\ \\sin\\frac\\alpha2 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\cos\\frac\\gamma2\\cos\\frac\\beta2\\cos\\frac\\alpha2 - \\sin\\frac\\gamma2\\sin\\frac\\beta2\\sin\\frac\\alpha2 \\\\ \\sin\\frac\\gamma2\\sin\\frac\\beta2\\cos\\frac\\alpha2 + \\cos\\frac\\gamma2\\cos\\frac\\beta2\\sin\\frac\\alpha2 \\\\ \\cos\\frac\\gamma2\\sin\\frac\\beta2\\cos\\frac\\alpha2 - \\sin\\frac\\gamma2\\cos\\frac\\beta2\\sin\\frac\\alpha2 \\\\ \\sin\\frac\\gamma2\\cos\\frac\\beta2\\cos\\frac\\alpha2 + \\cos\\frac\\gamma2\\sin\\frac\\beta2\\sin\\frac\\alpha2 \\\\ \\end{pmatrix} \\end{aligned} \\]","tags":["math","robotics"]},{"location":"2024/09/17/rotation-representations-in-robotics/#2_11","title":"\u56db\u5143\u6570 2 \u6b27\u62c9\u89d2","text":"<p>\u76f4\u63a5\u4ece\u56db\u5143\u6570\u8f6c\u4e3a\u6b27\u62c9\u89d2\u6bd4\u8f83\u56f0\u96be\uff0c\u6211\u4eec\u53ef\u4ee5\u5229\u7528 \u56db\u5143\u6570 --&gt; \u65cb\u8f6c\u77e9\u9635 --&gt; \u6b27\u62c9\u89d2 \u7684\u95f4\u63a5\u8f6c\u5316\u65b9\u5f0f\u51cf\u5c11\u8ba1\u7b97\u96be\u5ea6</p> \\[ \\begin{cases} \\alpha = \\arctan(\\frac{2yz+2wx}{1-2x^2-2y^2}) \\\\ \\beta = \\arcsin(2wy-2xz) \\\\ \\gamma = \\arctan(\\frac{2xy+2wz}{1-2y^2-2z^2}) \\\\ \\end{cases} \\] <p>\u540c\u6837\u5730\uff0c\u4f7f\u7528\u6b27\u62c9\u89d2\u8fdb\u884c\u8868\u793a\uff0c\u67d0\u4e9b\u60c5\u51b5\u4f1a\u51fa\u73b0\u4e07\u5411\u8282\u9501\u548c\u5947\u5f02\u7684\u73b0\u8c61\uff0c\u9700\u8981\u7279\u6b8a\u5904\u7406</p>","tags":["math","robotics"]},{"location":"2024/09/17/rotation-representations-in-robotics/#_1","title":"\u8868\u793a\u65b9\u6cd5\u5bf9\u6bd4","text":"\u65cb\u8f6c\u77e9\u9635 Rotation Matrix \u6b27\u62c9\u89d2 Euler Angles \u8f74\u89d2 Axis Angle \u56db\u5143\u6570 Quarternions \u4f18 \u53ef\u4ee5\u8868\u793a\u4e09\u7ef4\u7a7a\u95f4\u4efb\u4e00\u59ff\u6001\uff0c\u5e76\u63a8\u5e7f\u5230 n \u7ef4\u7a7a\u95f4 \u7b26\u5408\u76f4\u89c9 \u89e3\u51b3\u4e86\u4e07\u5411\u9501 \u53ef\u4ee5\u5e73\u6ed1\u63d2\u503c\uff0c\u65e0\u4e07\u5411\u8282\u9501\u73b0\u8c61\uff08\u672c\u8d28\u662f\u4ece\u56db\u7ef4\u7a7a\u95f4\u63cf\u8ff0\u4e09\u7ef4\u59ff\u6001\uff09 \u52a3 \u4e09\u7ef4\u7a7a\u95f4\u7684\u59ff\u6001\u5374\u7528\u4e86\u4e5d\u4e2a\u91cf\u8fdb\u884c\u8868\u793a\uff0c\u5b58\u50a8\u5197\u4f59 \u5947\u5f02\u548c\u4e07\u5411\u8282\u9501\u73b0\u8c61\u65e0\u6cd5\u514b\u670d\uff0c\u65e0\u6cd5\u5e73\u6ed1\u63d2\u503c \u65e0\u6cd5\u5e73\u6ed1\u63d2\u503c\uff08\u672c\u8d28\u662f\u4e09\u7ef4\u7684\uff09 \u53ea\u5bf9\u4e09\u7ef4\u7a7a\u95f4\u6709\u6548","tags":["math","robotics"]},{"location":"2024/12/05/handling-gauge-freedom-in-vins/","title":"Handling Gauge Freedom in VINS","text":"","tags":["slam"]},{"location":"2024/12/05/handling-gauge-freedom-in-vins/#handling-gauge-freedom-in-vins","title":"Handling Gauge Freedom in VINS","text":"<p>Z. Zhang, G. Gallego, and D. Scaramuzza, \"On the Comparison of Gauge Freedom Handling in Optimization-Based Visual-Inertial State Estimation,\" IEEE Robot. Autom. Lett., vol. 3, no. 3, pp. 2710\u20132717, Jul. 2018, doi: 10.1109/LRA.2018.2833152.</p> <p>\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u8ba8\u8bba\u4e86\u89c6\u89c9\u60ef\u6027\u7cfb\u7edf\u4e2d\u89c4\u8303\u81ea\u7531\u5ea6\uff08Gauge Freedom\uff09\u7684\u5904\u7406\u65b9\u6cd5\uff0c\u5e76\u5bf9\u6bd4\u4e86\u4e0d\u540c\u7b56\u7565\u7684\u6548\u679c\u3002</p>","tags":["slam"]},{"location":"2024/12/05/handling-gauge-freedom-in-vins/#_1","title":"\u4e09\u79cd\u5904\u7406\u65b9\u6cd5","text":"<p>\u89c6\u89c9\u60ef\u6027\u7cfb\u7edf\u5b58\u5728\u56db\u81ea\u7531\u5ea6\u4e0d\u53ef\u89c2\uff08\u5e73\u79fb\u548c\u7ed5\u91cd\u529b\u65b9\u5411\u7684\u65cb\u8f6c\uff09\uff0c\u9ad8\u65af\u725b\u987f\u6cd5\u6c42\u89e3\u7ebf\u6027\u65b9\u7a0b\u65f6\uff0c\u5947\u5f02\u77e9\u9635 H \u4e0d\u6ee1\u79e9\uff08\u5b58\u5728\u56db\u7ef4\u7684\u96f6\u7a7a\u95f4\uff09\uff0c\u6c42\u89e3\u7a33\u5b9a\u6027\u8f83\u5dee\u3002\u8fd9\u4e9b\u989d\u5916\u7684\u89c4\u8303\u81ea\u7531\u5ea6 Gauge Freedom \u5fc5\u987b\u88ab\u59a5\u5584\u5904\u7406\u3002\u4f5c\u8005\u5bf9\u6bd4\u4e86\u4e09\u79cd\u5e38\u89c1\u7684\u5904\u7406\u7b56\u7565\u4ee5\u53ca\u5b83\u4eec\u5bf9\u5e94\u7684\u6548\u679c\u3002</p>","tags":["slam"]},{"location":"2024/12/05/handling-gauge-freedom-in-vins/#gauge-fixation","title":"Gauge Fixation","text":"<p>\u9009\u62e9\u5c06\u7b2c\u4e00\u5e27\u7684\u6444\u50cf\u673a\u4f4d\u59ff\u56fa\u5b9a\uff0c\u5728\u5168\u5c40\u4f18\u5316\u4e2d\u4e0d\u66f4\u65b0\u3002\u8fd9\u7b49\u4ef7\u4e8e\u5728\u5168\u5c40\u4f18\u5316\u65f6\u5c06\u7b2c\u4e00\u5e27\u6444\u50cf\u673a\u4f4d\u59ff\u5bf9\u5e94\u7684\u6b8b\u5dee\u7684\u96c5\u53ef\u6bd4\u77e9\u9635\u8bbe\u4e3a\u96f6\u77e9\u9635\u3002</p>","tags":["slam"]},{"location":"2024/12/05/handling-gauge-freedom-in-vins/#gauge-prior","title":"Gauge Prior","text":"<p>\u9009\u62e9\u4e3a\u7b2c\u4e00\u5e27\u4f4d\u59ff\u52a0\u4e0a\u5b9a\u4e49\u5728\u6d41\u5f62\u4e0a\u7684\u6b63\u5219\u5316\u60e9\u7f5a\u9879\uff1a $\\(||\\mathbf r_0^P||_{\\Sigma_0^P}^2=||(\\mathbf p_0-\\mathbf p_0^0,\\Delta\\phi_{0z})||_{\\Sigma_0^P}^2\\)$</p>","tags":["slam"]},{"location":"2024/12/05/handling-gauge-freedom-in-vins/#free-gauge","title":"Free Gauge","text":"<p>\u65b9\u6cd5\u5219\u4e0d\u589e\u52a0\u5148\u9a8c\u7ea6\u675f\uff0c\u800c\u662f\u5728\u4f18\u5316\u65f6\u4f7f\u7528\u963b\u5c3c\u725b\u987f\u6cd5\uff0c\u4f7f \\(\\epsilon\\mathbf I+\\mathbf H\\) \u6b63\u5b9a\u3002</p>","tags":["slam"]},{"location":"2024/12/05/handling-gauge-freedom-in-vins/#_2","title":"\u6536\u655b\u6027\u5206\u6790","text":"<p>\u5728\u6743\u91cd\u77e9\u9635\u7b49\u4e8e\u534f\u65b9\u5dee\u6784\u6210\u7684\u5355\u4f4d\u9635 \\(\\mathbf\\Sigma_0^P = \\boldsymbol\\sigma_0^2\\mathbf I\\) \u65f6\uff1a $\\(||\\mathbf r_0^P||_{\\Sigma_0^P}^2=w^P||\\mathbf r_0^P||^2, w^P = \\frac1{\\sigma_0^2}\\)$</p> <p>\u5f53 \\(w^P\\to\\infty\\) \u65f6\uff0cGauge Prior \u53d8\u6210 Gauge Fixation\uff0c\u5f53 \\(w^P=0\\) \u65f6\uff0cGauge Prior \u53d8\u6210 Gauge Free\u3002</p> <p>\u6743\u91cd \\(w\\) \u5bf9\u5747\u65b9\u8bef\u5dee\u3001\u8fed\u4ee3\u6b21\u6570\u3001\u6536\u655b\u65f6\u95f4\u7684\u5f71\u54cd\u90fd\u4e0d\u5927\uff0c\u5e76\u4e14\u5728\u8d85\u8fc7\u67d0\u4e2a\u9608\u503c\u540e\uff0c\u4e09\u8005\u90fd\u8d8b\u4e8e\u5e73\u7a33\u3002</p> <p></p> <p>\u4f5c\u8005\u8fdb\u4e00\u6b65\u5b9e\u9a8c\u8ba8\u8bba\u8fd9\u79cd\u53cd\u5e38\u7684\u6536\u655b\u65f6\u95f4\u7684\u589e\u52a0\uff1a</p> <ul> <li>\u5bf9\u4e8e\u975e\u5e38\u5927\u7684\u5148\u9a8c\u6743\u91cd \\(10^8\\)\uff0c\u8be5\u7b97\u6cd5\u4f1a\u964d\u4f4e\u91cd\u65b0\u6295\u5f71\u8bef\u5dee\uff0c\u540c\u65f6\u4fdd\u6301\u5148\u9a8c\u8bef\u5dee\u51e0\u4e4e\u7b49\u4e8e\u96f6\u3002</li> <li>\u5bf9\u4e8e\u8f83\u5c0f\u7684\u5148\u9a8c\u6743\u91cd\uff08\u4f8b\u5982 \\(50\\sim500\\)\uff09\uff0c\u4f18\u5316\u7b97\u6cd5\u4f1a\u5728\u524d\u4e24\u6b21\u8fed\u4ee3\u4e2d\u964d\u4f4e\u91cd\u65b0\u6295\u5f71\u8bef\u5dee\uff0c\u4f46\u4ee3\u4ef7\u662f\u589e\u52a0\u5148\u9a8c\u8bef\u5dee\u3002\u7136\u540e\u4f18\u5316\u7b97\u6cd5\u82b1\u8d39\u8bb8\u591a\u6b21\u8fed\u4ee3\u6765\u5fae\u8c03\u5148\u9a8c\u8bef\u5dee\uff0c\u540c\u65f6\u4fdd\u6301\u91cd\u65b0\u6295\u5f71\u8bef\u5dee\u8f83\u5c0f\uff08\u6cbf\u8f68\u9053\u79fb\u52a8\uff09\uff0c\u56e0\u6b64\u8ba1\u7b97\u65f6\u95f4\u4f1a\u589e\u52a0\u3002</li> </ul> <p></p> <p>\u603b\u7684\u6765\u8bf4\uff0c\u4e09\u79cd\u65b9\u6cd5\u5bf9\u7cbe\u5ea6\u7684\u5f71\u54cd\u5e76\u4e0d\u663e\u8457\u3002\u5728\u6743\u91cd\u5408\u9002\u7684\u60c5\u51b5\u4e0b\uff0cGauge Prior \u65b9\u6cd5\u4e0e Gauge Fixation \u51e0\u4e4e\u6709\u76f8\u540c\u7684\u8868\u73b0\uff0c\u800c Free Gauge \u65b9\u6cd5\u7531\u4e8e\u9700\u8981\u66f4\u5c11\u7684\u8fed\u4ee3\u6b21\u6570\u6765\u6536\u655b\uff0c\u6d88\u8017\u7684\u65f6\u95f4\u4f1a\u8f83\u5c11\u3002</p>","tags":["slam"]},{"location":"2024/12/12/a-continuous-time-vins/","title":"A Continuous-time VINS","text":"","tags":["slam"]},{"location":"2024/12/12/a-continuous-time-vins/#a-continuous-time-vins","title":"A Continuous-time VINS","text":"<p>S. Lovegrove, A. Patron-Perez, and G. Sibley, \u201cSpline Fusion: A continuous-time representation for visual-inertial fusion with application to rolling shutter cameras,\u201d in Procedings of the British Machine Vision Conference 2013, Bristol: British Machine Vision Association, 2013, p. 93.1-93.11. doi: 10.5244/C.27.93.</p> <p>\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u5de5\u4f5c\u662f\u5efa\u7acb\u4e86\u5377\u5e18\u76f8\u673a\u7684\u8fde\u7eed\u65f6\u57df\u4e0b\u7684\u6570\u5b66\u6a21\u578b\uff0c\u521d\u59cb\u5316\u65b9\u6cd5\u662f\u5229\u7528 IMU \u4e3b\u52a8\u5bf9\u9f50\u89c6\u89c9\u3002</p> <p></p> <p>\u76f8\u6bd4\u79bb\u6563\u65f6\u57df\uff0c\u5728\u8fde\u7eed\u65f6\u57df\u4e0a\u8868\u793a\u6709\u52a9\u4e8e\u878d\u5408\u9ad8\u5e27\u7387\u7684\u4f20\u611f\u5668\u548c\u5f02\u6b65\u65f6\u95f4\u6233\u7684\u8bbe\u5907\u3002\u5bf9\u5377\u5e18\u76f8\u673a\u7684\u7814\u7a76\u5c1a\u672a\u6210\u719f\uff0c\u73b0\u6709\u7684\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u5982\u4f55\u6d88\u9664\u5377\u5e18\u76f8\u673a\u7684\u7578\u53d8\uff0c\u800c\u540e\u590d\u7528\u6807\u51c6\u5168\u5c40\u5feb\u95e8\u76f8\u673a\u7684 SLAM \u6a21\u578b\uff0c\u8fd9\u79cd\u89e3\u8026\u5408\u7684\u5904\u7406\u65b9\u5f0f\u589e\u52a0\u4e86\u65e0\u6cd5\u4fee\u6b63\u7684\u504f\u5dee\u3002</p> <p>\u5355\u76ee\u89c6\u89c9\u7cfb\u7edf\u5b58\u5728 7 \u81ea\u7531\u5ea6\u4e0d\u53ef\u89c2\uff1a6 \u81ea\u7531\u5ea6\u59ff\u6001+\u5c3a\u5ea6\uff0c\u4e00\u79cd\u65b9\u6cd5\u901a\u8fc7\u56de\u73af\u68c0\u6d4b\u548c\u5e8f\u5217\u56fe\u677e\u5f1b\u5bf9\u5c3a\u5ea6\u663e\u5f0f\u53c2\u6570\u5316\uff0c\u53e6\u4e00\u79cd\u65b9\u6cd5\u662f\u52a0\u5165\u53ef\u4ee5\u6d4b\u91cf\u7edd\u5bf9\u5c3a\u5ea6\u7684\u8bbe\u5907\u3002\u5728\u672c\u6587\u4e2d\u4f5c\u8005\u4f7f\u7528\u4e86\u60ef\u6027\u5355\u5143\u3002\u76f8\u6bd4\u8fc7\u5f80\u5efa\u7acb\u5728\u6b27\u62c9\u89d2\u4e0a\u7684\u6570\u5b66\u8868\u793a\uff0c\u4f5c\u8005\u5f15\u5165\u674e\u7fa4\u548c\u674e\u4ee3\u6570\u4e0a\u7684\u65cb\u8f6c\u8868\u793a\u907f\u514d\u4e86\u5947\u5f02\u70b9\uff0c\u540c\u65f6\u80fd\u591f\u66f4\u597d\u5730\u8fd1\u4f3c\u6700\u5c0f\u626d\u77e9\u7684\u8f68\u8ff9\u3002</p> \\[ \\mathbf T_{b,a} = \\begin{bmatrix} \\mathbf R_{b,a} &amp; \\mathbf a_b \\\\ \\mathbf 0^T &amp; 1 \\end{bmatrix},\\ \\mathbf T_{b,a}\\in\\mathbb{SE}3,\\ \\mathbf R_{b,a}\\in\\mathbb{SO}3 \\] <p>\u4f5c\u8005\u5e0c\u671b\u8f68\u8ff9\u7684\u53c2\u6570\u5316\u65b9\u7a0b\u662f\u5c40\u90e8\u53ef\u63a7\uff0c\u4e8c\u9636\u5bfc\u8fde\u7eed\uff0c\u80fd\u8fd1\u4f3c\u6700\u5c0f\u529b\u77e9\u8f68\u8ff9\u7684\u3002\u4e09\u6b21 B \u6837\u6761\u66f2\u7ebf\u53ef\u4ee5\u5f88\u597d\u8868\u793a \\(\\mathbb R^3\\) \u4e0a\u7684\u8f68\u8ff9\uff0c\u4f46\u5728\u4e09\u7ef4\u65cb\u8f6c\u4e0a\u8868\u73b0\u4e00\u822c\u3002\u56e0\u6b64\u4f5c\u8005\u9009\u7528\u4e86\u674e\u4ee3\u6570\u4e0a\u7684\u7d2f\u8ba1\u57fa\u51fd\u6570\uff0c\u8fd9\u79cd\u51fd\u6570\u6700\u65e9\u88ab\u5e94\u7528\u5728\u8ba1\u7b97\u673a\u52a8\u753b\u4e2d\u7684\u56db\u5143\u6570\u63d2\u503c\u4e2d\u3002</p> <p>\u81ea\u7531\u5ea6\u4e3a \\(k-1\\) \u7684 B \u6837\u6761\u66f2\u7ebf\u7684\u6807\u51c6\u57fa\u51fd\u6570\u8868\u793a\u4e3a</p> \\[ \\begin{aligned} \\mathbf p(t) &amp;= \\sum_{i=0}^n\\mathbf p_i B_{i,k}(t) \\\\ &amp;= \\mathbf p_0 B_{0,k}(t) + \\mathbf p_1 B_{1,k}(t) + \\cdots + \\mathbf p_n B_{n,k}(t) \\\\ &amp;= \\mathbf p_0B_{0,k}(t) + \\sum_{i=1}^{n-1}\\mathbf p_iB_{i,k}(t) + \\mathbf p_nB_{n,k}(t) \\\\ &amp;= \\mathbf p_0\\tilde B_{0,k}(t) + \\sum_{i=1}^n\\mathbf p_i\\sum_{j=i}^nB_{j,k}(t) - \\sum_{i=0}^{n-1}\\mathbf p_i\\sum_{j=i+1}^nB_{j,k}(t) \\\\ &amp;= \\mathbf p_0\\tilde B_{0,k}(t) + \\sum_{i=1}^n(\\mathbf p_i-\\mathbf p_{i-1})\\tilde B_{i,k}(t) \\end{aligned} \\] <p>\u5176\u4e2d \\(\\mathbf p_i\\in\\mathbb R^N\\) \u662f\u5728 \\(t_i,i\\in[0,\\dots,n]\\) \u65f6\u523b\u7684\u63a7\u5236\u70b9\uff0c\\(B_{i,k}(t)\\) \u662f\u7531\u5fb7\u4f2f\u5c14-\u8003\u514b\u65af\u9012\u5f52\u516c\u5f0f De Boor - Cox recursive formula \u8ba1\u7b97\u5f97\u5230\u7684\u57fa\u51fd\u6570\u3002\u8fdb\u4e00\u6b65\u5229\u7528\u674e\u7fa4\u7684\u6307\u6570\u6620\u5c04\u548c\u5bf9\u6570\u6620\u5c04\uff0c\u5c06\u516c\u5f0f\u91cd\u5199\u5728\u674e\u7fa4\u57df\u4e0a</p> \\[ \\mathbf T_{w,s}(t) = \\exp(\\tilde B_{0,k}\\log(\\mathbf T_{w,0}))\\prod_{i=1}\\exp(\\tilde B_{i,k}(t)\\log(\\mathbf T_{w,i-1}^{-1}\\mathbf T_{w,i})) \\] <p>\u4f5c\u8005\u8fdb\u4e00\u6b65\u5728\u5047\u8bbe\u63a7\u5236\u70b9\u65f6\u95f4\u95f4\u9694\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\uff08\u5927\u591a\u6570\u5355\u76ee\u7cfb\u7edf\u5e94\u8be5\u90fd\u80fd\u6ee1\u8db3\uff09\uff0c\u5bf9\u65f6\u95f4\u505a\u4e86\u5f52\u4e00\u5316\u5904\u7406</p> \\[ u(t) = \\frac{t-t_0}{\\Delta t} - s_i \\] <p>\u5e76\u5728\u53c2\u6570 \\(k=4\\) \u4e0b\u8f6c\u5316\u4e3a\u77e9\u9635\u8868\u8fbe</p> \\[ \\tilde{\\mathbf B}(u) = \\mathbf C\\begin{bmatrix}1\\\\u\\\\u^2\\\\u^3\\end{bmatrix},\\dot{\\tilde{\\mathbf B}}(u) = \\mathbf C\\begin{bmatrix}0\\\\1\\\\2u\\\\3u^2\\end{bmatrix},\\ddot{\\tilde{\\mathbf B}}(u) = \\mathbf C\\begin{bmatrix}0\\\\0\\\\2\\\\6u\\end{bmatrix},\\mathbf C=\\frac16\\begin{bmatrix}6&amp;0&amp;0&amp;0\\\\5&amp;3&amp;-3&amp;1\\\\1&amp;3&amp;3&amp;-2\\\\0&amp;0&amp;0&amp;1\\end{bmatrix} \\] \\[ \\mathbf T_{w,s}(u) = \\mathbf T_{w,i-1}\\prod_{j=1}^3\\exp(\\tilde{\\mathbf B}(u)_j\\log(\\mathbf T_{w,i+j-1}^{-1}\\mathbf T_{w,i+j})) \\] <p>\\(\\dot{\\mathbf T}_{w,s},\\ddot{\\mathbf T}_{w,s}\\) \u5219\u7531\u94fe\u5f0f\u6cd5\u5219\u548c\u6307\u6570\u6620\u5c04\u7684\u4e00\u9636\u8fd1\u4f3c\u5f97\u5230</p> <p>\u7ed9\u5b9a\u7b2c\u4e00\u6b21\u89c2\u6d4b\u7684\u9006\u6df1\u5ea6 \\(\\mathbb Rho\\in\\mathbb R^+\\), \u5bf9\u5e94\u70b9\u5728\u4e24\u5e27\u7684\u5750\u6807\u5206\u522b\u4e3a \\(\\mathbf p_a,\\mathbf p_b\\in\\mathbb R^2\\), \u5176\u4e2d \\(\\pi(\\mathbf P)=\\frac1{P_2}[P_0,P_1]^T\\)</p> \\[ \\mathbf p_b = \\mathcal W(\\mathbf p_a;\\mathbf T_{b,a},\\mathbb Rho)=\\pi\\bigg(\\begin{bmatrix}\\mathbf K_b&amp;\\mathbf 0\\end{bmatrix}\\mathbf T_{b,a}\\begin{bmatrix}\\mathbf K_a^{-1}\\begin{bmatrix}\\mathbf p_a\\\\1\\end{bmatrix}&amp;\\mathbb Rho\\end{bmatrix}\\bigg) \\] <p>\u5bf9\u4e8e\u4e00\u822c\u7684\u89c6\u89c9\u60ef\u6027\u7cfb\u7edf\u7ed9\u51fa\u635f\u5931\u51fd\u6570</p> \\[ \\begin{aligned} E(\\theta) =&amp; \\sum_{\\hat{\\mathbf p}_m}\\left[\\hat{\\mathbf p}_m-\\mathcal W(\\mathbf p_r;\\mathbf T_{c,s}\\mathbf T_{w,s}(u_m)^{-1}\\mathbf T_{w,s}(u_r)\\mathbf T_{s,c},\\rho)\\right]_{\\Sigma_p}^2 +\\\\ &amp; \\sum_{\\hat\\omega_m}[\\hat\\omega_m-\\text{Gyro}(u_m)]_{\\Sigma_\\omega}^2 + \\sum_{\\hat{\\mathbf a}_m}[\\hat{\\mathbf a}_m-\\text{Accel}(u_m)]^2_{\\Sigma_\\mathbf a} \\end{aligned} \\] <p>\u800c\u5bf9\u5377\u5e18\u76f8\u673a\u5219\u5c06 \\(\\mathbf p_b\\) \u7528 \\(\\mathbf p_b(t)\\) \u66ff\u4ee3\u91cd\u65b0\u5efa\u6a21</p> \\[ \\mathbf p_b(t) = \\begin{bmatrix} x_b(t) \\\\ y_b(t) \\end{bmatrix} = \\mathcal W(\\mathbf p_a;\\mathbf T_{b,a}(t),\\rho) \\] \\[ \\mathbf p_b(t+\\Delta t) = \\mathcal W(\\mathbf p_a;\\mathbf T_{b,a}(t),\\rho) + \\Delta t\\cdot\\frac{\\partial}{\\partial t}\\mathcal W(\\mathbf p_a;\\mathbf T_{b,a}(t),\\rho) \\] \\[ y_b(t+\\Delta t) = \\frac{h(t+\\Delta t-s)}{e-s},\\quad\\Delta t=-\\frac{h\\cdot t_0+s\\cdot(y_b(t)-h)-e\\cdot y_b(t)}{(s-e)\\frac{\\partial}{\\partial t}\\mathcal W_y(\\mathbf p_a;\\mathbf T_{b,a}(t),\\rho)+h} \\] <p>\u8fd9\u4e00\u7cfb\u7edf\u8868\u73b0\u51fa\u4e86\u826f\u597d\u7684\u81ea\u6821\u51c6\u7684\u80fd\u529b</p> <p></p>","tags":["slam"]},{"location":"2024/12/20/robust-initialization-of-vins/","title":"Robust Initialization of VINS","text":"","tags":["slam"]},{"location":"2024/12/20/robust-initialization-of-vins/#robust-initialization-of-vins","title":"Robust Initialization of VINS","text":"<p>T. Qin and S. Shen, \u201cRobust initialization of monocular visual-inertial estimation on aerial robots,\u201d in 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Vancouver, BC: IEEE, Sep. 2017, pp. 4225\u20134232. doi: 10.1109/IROS.2017.8206284.</p> <p>\u8fd9\u7bc7\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u89c6\u89c9\u60ef\u6027\u7cfb\u7edf\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u677e\u8026\u5408\u7684\u65b9\u5f0f\u5bf9\u9f50 IMU \u4e0e\u89c6\u89c9\u6570\u636e\uff0c\u4e3b\u8981\u89e3\u51b3\u4e86\u5916\u53c2\u6570\u6807\u5b9a\u3001\u9640\u87ba\u4eea\u504f\u7f6e\u4f30\u8ba1\u3001\u901f\u5ea6\u91cd\u529b\u5c3a\u5ea6\u548c\u89c6\u89c9\u60ef\u6027\u5bf9\u9f50\u7b49\u5173\u952e\u95ee\u9898\u3002</p> <p></p>","tags":["slam"]},{"location":"2024/12/20/robust-initialization-of-vins/#_1","title":"\u95ee\u9898\u5efa\u6a21","text":"<p>IMU \u6d4b\u91cf\u6a21\u578b\u6709</p> \\[ \\begin{aligned} \\hat{\\boldsymbol\\omega} &amp;= \\boldsymbol\\omega^b+\\mathbf b^g+\\mathbf n^g \\\\ \\hat{\\mathbf a} &amp;= \\mathbf R_{bw}(\\mathbf a^w+\\mathbf g^w)+\\mathbf b^a+\\mathbf n^a \\end{aligned} \\] <p>\u9884\u79ef\u5206</p> \\[ \\begin{aligned} \\boldsymbol\\alpha_{b_ib_j}&amp;=\\iint_{t \\in [i,j]} \\mathbf R_{b_ib_t} (\\hat{\\mathbf a}^{b_t}-\\mathbf b^a) {\\rm d} t^2\\\\ \\boldsymbol\\beta_{b_ib_j} &amp;= \\int_{t \\in [i,j]} \\mathbf R_{b_ib_t} (\\hat{\\mathbf a}^{b_t}-\\mathbf b^a) {\\rm d} t \\\\ \\mathbf q_{b_ib_j}&amp;= \\int_{t \\in [i,j]} \\mathbf q_{b_ib_t} \\otimes \\begin{bmatrix} 0 \\\\ \\frac12(\\hat{\\boldsymbol\\omega}^{b_t}-\\mathbf b^g) \\end{bmatrix} {\\rm d}t \\end{aligned} \\] <p>\u6709\u4e00\u9636\u6cf0\u52d2\u5c55\u5f00\u8fd1\u4f3c</p> \\[ \\begin{aligned} \\boldsymbol\\alpha_{b_ib_j} &amp;\\approx \\hat{\\boldsymbol\\alpha}_{b_ib_j} + \\mathbf J_{b^a}^\\alpha\\delta\\mathbf b^a + \\mathbf J_{b^g}^\\alpha\\delta\\mathbf b^g \\\\ \\boldsymbol\\beta_{b_ib_j} &amp;\\approx \\hat{\\boldsymbol\\beta}_{b_ib_j} + \\mathbf J_{b^a}^\\beta\\delta\\mathbf b^a + \\mathbf J_{b^g}^\\beta\\delta\\mathbf b^g \\\\ \\mathbf q_{b_kb_{k+1}} &amp;\\approx \\hat{\\mathbf q}_{b_kb_{k+1}}\\otimes\\begin{bmatrix}1\\\\\\frac12\\mathbf J_{b^g}^{\\mathbf q}\\delta\\mathbf b^g\\end{bmatrix} \\end{aligned} \\] <p>SfM \u7ed9\u51fa\u89c6\u89c9\u7ea6\u675f\uff0c\\(\\bar \u00b7\\) \u8868\u793a\u975e\u7c73\u5236\u5355\u4f4d</p> \\[ \\begin{aligned} \\mathbf q_{c_0b_k} &amp;= \\mathbf q_{c_0c_k} \\otimes \\mathbf q_{bc}^{-1} \\\\ s\\bar{\\mathbf p}_{c_0b_k} &amp;= s\\bar{\\mathbf p}_{c_0c_k} - \\mathbf R_{c_0b_k}\\mathbf p_{bc} \\end{aligned} \\]","tags":["slam"]},{"location":"2024/12/20/robust-initialization-of-vins/#mathbf-q_bc","title":"\u4f30\u8ba1\u5916\u53c2\u6570 \\(\\mathbf q_{bc}\\)","text":"<p>\u5bf9\u4e8e\u76f8\u90bb\u65f6\u523b \\(k,k+1\\) \u7684 IMU \u65cb\u8f6c\u79ef\u5206 \\(\\mathbf q_{b_kb_{k+1}}\\) \u548c\u89c6\u89c9\u6d4b\u91cf \\(\\mathbf q_{c_kc_{k+1}}\\)</p> \\[ \\mathbf q_{b_kc_{k+1}}=\\mathbf q_{bc}\\otimes\\mathbf q_{c_kc_{k+1}}=\\mathbf q_{b_kb_{k+1}}\\otimes\\mathbf q_{bc} \\] \\[ \\left([\\mathbf q_{c_kc_{k+1}}]_R - [\\mathbf q_{b_kb_{k+1}}]_L\\right)\\mathbf q_{bc}=\\mathbf0 \\] <p>\\([\u00b7]_L, [\u00b7]_R\\) \u4e3a\u56db\u5143\u6570\u7684\u5de6\u4e58\u77e9\u9635\u548c\u53f3\u4e58\u77e9\u9635\uff0c\\([\u00b7]_\\times\\) \u4e3a\u53cd\u5bf9\u79f0\u77e9\u9635</p> \\[ \\begin{aligned}\\ [\\mathbf q]_L &amp;=q_\\omega\\mathbf I+\\begin{bmatrix}0 &amp; -\\mathbf q_v^T \\\\ \\mathbf q_v &amp; [\\mathbf q_v]_\\times\\end{bmatrix} \\\\ [\\mathbf q]_R &amp;= q_\\omega\\mathbf I+\\begin{bmatrix}0 &amp; -\\mathbf q_v^T \\\\ \\mathbf q_v &amp; -[\\mathbf q_v]_\\times\\end{bmatrix} \\\\ [\\boldsymbol\\omega]_\\times &amp;= \\begin{bmatrix}0 &amp; -\\omega_z &amp; \\omega_y \\\\ \\omega_z &amp; 0 &amp; -\\omega_x \\\\ -\\omega_y &amp; \\omega_x &amp; 0\\end{bmatrix} \\end{aligned} \\] <p>\u5c06\u591a\u4e2a\u65f6\u523b\u7684 IMU \u9884\u79ef\u5206\u548c\u89c6\u89c9\u6d4b\u91cf\u7d2f\u8ba1\uff0c\u5373\u53ef\u5f97\u5230\u5173\u4e8e \\(\\mathbf q_{bc}\\) \u7684\u8d85\u5b9a\u65b9\u7a0b\u7ec4</p> \\[ \\begin{bmatrix} [\\mathbf q_{c_0c_1}]_R-[\\mathbf q_{b_0b_1}]_L \\\\ [\\mathbf q_{c_1c_2}]_R-[\\mathbf q_{b_1b_2}]_L \\\\ \\vdots \\\\ [\\mathbf q_{c_nc_{n-1}}]_R-[\\mathbf q_{b_nb_{n-1}}] \\\\ \\end{bmatrix} \\mathbf q_{bc} = \\mathbf 0 \\] <p>\u5229\u7528\u5947\u5f02\u503c\u5206\u89e3\u53ef\u89e3\u5f97 \\(\\mathbf q_{bc}\\)</p>","tags":["slam"]},{"location":"2024/12/20/robust-initialization-of-vins/#_2","title":"\u4f30\u8ba1\u9640\u87ba\u4eea\u504f\u7f6e","text":"<p>\u6807\u5b9a\u5f97\u5230 \\(\\mathbf q_{bc}\\) \u540e\uff0c\u5229\u7528\u65cb\u8f6c\u7ea6\u675f\uff0c\u53ef\u4f30\u8ba1\u5904\u9640\u87ba\u4eea\u504f\u7f6e</p> \\[ \\delta\\mathbf b^g = \\arg\\min_{\\delta\\mathbf b^g}\\sum_{k\\in{\\mathcal B}} \\left|\\left|\\left\\lfloor\\mathbf q_{c_0b_{k+1}}^{-1}\\otimes\\mathbf q_{c_0b_k} \\otimes\\mathbf q_{b_kb_{k+1}} \\right\\rfloor_{\\text{xyz}}\\right|\\right|^2 \\] <p>\\(\\mathcal B\\) \u4e3a\u6240\u6709\u5173\u952e\u5e27\u7684\u96c6\u5408\uff0c\u5176\u4e2d\u76ee\u6807\u51fd\u6570\u53d6\u6700\u5c0f\u503c \\(0\\) \u65f6</p> \\[ \\mathbf q_{c_0b_{k+1}}^{-1}\\otimes\\mathbf q_{c_0b_k} \\otimes\\mathbf q_{b_kb_{k+1}}=\\begin{bmatrix}1\\\\\\mathbf0\\end{bmatrix} \\] <p>\u53c8\u7531\u65cb\u8f6c\u9884\u79ef\u5206\u4e00\u9636\u6cf0\u52d2\u8fd1\u4f3c</p> \\[ \\mathbf q_{b_kb_{k+1}} \\approx \\hat{\\mathbf q}_{b_kb_{k+1}}\\otimes\\begin{bmatrix}1\\\\\\frac12\\mathbf J_{b^g}^{\\mathbf q}\\delta\\mathbf b^g\\end{bmatrix} \\] <p>\u8054\u7acb\u5f97\u5230</p> \\[ \\begin{bmatrix}1\\\\\\frac12\\mathbf J_{b^g}^{\\mathbf q}\\delta\\mathbf b^g\\end{bmatrix} = \\hat{\\mathbf q}_{b_kb_{k+1}}^{-1}\\otimes\\mathbf q_{c_0b_k}^{-1}\\otimes\\mathbf q_{c_0b_{k+1}} \\] <p>\u8003\u8651\u865a\u90e8</p> \\[ \\mathbf J_{b^g}^{\\mathbf q}\\delta\\mathbf b^g = 2\\left\\lfloor\\hat{\\mathbf q}_{b_kb_{k+1}}^{-1}\\otimes\\mathbf q_{c_0b_k}^{-1}\\otimes\\mathbf q_{c_0b_{k+1}}\\right\\rfloor_{\\text{xyz}} \\] <p>\u8fdb\u4e00\u6b65\u53ef\u4ee5\u6784\u5efa\u6b63\u5b9a\u65b9\u7a0b\u7ec4\uff0c\u901a\u8fc7 Cholesky \u5206\u89e3\u6c42\u89e3 \\(\\delta\\mathbf b^g\\)</p> \\[ (\\mathbf J_{b^g}^{\\mathbf q})^T\\mathbf J_{b^g}^{\\mathbf q}\\delta\\mathbf b^g = 2(\\mathbf J_{b^g}^{\\mathbf q})^T\\left\\lfloor\\hat{\\mathbf q}_{b_kb_{k+1}}^{-1}\\otimes\\mathbf q_{c_0b_k}^{-1}\\otimes\\mathbf q_{c_0b_{k+1}}\\right\\rfloor_{\\text{xyz}} \\] <p>\u89e3\u5f97 \\(\\delta\\mathbf b^g\\) \u540e\uff0c\u91cd\u65b0\u8ba1\u7b97\u9884\u79ef\u5206\u9879 \\(\\hat{\\boldsymbol\\alpha}_{b_kb_k+1}, \\hat{\\boldsymbol\\beta}_{b_kb_{k+1}},\\hat{\\mathbf q}_{b_kb_{k+1}}\\)</p>","tags":["slam"]},{"location":"2024/12/20/robust-initialization-of-vins/#_3","title":"\u521d\u59cb\u5316\u901f\u5ea6\u3001\u91cd\u529b\u548c\u5c3a\u5ea6\u56e0\u5b50","text":"<p>\u6240\u6709\u6211\u4eec\u5e0c\u671b\u4f30\u8ba1\u7684\u53d8\u91cf\u5305\u62ec</p> \\[ \\mathcal X_I=\\begin{bmatrix}\\mathbf v_0^{b_0} &amp; \\mathbf v_1^{b_1} &amp; \\cdots \\mathbf v_n^{b_n} &amp; \\mathbf g^{c_0} &amp; \\mathbf s\\end{bmatrix}^T \\] <p>\u7531 \\(1.3\\) \u5f0f\u548c \\(2.1\\) \u5f0f\uff0c\u5f97\u5230</p> \\[ \\begin{aligned} &amp; \\boldsymbol\\alpha_{b_kb_{k+1}} = \\mathbf R_{b_kc_0} \\left( s(\\bar{\\mathbf p}_{c_0b_{k+1}} - \\bar{\\mathbf p}_{c_0b_k}) + \\frac12\\mathbf g^{c_0}\\Delta t_k^2 - \\mathbf R_{c_0b_k}\\mathbf v_{k}^{b_k} \\Delta t_k \\right) \\\\ &amp; \\boldsymbol\\beta_{b_kb_{k+1}} = \\mathbf R_{b_kc_0} (\\mathbf R_{c_0b_{k+1}}\\mathbf v_k^{b_k}+\\mathbf g^{c_0}\\Delta t_k - \\mathbf R_{c_0b_k}\\mathbf v_k^{b_k}) \\end{aligned} \\] <p>\u6574\u7406\u65b9\u7a0b\u5f97\u5230</p> \\[ \\hat{\\mathbf z}_{b_{k+1}}^{b_k} = \\begin{bmatrix} \\hat{\\boldsymbol\\alpha}_{b_kb_{k+1}} - \\mathbf p_{bc} + \\mathbf R_{b_kc_0}\\mathbf R_{c_0b_{k+1}}\\mathbf p_{bc} \\\\ \\hat{\\boldsymbol\\beta}_{b_kb_{k+1}} \\end{bmatrix} = \\mathbf H^k\\mathcal X_I^k+\\mathbf n^k \\] <p>\u5176\u4e2d</p> \\[ \\begin{aligned} \\mathcal X_I^k &amp;= \\begin{bmatrix} \\mathbf v^{b_k}_k &amp; \\mathbf v^{b_{k+1}}_{k+1} &amp; \\mathbf g^{c_0} &amp; s \\end{bmatrix}^T \\\\ \\mathbf H_{b_{k+1}}^{b_k} &amp;= \\begin{bmatrix} -\\mathbf I\\Delta t_k &amp; \\mathbf0 &amp; \\frac12\\mathbf R_{b_kc_0}\\Delta t_k^2 &amp; \\mathbf R_{b_kc_0}(\\bar{\\mathbf p}_{c_0b_{k+1}} - \\bar{\\mathbf p}_{c_0b_k}) \\\\ -\\mathbf I &amp; \\mathbf R_{b_kc_0}\\mathbf R_{c_0b_{k+1}} &amp; \\mathbf R_{b_kc_0}\\Delta t_k &amp; 0 \\end{bmatrix} \\end{aligned} \\] <p>\u8fdb\u800c\u53ef\u4ee5\u8f6c\u5316\u4e3a\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u6c42\u89e3</p> \\[ \\mathcal X_I=\\arg\\min_{\\mathcal X_I}\\sum_{k\\in\\mathcal B}||\\hat{\\mathbf z}_{b_{k+1}}^{b_k} - \\mathbf H_{b_{k+1}}^{b_k}\\mathcal X_I^k||^2 \\] <p>\u540c\u6837\u53ef\u4ee5\u901a\u8fc7 Chologky \u5206\u89e3\u6c42\u5f97</p>","tags":["slam"]},{"location":"2024/12/20/robust-initialization-of-vins/#_4","title":"\u91cd\u529b\u5411\u91cf\u4f18\u5316","text":"<p>\u5728\u91cd\u529b\u6a21\u957f\u5df2\u77e5\u7684\u60c5\u51b5\u4e0b\uff0c\u91cd\u529b\u5411\u91cf\u5b9e\u9645\u81ea\u7531\u5ea6\u4e3a \\(2\\)\uff0c\u53ef\u4ee5\u5229\u7528\u7403\u9762\u5750\u6807\u8fdb\u884c\u53c2\u6570\u5316</p> \\[ \\hat{\\mathbf g}^{c_0}=||g||\\bigg(\\frac{\\tilde{\\mathbf g}^{c_0}}{||\\tilde{\\mathbf g}^{c_0}||}+w_1\\mathbf b_1+w_2\\mathbf b_2\\bigg) \\] <p>\\(\\tilde{\\mathbf g}^{c_0}\\) \u4e3a \\(2.15\\) \u4e2d\u6c42\u5f97\u7684\u91cd\u529b\u5411\u91cf\uff0c\u8bb0 \\(\\frac{\\tilde{\\mathbf g}^{c_0}}{||\\tilde{\\mathbf g}^{c_0}||}\\) \u4e3a \\(\\hat{\\bar{\\mathbf g}}^{c_0}\\)</p> <p></p> <p>\u53ef\u4ee5\u901a\u8fc7\u5982\u4e0b\u65b9\u5f0f\u627e\u5230\u4e00\u7ec4\u57fa\u5e95\u5782\u76f4\u4e8e \\(\\hat{\\bar{\\mathbf g}}^{c_0}\\)</p> \\[ \\begin{aligned} &amp; \\mathbf b_1 = \\begin{cases} \\hat{\\bar{\\mathbf g}}^{c_0}\\times[1,0,0]^T, &amp;\\hat{\\bar{\\mathbf g}}^{c_0}\\ne[1,0,0]^T \\\\ \\hat{\\bar{\\mathbf g}}^{c_0}\\times[0,0,1]^T, &amp;\\text{otherwise} \\end{cases} \\\\ &amp; \\mathbf b_2 = \\hat{\\bar{\\mathbf g}}^{c_0}\\times \\mathbf b_1 \\end{aligned} \\] <p>\u5c06 \\(2.17\\) \u5f0f\u4ee3\u5165 \\(2.15\\) \u5f0f\uff0c\u5f97\u5230</p> \\[ \\begin{aligned} \\mathcal X_I^k &amp;= \\begin{bmatrix} \\mathbf v^{b_k}_k &amp; \\mathbf v^{b_{k+1}}_{k+1} &amp; \\mathbf w^{c_0} &amp; s \\end{bmatrix}^T \\\\ \\mathbf H_{b_{k+1}}^{b_k} &amp;= \\begin{bmatrix} -\\mathbf I\\Delta t_k &amp; \\mathbf0 &amp; \\frac12\\mathbf R_{b_kc_0}\\begin{bmatrix}\\mathbf b_1^T\\\\\\mathbf b_2^T\\end{bmatrix}\\Delta t_k^2 &amp; \\mathbf R_{b_kc_0}(\\bar{\\mathbf p}_{c_0b_{k+1}} - \\bar{\\mathbf p}_{c_0b_k}) \\\\ -\\mathbf I &amp; \\mathbf R_{b_kc_0}\\mathbf R_{c_0b_{k+1}} &amp; \\mathbf R_{b_kc_0}\\begin{bmatrix}\\mathbf b_1^T\\\\\\mathbf b_2^T\\end{bmatrix}\\Delta t_k &amp; 0 \\end{bmatrix} \\end{aligned} \\] <p>\u89c2\u6d4b\u65b9\u7a0b\u53d8\u4e3a</p> \\[ \\hat{\\mathbf z}_{b_{k+1}}^{b_k} = \\begin{bmatrix} \\hat{\\boldsymbol\\alpha}_{b_kb_{k+1}} - \\mathbf p_{bc} + \\mathbf R_{b_kc_0}\\mathbf R_{c_0b_{k+1}}\\mathbf p_{bc}-\\frac12\\mathbf R_{b_kc_0}\\tilde{\\mathbf g}^{c_0}\\Delta t_k^2 \\\\ \\hat{\\boldsymbol\\beta}_{b_kb_{k+1}} -\\mathbf R_{b_kc_0}\\tilde{\\mathbf g}^{c_0}\\Delta t_k \\end{bmatrix} \\] <p>\u5229\u7528\u6700\u5c0f\u4e8c\u4e58\u5bf9 \\(\\mathcal X_I^k\\) \u8fdb\u4e00\u6b65\u4f18\u5316</p>","tags":["slam"]},{"location":"2024/12/20/robust-initialization-of-vins/#_5","title":"\u89c6\u89c9\u60ef\u6027\u5bf9\u9f50","text":"<p>\u6839\u636e\u65cb\u8f6c\u7684\u6027\u8d28\u548c\u674e\u4ee3\u6570\u7684\u6307\u6570\u6620\u5c04\uff0c\u6211\u4eec\u53ef\u4ee5\u6784\u5efa\u4ece \\(c_0\\) \u7cfb\u5230 \\(w\\) \u7cfb\u7684\u65cb\u8f6c\u77e9\u9635 \\(\\mathbf R_{wc_0}\\)</p> \\[ \\mathbf R_{wc_0}=\\exp\\bigg[\\arctan\\left(\\frac{||\\hat{\\mathbf g}^{c_0}\\times\\hat{\\mathbf g}^{w}||}{\\hat{\\mathbf g}^{c_0}\\cdot\\hat{\\mathbf g}^{w}}\\right)\\cdot\\frac{\\hat{\\mathbf g}^{c_0}\\times\\hat{\\mathbf g}^{w}}{||\\hat{\\mathbf g}^{c_0}\\times\\hat{\\mathbf g}^{w}||}\\bigg] \\] <p>\u63a5\u7740\u4e3a\u6240\u6709 \\(c_0\\) \u7cfb\u4e3a\u5750\u6807\u7cfb\u7684\u5411\u91cf\u5de6\u4e58 \\(\\mathbf R_{wc_0}\\)\uff0c\u540c\u65f6\u5c06\u975e\u7c73\u5236\u7684 \\(\\bar{\\mathbf p}\\) \u901a\u8fc7\u5c3a\u5ea6\u56e0\u5b50 \\(s\\) \u6062\u590d\u4e3a \\(\\mathbf p\\)</p>","tags":["slam"]},{"location":"2024/12/20/robust-initialization-of-vins/#_6","title":"\u672a\u4f30\u8ba1\u7684\u53c2\u6570","text":"<p>\u4f5c\u8005\u901a\u8fc7\u5b9e\u9a8c\u6307\u51fa\u4e8c\u8005\u52a0\u901f\u5ea6\u8ba1\u504f\u7f6e \\(\\mathbf b^a\\) \u548c\u76f8\u673a\u4e0e IMU \u95f4\u7684\u5e73\u79fb\u5411\u91cf \\(\\mathbf p_{bc}\\) \u5bf9\u7cfb\u7edf\u7cbe\u5ea6\u5f71\u54cd\u6781\u5c0f\uff0c\u53ef\u4ee5\u4e0d\u5728\u521d\u59cb\u5316\u4e2d\u663e\u5f0f\u4f18\u5316</p> <p></p>","tags":["slam"]},{"location":"2025/11/08/%E4%BB%8E%E7%B3%BB%E7%BB%9F%E4%B9%8B%E7%BE%8E%E5%88%B0%E7%BB%8F%E5%85%B8%E6%8E%A7%E5%88%B6%E8%AE%BA/","title":"\u4ece\u300a\u7cfb\u7edf\u4e4b\u7f8e\u300b\u5230\u7ecf\u5178\u63a7\u5236\u8bba","text":"","tags":["math","control"]},{"location":"2025/11/08/%E4%BB%8E%E7%B3%BB%E7%BB%9F%E4%B9%8B%E7%BE%8E%E5%88%B0%E7%BB%8F%E5%85%B8%E6%8E%A7%E5%88%B6%E8%AE%BA/#_1","title":"\u4ece\u300a\u7cfb\u7edf\u4e4b\u7f8e\u300b\u5230\u7ecf\u5178\u63a7\u5236\u8bba","text":"<p>\u5f53\u6211\u4eec\u8c08\u8bba\u590d\u6742\u7cfb\u7edf\u65f6\uff0c\u5fb7\u5185\u62c9\u00b7\u6885\u591a\u65af\u7684\u300a\u7cfb\u7edf\u4e4b\u7f8e\u300b\u63d0\u4f9b\u4e86\u4e00\u5957\u7406\u89e3\u52a8\u6001\u95ee\u9898\u7684\u8bed\u6cd5\uff0c\u800c\u7ecf\u5178\u63a7\u5236\u8bba\u5219\u8d4b\u4e88\u6211\u4eec\u5206\u6790\u548c\u8bbe\u8ba1\u7cfb\u7edf\u7684\u6570\u5b66\u8bed\u8a00\u3002\u8fd9\u4e24\u8005\u7ed3\u5408\uff0c\u5f62\u6210\u4e86\u4e00\u6761\u4ece\u7406\u89e3\u5230\u9a7e\u9a6d\u52a8\u6001\u7cfb\u7edf\u7684\u5b8c\u6574\u8def\u5f84\u3002</p> <p>\u300a\u7cfb\u7edf\u4e4b\u7f8e\u300b\u4e2d\u63cf\u8ff0\u7684\u5404\u79cd\u7cfb\u7edf\u57fa\u6a21\uff0c\u5982\u6210\u957f\u4e0a\u9650\u3001\u76ee\u6807\u4fb5\u8680\u3001\u8f6c\u79fb\u8d1f\u62c5\u7b49\uff0c\u672c\u8d28\u4e0a\u662f\u5728 \u5b9a\u6027\u63cf\u8ff0\u4e00\u7c7b\u666e\u904d\u5b58\u5728\u7684\u52a8\u6001\u7cfb\u7edf\u7ed3\u6784\u3002\u8fd9\u4e9b\u57fa\u6a21\u5c31\u50cf\u662f\u63a7\u5236\u8bba\u4e2d \u7cfb\u7edf\u6846\u56fe \u548c \u4f20\u9012\u51fd\u6570 \u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u751f\u52a8\u539f\u578b\u3002\u5f53\u6211\u4eec\u80fd\u591f\u8bc6\u522b\u51fa\u8fd9\u4e9b\u57fa\u6a21\uff0c\u5c31\u76f8\u5f53\u4e8e\u5728\u590d\u6742\u6027\u95ee\u9898\u4e2d\u5feb\u901f\u5b9a\u4f4d\u5230\u4e86\u7cfb\u7edf\u7684\u6838\u5fc3\u7ed3\u6784\uff0c\u4e3a\u540e\u7eed\u7684\u5b9a\u91cf\u5206\u6790\u6307\u660e\u4e86\u65b9\u5411\u3002</p> <p>\u5728\u63a7\u5236\u8bba\u4e2d\uff0c\u72b6\u6001\u53d8\u91cf \\(\\mathbf{x}(t)\\) \u7684\u96c6\u5408\u5b8c\u6574\u5730\u63cf\u8ff0\u4e86\u7cfb\u7edf\u5728\u4efb\u610f\u65f6\u523b\u7684\u52a8\u6001\u72b6\u51b5\uff0c\u8fd9\u4e0e \u5b58\u91cf \u7684\u6982\u5ff5\u5b8c\u7f8e\u5bf9\u5e94\u3002\u65e0\u8bba\u662f\u6d74\u7f38\u7684\u6c34\u4f4d\u3001\u516c\u53f8\u7684\u5e93\u5b58\uff0c\u8fd8\u662f\u4e2a\u4eba\u7684\u77e5\u8bc6\u50a8\u5907\uff0c\u5b83\u4eec\u90fd\u662f\u7cfb\u7edf\u7684\u72b6\u6001\uff0c\u662f\u8fc7\u53bb\u6240\u6709\u6d41\u5165\u548c\u6d41\u51fa\u6d41\u91cf\u7684\u79ef\u5206\u7ed3\u679c\u3002\u7406\u89e3\u4e00\u4e2a\u7cfb\u7edf\uff0c\u59cb\u4e8e\u5b9a\u4e49\u5176\u5173\u952e\u72b6\u6001\u53d8\u91cf\u3002</p> <p>\u6d41\u91cf \u4f5c\u4e3a\u6539\u53d8\u5b58\u91cf\u7684\u901f\u7387\uff0c\u5728\u63a7\u5236\u8bba\u4e2d\u626e\u6f14\u7740 \u8f93\u5165\u4fe1\u53f7 \\(\\mathbf{u}(t)\\) \u7684\u89d2\u8272\u3002\u6211\u4eec\u80fd\u591f\u4e3b\u52a8\u8c03\u8282\u7684\u6d41\u91cf\uff0c\u5982\u6295\u8d44\u989d\u3001\u5b66\u4e60\u5f3a\u5ea6\uff0c\u5bf9\u5e94\u7740\u63a7\u5236\u8f93\u5165\uff1b\u800c\u6765\u81ea\u5916\u90e8\u3001\u65e0\u6cd5\u76f4\u63a5\u63a7\u5236\u4f46\u5f71\u54cd\u7cfb\u7edf\u7684\u6d41\u91cf\uff0c\u5982\u5e02\u573a\u6ce2\u52a8\u3001\u610f\u5916\u652f\u51fa\uff0c\u5219\u5bf9\u5e94\u7740\u6270\u52a8\u8f93\u5165\u3002\u7cfb\u7edf\u7684\u884c\u4e3a\uff0c\u6b63\u662f\u8fd9\u4e9b\u8f93\u5165\u4fe1\u53f7\u4f5c\u7528\u4e8e\u7cfb\u7edf\u52a8\u529b\u5b66\u6240\u4ea7\u751f\u7684\u7ed3\u679c\u3002</p> <p>\u7cfb\u7edf\u7684\u52a8\u6001\u6f14\u5316\u7531\u72b6\u6001\u7a7a\u95f4\u65b9\u7a0b\u63cf\u8ff0 \\(\\dot{\\mathbf{x}}(t)=f[\\mathbf{x}(t),\\mathbf{u}(t),t]\\)\u3002\u5728\u7ebf\u6027\u65f6\u4e0d\u53d8\u7cfb\u7edf\u4e2d\uff0c\u7b80\u5316\u4e3a \\(\\dot{\\mathbf{x}}(t)=A\\mathbf{x}(t)+B\\mathbf{u}(t)\\)\uff0c\u5176\u4e2d\\(A\\)\u77e9\u9635\u63cf\u8ff0\u4e86\u5b58\u91cf\u95f4\u7684\u5185\u90e8\u76f8\u4e92\u4f5c\u7528\uff0cB \u77e9\u9635\u53cd\u6620\u4e86\u8f93\u5165\u5bf9\u5b58\u91cf\u53d8\u5316\u7684\u5f71\u54cd\u3002\u6709\u65f6\u5b58\u91cf\u65e0\u6cd5\u76f4\u63a5\u5f97\u5230\uff0c\u9700\u8981\u901a\u8fc7\u89c2\u6d4b \\(\\mathbf{y}(t)=g[\\mathbf{x}(t),t]\\) \u95f4\u63a5\u6d4b\u91cf\u3002</p> <p>\u53cd\u9988\u56de\u8def\u662f\u4e24\u8005\u4ea4\u6c47\u7684\u6838\u5fc3\u5730\u5e26\u3002\u300a\u7cfb\u7edf\u4e4b\u7f8e\u300b\u4e2d\u7684 \u8c03\u8282\u56de\u8def \u5bf9\u5e94\u7740\u63a7\u5236\u8bba\u7684 \u8d1f\u53cd\u9988\u95ed\u73af\uff0c\u5176\u76ee\u6807\u662f\u4f7f\u7cfb\u7edf\u8f93\u51fa\u8ddf\u8e2a\u53c2\u8003\u4fe1\u53f7\u3002\u7cfb\u7edf\u4e2d\u7684\u5dee\u8ddd\u5c31\u662f\u8bef\u5dee\u4fe1\u53f7\uff0c\u63a7\u5236\u5668\u6839\u636e\u8bef\u5dee\u4ea7\u751f\u63a7\u5236\u52a8\u4f5c\uff0c\u4ee5\u6291\u5236\u6270\u52a8\u3001\u7ef4\u6301\u7a33\u5b9a\u3002\u800c \u589e\u5f3a\u56de\u8def \u5219\u5bf9\u5e94\u6b63\u53cd\u9988\u95ed\u73af\uff0c\u5176\u7279\u70b9\u662f\u8bef\u5dee\u88ab\u4e0d\u65ad\u653e\u5927\uff0c\u5bfc\u81f4\u7cfb\u7edf\u8f93\u51fa\u6307\u6570\u7ea7\u589e\u957f\u6216\u5d29\u6e83\u3002</p> <p>\u300a\u7cfb\u7edf\u4e4b\u7f8e\u300b\u5f3a\u8c03\u7684 \u5ef6\u8fdf \u662f\u7cfb\u7edf\u52a8\u6001\u590d\u6742\u6027\u7684\u4e3b\u8981\u6765\u6e90\uff0c\u5728\u63a7\u5236\u8bba\u4e2d\u8fd9\u5bf9\u5e94\u7740\u7cfb\u7edf\u7684 \u52a8\u6001\u7279\u6027\uff0c\u5982\u65f6\u95f4\u5e38\u6570\u548c\u7eaf\u6ede\u540e\u3002\u5ef6\u8fdf\u4f1a\u5bfc\u81f4\u4fe1\u606f\u53cd\u9988\u4e0d\u53ca\u65f6\uff0c\u4f7f\u8d1f\u53cd\u9988\u7cfb\u7edf\u4ea7\u751f\u632f\u8361\uff0c\u6216\u8ba9\u6b63\u53cd\u9988\u7cfb\u7edf\u5728\u4e0d\u77e5\u4e0d\u89c9\u4e2d\u6ed1\u5411\u6df1\u6e0a\u3002\u5206\u6790\u7cfb\u7edf\u7684\u9891\u7387\u54cd\u5e94\u6216\u9636\u8dc3\u54cd\u5e94\uff0c\u6b63\u662f\u4e3a\u4e86\u91cf\u5316\u5e76\u7406\u89e3\u8fd9\u79cd\u5ef6\u8fdf\u6548\u5e94\u3002</p> <p>\u5bfb\u627e\u6760\u6746\u70b9 \u7684\u8fc7\u7a0b\uff0c\u672c\u8d28\u4e0a\u662f\u5728\u8fdb\u884c \u63a7\u5236\u5668\u8bbe\u8ba1 \u548c\u6781\u70b9\u914d\u7f6e\u3002\u6539\u53d8\u7cfb\u7edf\u7684\u76ee\u6807\u6216\u8303\u5f0f\uff0c\u76f8\u5f53\u4e8e\u91cd\u65b0\u8bbe\u5b9a\u4e86\u6574\u4e2a\u63a7\u5236\u7cfb\u7edf\u7684\u53c2\u8003\u8f93\u5165\uff1b\u91cd\u5851\u7cfb\u7edf\u7ed3\u6784\uff0c\u76f8\u5f53\u4e8e\u4fee\u6539\u4e86\u7cfb\u7edf\u672c\u8eab\u7684\u7279\u6027\uff1b\u800c\u8c03\u6574\u53c2\u6570\uff0c\u4ec5\u4ec5\u662f\u5728\u4f18\u5316\u63a7\u5236\u5668\u589e\u76ca\u3002\u63a7\u5236\u7406\u8bba\u544a\u8bc9\u6211\u4eec\uff0c\u6700\u9ad8\u6548\u7684\u5e72\u9884\u6765\u81ea\u4e8e\u6539\u53d8\u7cfb\u7edf\u672c\u8eab\u7684\u52a8\u6001\u7279\u6027\uff0c\u8fd9\u4e5f\u6b63\u5bf9\u5e94\u7740\u300a\u7cfb\u7edf\u4e4b\u7f8e\u300b\u4e2d\u6392\u540d\u9760\u524d\u7684\u9ad8\u9636\u6760\u6746\u70b9\u3002</p> <p>\u901a\u8fc7\u8fd9\u4e2a\u878d\u5408\u6846\u67b6\uff0c\u6211\u4eec\u83b7\u5f97\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u601d\u7ef4\u5de5\u5177\uff1a\u65e2\u80fd\u7528\u300a\u7cfb\u7edf\u4e4b\u7f8e\u300b\u7684\u900f\u955c\u4e00\u773c\u770b\u7a7f\u590d\u6742\u95ee\u9898\u7684\u7ed3\u6784\uff0c\u53c8\u80fd\u8c03\u7528\u63a7\u5236\u8bba\u7684\u5de5\u5177\u7bb1\u5bf9\u5176\u8fdb\u884c\u5b9a\u91cf\u5206\u6790\u548c\u7cbe\u51c6\u5e72\u9884\u3002\u8fd9\u4f7f\u6211\u4eec\u7684\u51b3\u7b56\u4ece\u4e00\u95e8\u827a\u672f\u8d70\u5411\u4e00\u95e8\u79d1\u5b66\uff0c\u8ba9\u6211\u4eec\u4ece\u7cfb\u7edf\u7684\u88ab\u52a8\u89c2\u5bdf\u8005\uff0c\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u7684\u7cfb\u7edf\u67b6\u6784\u5e08\u3002</p>","tags":["math","control"]},{"location":"2025/04/12/optimal-estimation-theory/","title":"Optimal Estimation Theory","text":"","tags":["hkust","estimation"]},{"location":"2025/04/12/optimal-estimation-theory/#optimal-estimation-theory","title":"Optimal Estimation Theory","text":"<p>This is the lecture notes for \"ELEC 5650: Networked Sensing, Estimation and Control\" in the 2024-25 Spring semester, delivered by Prof. Ling Shi at HKUST. In this session, we will explore maximum a posteriori (MAP) estimation, minimum mean squared error (MMSE) estimation, maximum likelihood (ML) estimation, weighted least squares estimation, and linear minimum mean square error (LMMSE) estimation.</p>","tags":["hkust","estimation"]},{"location":"2025/04/12/optimal-estimation-theory/#map-maximum-a-posterior-estimation","title":"MAP (Maximum A Posterior) Estimation","text":"<p>\\(x\\) is the parameter to be estimated</p> \\[ \\hat x=\\arg\\max_x \\begin{cases} f(x|y),&amp;x\\text{ is continuous} \\\\ p(x|y),&amp;x\\text{ is discrete} \\\\ \\end{cases} \\]","tags":["hkust","estimation"]},{"location":"2025/04/12/optimal-estimation-theory/#mmse-minimum-mean-squared-error-estimation","title":"MMSE (Minimum Mean Squared Error) Estimation","text":"\\[ \\hat x=\\arg\\min_{\\hat x}E[e^Te|y]=\\arg\\min_{\\hat x}E[\\hat x|y],\\ e=x-\\hat x \\] \\[ \\hat x=\\int x\\cdot f(x|y)\\ {\\rm d}x\\quad\\text{or}\\quad\\sum x\\cdot p(x|y) \\] <p>Proof:</p> \\[ \\begin{aligned} E[e^Te] &amp;= E[(x-\\hat x)^T(x-\\hat x)|y] \\\\ &amp;= E[x^Tx|y] - 2\\hat x^TE[x|y]+\\hat x^T\\hat x \\\\ \\end{aligned} \\] \\[ \\frac\\partial{\\partial\\hat x}(E[x^Tx|y] - 2\\hat x^TE[x|y]+\\hat x^T\\hat x)=0 \\] \\[ -2E[X|Y]+2\\hat x=0 \\] \\[ \\hat x_\\text{MMSE} = E[X|Y] \\]","tags":["hkust","estimation"]},{"location":"2025/04/12/optimal-estimation-theory/#ml-maximum-likelihood-estimation","title":"ML (Maximum Likelihood) Estimation","text":"<p>Non Bayesian. \\(p(y|x)\\) is conditional probability and \\(p(y;x)\\) is parameterized probability, \\(p(y|x)\\not\\Leftrightarrow p(y;x)\\).</p> <p>Assume we have \\(n\\) measurements \\(\\mathcal X=(X_1,\\cdots,X_n)\\), we use \\(p(\\mathcal X;\\theta)\\) to describe the joint probability of \\(\\mathcal X\\).</p> \\[ \\hat\\theta_n=\\arg\\max_\\theta\\begin{cases} f(\\mathcal X;\\theta),&amp;\\theta\\text{ is continuous} \\\\ p(\\mathcal X;\\theta),&amp;\\theta\\text{ is discrete} \\\\ \\end{cases} \\] \\[ p(\\mathcal X;\\theta)=\\prod_{i=1}^np(X_i;\\theta)\\quad\\Leftrightarrow\\quad\\log p(\\mathcal X;\\theta)=\\sum_{i=1}^n\\log p(X_i;\\theta) \\]","tags":["hkust","estimation"]},{"location":"2025/04/12/optimal-estimation-theory/#map-ml","title":"MAP &amp; ML","text":"\\[ \\begin{aligned} \\hat\\theta_\\text{MAP} &amp;= \\arg\\max_{\\theta} p(\\theta|x)=\\arg\\max_\\theta\\frac{p(\\theta)p(x|\\theta)}{p(x)}p(\\theta|x)=\\arg\\max_\\theta p(\\theta)p(x|\\theta) \\\\ \\hat\\theta_\\text{ML} &amp;= \\arg\\max_\\theta p(x;\\theta) \\end{aligned} \\]","tags":["hkust","estimation"]},{"location":"2025/04/12/optimal-estimation-theory/#weighted-least-square-estimation","title":"Weighted Least Square Estimation","text":"\\[ \\mathcal E(x) = ||Ax-b||^2_\\Sigma=x^TA^T\\Sigma^{-1}Ax-2b^T\\Sigma^{-1}Ax+b^T\\Sigma^{-1}b \\] \\[ \\nabla\\mathcal E = 2A^T\\Sigma^{-1}Ax - 2A^T\\Sigma^{-1}b \\] \\[ \\hat x = (A^T\\Sigma^{-1}A)^{-1}A^T\\Sigma^{-1}b \\]","tags":["hkust","estimation"]},{"location":"2025/04/12/optimal-estimation-theory/#lmmse-linear-minimum-mean-square-error-estimation","title":"LMMSE (Linear Minimum Mean Square Error) Estimation","text":"<p>LMMSE estimation wants to find a linear estimator</p> \\[ \\hat x=Ky+b \\] <p>such that minimize the mean square error</p> \\[ \\begin{aligned} \\text{MSE} &amp;= E[(x-\\hat x)^T(x-\\hat x)] \\\\ &amp;= E[x^Tx]-2E[x^T\\hat x]+E[\\hat x^T\\hat x] \\\\ &amp;= E[x^Tx]-2E[x^T(Ky+b)]+E[(Ky+b)^T(Ky+b)] \\\\ \\end{aligned} \\] \\[ \\frac{\\partial\\text{MSE}}{\\partial b} = -2E[x]+2b+2KE[y] = 0 \\] \\[ b=\\mu_x-K\\mu_y \\] \\[ \\begin{aligned} \\text{MSE} &amp;= E[x^Tx]-2E[x^T(Ky+b)]+E[(Ky+b)^T(Ky+b)] \\\\ &amp;= E[x^Tx]-2E[x^T(Ky+\\mu_x-K\\mu_y)]+E[(Ky+\\mu_x-K\\mu_y)^T(Ky+\\mu_x-K\\mu_y)] \\\\ &amp;= E[x^Tx]-2E[x^TKy]-2\\mu_x^T\\mu_x+2\\mu_x^TK\\mu_y+E[y^TK^TKy]+\\mu_x^T\\mu_x-2\\mu_x^TK\\mu_y+\\mu_y^TK^TK\\mu_y \\end{aligned} \\] \\[ \\frac{\\partial\\text{MSE}}{\\partial K} = -2\\Sigma_{xy}+2K\\Sigma_{yy}=0 \\] \\[ K=\\Sigma_{xy}\\Sigma_{yy}^{-1} \\] \\[ \\hat x=Ky+b=\\mu_x+\\Sigma_{xy}\\Sigma_{yy}^{-1}(y-\\mu_y) \\] \\[ \\Sigma_{\\hat x\\hat x} = \\Sigma_{xx}-\\Sigma_{xy}\\Sigma_{yy}^{-1}\\Sigma_{yx} \\]","tags":["hkust","estimation"]},{"location":"2025/04/12/optimal-estimation-theory/#orthogonality-principle","title":"Orthogonality Principle","text":"\\[ \\begin{aligned} \\langle x-Ky-b,y\\rangle &amp;= E[(x-Ky-b)y^T] \\\\ &amp;= E[xy^T]-KE[yy^T]-bE[y^T] \\\\ &amp;= \\Sigma_{xy}+\\mu_x\\mu_y^T-K(\\Sigma_{yy}+\\mu_y\\mu_y^T)-(\\mu_x-K\\mu_y)\\mu_y^T \\\\ &amp;= \\Sigma_{xy} - (\\Sigma_{xy}\\Sigma_{yy}^{-1})\\Sigma_{yy} \\\\ &amp;=0 \\end{aligned} \\] \\[ x-(Ky+b)\\perp y \\] <p>This shows that error \\(e=x-\\hat x\\) is independent of observation \\(y\\).</p>","tags":["hkust","estimation"]},{"location":"2025/04/12/optimal-estimation-theory/#innovation-process","title":"Innovation Process","text":"<p>Calculating \\(\\Sigma_{yy}\\) consumes lots of time, however, if \\(\\Sigma_{yy}\\) is diagonal the thing becomes easy. By G.S. process, we can obtain orthogonality vectors \\(\\vec e_1,\\cdots\\vec e_k\\) and the lower triangular transform matrix \\(F\\) from \\(\\vec y_1,\\cdots,\\vec y_k\\). The key idea of \u200b\u200borthogonal projection is to decompose the observation vector \\(y_k\\) into a part related to the past prediction value, which can be predicted by \\(y_1,\\cdots y_{k-1}\\), and a new part that is irrelevant to the past prediction value (innovation).</p> \\[ e=Fy \\] <p>Then the covariance can be calculated by</p> \\[ \\Sigma_{ee}=F\\Sigma_{yy}F^T,\\quad\\Sigma_{ex}=F\\Sigma_{yx} \\] \\[ K_e=\\Sigma_{ex}\\Sigma_{ee}^{-1}=F\\Sigma_{yx}(F^T)^{-1}\\Sigma_{yy}^{-1}F^{-1} \\] <p>Although \\(K_e\\) is not equal to \\(K\\), it serves as the Kalman gain in the transformed or projected space defined by the matrix \\(F\\).</p> <p>For new coming \\(\\vec y_{t+1}\\) we can find \\(\\vec e^{k+1}\\) by G.S. process</p> \\[ \\begin{aligned} e_{k+1}&amp;=y_{k+1}-\\hat y_{k+1|k}\\\\ &amp;=y_{k+1}-\\text{proj}(y_{k+1};\\mathcal E_k)\\\\ &amp;=y_{k+1}-\\sum_{i=1}^k\\frac{\\langle y_{k+1},e_i\\rangle}{\\langle e_i,e_i\\rangle}\\cdot e_i \\end{aligned} \\] <p>It satisfies</p> \\[ \\langle e_{k+1},y_{i}\\rangle=E[e_{k+1}y_{i}^T]=0,\\quad\\forall i\\in[1,k] \\] <p>To estimate \\(x\\) at \\(k+1\\)</p> \\[ \\begin{aligned} \\hat x_{k+1}&amp;=\\text{proj}(x_{k+1};\\mathcal E_{k+1})\\\\ &amp;=\\sum_{i=1}^{k+1}\\frac{\\langle x_{k+1},e_i\\rangle}{\\langle e_i,e_i\\rangle}\\cdot e_i \\\\ &amp;=\\sum_{i=1}^k\\frac{\\langle x_{k+1},e_i\\rangle}{\\langle e_i,e_i\\rangle}\\cdot e_ + \\frac{\\langle x_{k+1},e_{k+1}\\rangle}{\\langle e_{k+1},e_{k+1}\\rangle}\\cdot e_{k+1}\\\\ &amp;=\\sum_{i=1}^k\\frac{\\langle x_k,e_i\\rangle}{\\langle e_i,e_i\\rangle}\\cdot e_i + \\frac{\\langle x_{k+1},e_{k+1}\\rangle}{\\langle e_{k+1},e_{k+1}\\rangle}\\cdot e_{k+1}\\\\ &amp;= \\hat x_k+ \\frac{\\langle x_{k+1},e_{k+1}\\rangle}{\\langle e_{k+1},e_{k+1}\\rangle}\\cdot e_{k+1} \\end{aligned} \\]","tags":["hkust","estimation"]},{"location":"2025/04/20/kalman-filter-in-3-ways/","title":"Kalman Filter in 3 Ways","text":"","tags":["hkust","estimation"]},{"location":"2025/04/20/kalman-filter-in-3-ways/#kalman-filter-in-3-ways","title":"Kalman Filter in 3 Ways","text":"<p>This is the lecture notes for \"ELEC 5650: Networked Sensing, Estimation and Control\" in the 2024-25 Spring semester, delivered by Prof. Ling Shi at HKUST. In this session, we will deviate Kalman Filter from three different perspectives: Geometric, Probabilistic, and Optimization approaches.</p>","tags":["hkust","estimation"]},{"location":"2025/04/20/kalman-filter-in-3-ways/#takeaway-notes","title":"Takeaway Notes","text":"<p>Consider an LTI system with initial conditions \\(\\hat x_{0\\vert0}\\) and \\(\\hat P_{0\\vert0}\\)</p> \\[ \\begin{aligned} x_{k+1} &amp;= Ax_k+Bu_k+\\omega_k,&amp;\\omega_k\\sim\\mathcal N(0,Q) \\\\ y_k &amp;= Cx_k + \\nu_k, &amp;\\nu_k\\sim\\mathcal N(0,R) \\end{aligned} \\] <p>Find the estimation of \\(x_k\\) given \\(\\set{u_0,u_1\\cdots,u_k}\\) and \\(\\set{y_0,y_1\\cdots,y_k}\\).</p>","tags":["hkust","estimation"]},{"location":"2025/04/20/kalman-filter-in-3-ways/#assumptions","title":"Assumptions","text":"<ol> <li>\\((A,B)\\) is controllable and \\((A,C)\\) is observable</li> <li>\\(Q\\succeq0,R\\succeq0,P_0\\succeq0\\)</li> <li>\\(\\omega_k\\), \\(\\nu_k\\) and \\(\\hat x_0\\) are mutually uncorelated</li> <li>The future state of the system is conditionally independent of the past states given the current state</li> </ol>","tags":["hkust","estimation"]},{"location":"2025/04/20/kalman-filter-in-3-ways/#time-update","title":"Time Update","text":"\\[ \\begin{aligned} \\hat x_{k\\vert k-1}&amp;=A\\hat x_{k-1\\vert k-1}+Bu_k \\\\ \\hat P_{k\\vert k-1} &amp;= A\\hat P_{k-1\\vert k-1}A^T+Q \\end{aligned} \\]","tags":["hkust","estimation"]},{"location":"2025/04/20/kalman-filter-in-3-ways/#measurement-update","title":"Measurement Update","text":"\\[ \\begin{aligned} K_k &amp;= \\hat P_{k\\vert k-1}C^T(C\\hat P_{k\\vert k-1}C^T+R)^{-1} \\\\ \\hat x_{k\\vert k} &amp;= \\hat x_{k\\vert k-1} + K_k(y_k-C\\hat x_{k\\vert k-1}) \\\\ \\hat P_{k\\vert k} &amp;= \\hat P_{k\\vert k-1}-K_kC\\hat P_{k\\vert k-1} = (\\hat P_{k\\vert k-1}^{-1}+C^TR^{-1}C)^{-1}\\\\ \\end{aligned} \\]","tags":["hkust","estimation"]},{"location":"2025/04/20/kalman-filter-in-3-ways/#geometric-perspective-lmmse-estimation","title":"Geometric Perspective (LMMSE Estimation)","text":"<p>The Geometric perspective views Kalman Filter as a Linear Minimum Mean Square Error (LMMSE) estimator, which is rooted in orthogonal projection theory in Hilbert space. The key insight is that the Kalman Filter's innovation term \\(e_k\\) is orthogonal to all past observations \\(\\mathcal{Y}_{k-1}\\), ensuring the new information being incorporated is statistically independent of previous measurements, thus maintaining the estimator's optimality.</p>","tags":["hkust","estimation"]},{"location":"2025/04/20/kalman-filter-in-3-ways/#time-update_1","title":"Time Update","text":"\\[ \\begin{aligned} \\hat{x}_{k\\vert k-1} &amp;= \\text{proj}_{\\mathcal{Y}_{k-1}}(x_k) \\\\ &amp;= \\text{proj}_{\\mathcal{Y}_{k-1}}(Ax_{k-1}+Bu_k+\\omega_k) \\\\ &amp;= A\\cdot\\text{proj}_{\\mathcal{Y}_{k-1}}(x_{k-1}) + B\\cdot\\text{proj}_{\\mathcal{Y}_{k-1}}(u_k)+\\text{proj}_{\\mathcal{Y}_{k-1}}(\\omega_k) \\\\ &amp;= A\\hat x_{k-1\\vert k-1} + Bu_k \\\\ \\\\ \\tilde{x}_{k\\vert k-1}&amp;= x_k-\\hat{x}_{k\\vert k-1} \\\\ \\\\ \\hat{P}_{k\\vert k-1} &amp;= \\mathbb E[\\tilde{x}_{k\\vert k-1}\\tilde{x}_{k\\vert k-1}^T] \\\\ &amp;= \\mathbb{E}[(x_k-\\hat{x}_{k\\vert k-1})(x_k-\\hat{x}_{k\\vert k-1})^T] \\\\ &amp;= \\mathbb{E}[(A\\tilde{x}_{k-1\\vert k-1}+\\omega_k)(A\\tilde{x}_{k-1\\vert k-1}+\\omega_k)^T] \\\\ &amp;= A\\mathbb{E}[\\tilde{x}_{k-1\\vert k-1}\\tilde{x}_{k-1\\vert k-1}^T]A^T+2A\\mathbb{E}[\\tilde{x}_{k-1\\vert k-1}\\omega_k^T]+\\mathbb{E}[\\omega_k\\omega_k^T] \\\\ &amp;= A\\hat{P}_{k-1\\vert k-1}A^T+Q \\end{aligned} \\]","tags":["hkust","estimation"]},{"location":"2025/04/20/kalman-filter-in-3-ways/#measurement-update_1","title":"Measurement Update","text":"\\[ \\begin{aligned} e_k &amp;= y_k - \\hat{y}_{k\\vert k-1} \\\\ &amp;= y_k - \\text{proj}_{\\mathcal{Y}_{k-1}}(y_k) \\\\ &amp;= y_k - \\text{proj}_{\\mathcal{Y}_{k-1}}(Cx_k+\\nu_k) \\\\ &amp;= y_k - C\\cdot\\text{proj}_{\\mathcal{Y}_{k-1}}(x_k)-\\text{proj}_{\\mathcal{Y}_{k-1}}(\\nu_k) \\\\ &amp;= y_k - C\\hat{x}_{k\\vert k-1} \\\\ \\\\ \\hat{x}_{k\\vert k} &amp;= \\text{proj}_{\\mathcal{Y}_{k}}(x_k) \\\\ &amp;= \\hat{x}_{k\\vert k-1}+K_ke_k \\\\ &amp;= \\hat{x}_{k\\vert k-1}+K_k(y_k - C\\hat{x}_{k\\vert k-1}) \\\\ \\\\ \\tilde{x}_{k\\vert k}&amp;= x_k-\\hat{x}_{k\\vert k} \\\\ \\\\ \\hat{P}_{k\\vert k} &amp;= \\mathbb{E}[\\tilde{x}_{k\\vert k}\\tilde{x}_{k\\vert k}^T] \\\\ &amp;= \\mathbb{E}[(x_k-\\hat{x}_{k\\vert k})(x_k-\\hat{x}_{k\\vert k})^T] \\\\ &amp;= \\mathbb{E}[(\\tilde{x}_{k\\vert k-1}-K_ke_k)(\\tilde{x}_{k\\vert k-1}-K_ke_k)^T] \\\\ &amp;= \\mathbb{E}[\\tilde{x}_{k\\vert k-1}\\tilde{x}_{k\\vert k-1}^T] - 2K_k\\mathbb{E}[e_k\\tilde{x}_{k\\vert k-1}^T]+K_k\\mathbb{E}[e_ke_k^T]K_k^T \\\\ &amp;= \\hat{P}_{k\\vert k-1} - 2K_k\\mathbb{E}[(y_k-C\\hat{x}_{k\\vert k-1})\\tilde{x}_{k\\vert k-1}^T]+K_k\\mathbb{E}[(y_k-C\\hat{x}_{k\\vert k-1})(y_k-C\\hat{x}_{k\\vert k-1})^T]K_k^T \\\\ &amp;= \\hat{P}_{k\\vert k-1} - 2K_kC\\hat{P}_{k\\vert k-1} + K_k(C\\hat{P}_{k\\vert k-1}C^T+R)K_k^T \\\\ \\\\ \\frac{\\partial\\text{tr}(\\hat{P}_{k\\vert k})}{\\partial K_k} &amp;= -2C\\hat{P}_{k\\vert k-1} + 2K_k(C\\hat{P}_{k\\vert k-1}C^T+R) = 0 \\\\ K_k &amp;= \\hat{P}_{k\\vert k-1}C^T(C\\hat{P}_{k\\vert k-1}C^T+R)^{-1} \\end{aligned} \\]","tags":["hkust","estimation"]},{"location":"2025/04/20/kalman-filter-in-3-ways/#probabilistic-perspective-bayesian-estimation","title":"Probabilistic Perspective (Bayesian Estimation)","text":"<p>The filter maintains a Gaussian belief state that gets refined through sequential application of Bayes' rule, where prediction corresponds to Chapman-Kolmogorov propagation and update implements Bayesian conditioning.</p> \\[ \\begin{aligned} &amp;p(x_{k}|y_{1:k}, u_{1:k})\\\\=&amp;p(x_{k}|y_k,y_{1:k-1}, u_{1:k})\\\\=&amp;\\frac{p(y_{k}|x_k,y_{1:k-1}, u_{1:k})\\cdot p(x_{k}|y_{1:k-1}, u_{1:k})}{p(y_{k}|y_{1:k-1}, u_{1:k})}\\\\ =&amp;\\eta\\cdot\\underbrace{p(y_k|x_k)}_\\text{observation model}\\cdot\\underbrace{p(x_{k}|y_{1:k-1}, u_{1:k})}_\\text{prior belief} \\\\ =&amp;\\eta\\cdot p(y_k|x_k)\\cdot\\int p(x_{k},x_{k-1}|y_{1:k-1}, u_{1:k})\\ {\\rm d}x_{k-1} \\\\ =&amp;\\eta\\cdot p(y_k|x_k)\\cdot\\int p(x_{k}|x_{k-1},y_{1:k-1},u_{1:k})\\cdot p(x_{k-1}|y_{1:k-1},u_{1:k})\\ {\\rm d}x_{k-1} \\\\ =&amp;\\eta\\cdot p(y_k|x_k)\\cdot\\int\\underbrace{p(x_{k}|x_{k-1},u_{k})}_\\text{motion model}\\cdot\\underbrace{p(x_{k-1}|y_{1:k-1},u_{1:k-1})}_\\text{previous belief}\\ {\\rm d}x_{k-1} \\\\ =&amp;\\eta\\cdot\\mathcal N(y_k;H_kx_k,R_k)\\cdot\\int\\mathcal{N}(x_k;Ax_{k-1}+Bu_k,Q_k)\\cdot\\mathcal{N}(x_{k-1};\\hat{x}_{k-1},\\hat{P}_{k-1})\\ {\\rm d}x_{k-1} \\\\ =&amp;\\eta\\cdot\\mathcal N(y_k;H_kx_k,R_k)\\cdot\\mathcal{N}(x_k;\\hat{x}_{k\\vert k-1},\\hat{P}_{k\\vert k-1}) \\\\ \\propto&amp;\\ \\mathcal{N}(x_k;\\hat{x}_k,\\hat{P}_k) \\end{aligned} \\] <p>Applying Bayesian Rule and Markov Assumptions to \\(p(x_k|y_{1:k},u_{1:k})\\), then the time update and the measurement update becomes very explicit.</p>","tags":["hkust","estimation"]},{"location":"2025/04/20/kalman-filter-in-3-ways/#optimization-perspective-map-estimation","title":"Optimization Perspective (MAP Estimation)","text":"<p>The Kalman Filter solves a weighted least-squares problem where the optimal state estimate minimizes a cost function balancing prediction fidelity against measurement consistency, with covariance matrices acting as natural weighting matrices.</p> <p>By Bayesian Rule, we know that</p> \\[ p(x_{k}|y_{1:k}, u_{1:k})\\propto\\underbrace{p(y_k|x_k)}_\\text{measurement}\\cdot\\underbrace{p(x_{k}|y_{1:k-1}, u_{1:k})}_\\text{prior} \\\\ \\] <p>To maximize the posterior probability, it is equivalent to minimizing its negative logarithmic posterior.</p> \\[ -\\ln{p(x_{k}|y_{1:k}, u_{1:k})}=-\\ln{p(y_k|x_k)}-\\ln{p(x_{k}|y_{1:k-1}, u_{1:k})}+C \\] <p>Applying Gaussian probability distribution</p> \\[ J_{x_k}=\\frac12||z_k-Cx_k||_{R_k^{-1}}^2+\\frac12||{x_k-\\hat{x}_{k\\vert k-1}}||_{\\hat{P}_{k\\vert k-1}}^2+C \\] \\[ \\hat{x}_{k\\vert k-1}=\\arg\\min_{x_k}\\left(||{x_k-\\hat{x}_{k\\vert k-1}}||_{\\hat{P}_{k\\vert k-1}}^2+|z_k-Cx_k||_{R_k^{-1}}^2\\right) \\] <p>Prior is given by</p> \\[ \\begin{aligned} \\hat{x}_{k\\vert k-1} &amp;= \\mathbb E[x_k|y_{1:k-1}, u_{1:k}] \\\\ \\hat{P}_{k\\vert k-1} &amp;= \\text{Cov}(x_k|y_{1:k-1}, u_{1:k}) \\end{aligned} \\]","tags":["hkust","estimation"]},{"location":"2025/04/24/linear-quadratic-regulator/","title":"Linear Quadratic Regulator","text":"","tags":["hkust","control"]},{"location":"2025/04/24/linear-quadratic-regulator/#linear-quadratic-regulator","title":"Linear Quadratic Regulator","text":"<p>This is the lecture notes for 'ELEC 5650: Networked Sensing, Estimation and Control' in the 2024-25 Spring semester, delivered by Prof. Ling Shi at HKUST. In this session, we will cover Linear Quadratic Regulator (LQR) theory and its applications in control systems.</p>","tags":["hkust","control"]},{"location":"2025/04/24/linear-quadratic-regulator/#dynamic-programming","title":"Dynamic Programming","text":"<p>Consider a discrete-time dynamical system over a finite horizon \\(N\\). The goal is to find a control policy that minimizes the expected cumulative cost:</p> \\[ J^\\pi(x_0)=\\mathbb E_{\\omega_k}\\left\\{g_N(x_N)+\\sum_{k=0}^{N-1}g_k[x_k,\\mu_k(x_k),\\omega_k]\\right\\} \\] <p>The system evolves according to:</p> \\[ x_{k+1} = f_k(x_k,u_k,\\omega_k) \\]","tags":["hkust","control"]},{"location":"2025/04/24/linear-quadratic-regulator/#principle-of-optimality","title":"Principle of Optimality","text":"<p>\"An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.\"</p> <p> -- Richard Bellman</p> <p>Optimal principle allows us to break down the multi-stage optimization problem into a sequence of simpler single-stage problems.</p> <p>Let \\(\\pi^*=\\set{\\mu_0^*,\\cdots,\\mu_{N-1}^*}\\) be an optimal policy. Then for any \\(k\\) and reachable state \\(x_k\\), the sub-policy \\(\\pi_{k\\to N-1}^*=\\set{\\mu_k^*,\\cdots,\\mu_{N-1}^*}\\) must minimize the cost-to-go:</p> \\[ J_{k\\to N}(x_k)=\\mathbb E_{\\omega_k}\\left\\{g_N(x_N)+\\sum_{i=k}^{N-1}g_i[x_i,\\mu_i(x_i),\\omega_i]\\right\\} \\] <p>This implies</p> \\[ J_k(x_k) = \\min_{u_k}\\mathbb E_{\\omega_k}\\left\\{g_k[x_k,\\mu_k(x_k),\\omega_k]+J_{k+1}f[x_k,\\mu_k(x_k),\\omega_k]\\right\\} \\]","tags":["hkust","control"]},{"location":"2025/04/24/linear-quadratic-regulator/#dynamic-programming-algorithm","title":"Dynamic Programming Algorithm","text":"<p>The solution is computed recursively through the following steps:</p>","tags":["hkust","control"]},{"location":"2025/04/24/linear-quadratic-regulator/#terminal-cost","title":"Terminal Cost","text":"\\[ J_N(x_N)=g_N(x_N) \\]","tags":["hkust","control"]},{"location":"2025/04/24/linear-quadratic-regulator/#backward-recursion","title":"Backward Recursion","text":"\\[ J_k(x_k)=\\min_{u_k\\in\\mathcal U}\\mathbb E_{\\omega_k}\\left\\{g_k(x_k,u_k,\\omega_k)+J_{k+1}[f(x_k,u_k,\\omega_k)]\\right\\} \\] \\[ \\mu_k^*(x_k)=\\arg\\min_{u_k\\in\\mathcal U}\\mathbb E_{\\omega_k}\\left\\{g_k(x_k,u_k,\\omega_k)+J_{k+1}[f(x_k,u_k,\\omega_k)]\\right\\} \\] <p>Consider the following linear system</p> \\[ x_{k+1} = Ax_k+Bu_k \\] <p>We wants to find a series of \\(u_0,\\cdots,u_{N-1}\\) to minimize</p> \\[ J=\\underbrace{x_N^TQx_N}_{g_N(x_N)}+\\sum_{k=0}^{N-1}(\\underbrace{x_k^TQx_k+u_k^TRu_k}_{g_k(x_k,u_k,\\omega_k)}),\\quad Q\\succeq0,R\\succ0 \\]","tags":["hkust","control"]},{"location":"2025/04/24/linear-quadratic-regulator/#solution","title":"Solution","text":"","tags":["hkust","control"]},{"location":"2025/04/24/linear-quadratic-regulator/#terminal-cost_1","title":"Terminal Cost","text":"\\[ J_N(x_N)=g_N(x_N)=x_N^TQx_N\\triangleq x_N^TP_Nx_N \\]","tags":["hkust","control"]},{"location":"2025/04/24/linear-quadratic-regulator/#backward-recursion_1","title":"Backward Recursion","text":"\\[ \\begin{aligned} J_{N-1}(x_{N-1}) &amp;= \\min_{u_{N-1}}\\left\\{ x_{N-1}^TQx_{N-1}+u_{N-1}^TRu_{N-1} + J_N(x_N) \\right\\} \\\\ &amp;= \\min_{u_{N-1}}\\left\\{ x_{N-1}^TQx_{N-1}+u_{N-1}^TRu_{N-1} + x_N^TP_Nx_N \\right\\} \\\\ &amp;= \\min_{u_{N-1}}\\left\\{ x_{N-1}^TQx_{N-1}+u_{N-1}^TRu_{N-1} + (Ax_{N-1}+Bu_{N-1})^TP_N(Ax_{N-1}+Bu_{N-1}) \\right\\} \\\\ &amp;= \\min_{u_{N-1}}\\left\\{u_{N-1}^T(R+B^TP_NB)u_{N-1}+2x_{N-1}^TA^TP_NBu_{N-1} + x_{N-1}^T(Q+A^TP_NA)x_{N-1}\\right\\} \\\\ \\end{aligned} \\] \\[ u_{N-1}^*= \\underbrace{-(R+B^TP_NB)^{-1}B^TP_NA}_{L_{N-1}}x_{N-1} \\] \\[ \\begin{aligned} J_{N-1}(x_{N-1}) &amp;= x_{N-1}^TP_{N-1}x_{N-1} \\\\ &amp;= u_{N-1}^T(R+B^TP_NB)u_{N-1}+2x_{N-1}^TA^TP_NBu_{N-1} + x_{N-1}^T(Q+A^TP_NA)x_{N-1} \\\\ &amp;= x_{N-1}^T[A^TP_N^TB(R+B^TP_NB)^{-1}B^TP_NA - 2A^TP_NB(R+B^TP_NB)^{-1}B^TP_NA+Q+A^TP_NA]x_{N-1}\\\\ &amp;= x_{N-1}^T[Q+A^TP_NA-A^TP_N^TB(R+B^TP_NB)^{-1}B^TP_NA]x_{N-1} \\end{aligned} \\] \\[ P_{N-1} = Q+A^TP_NA-A^TP_N^TB(R+B^TP_NB)^{-1}B^TP_NA \\]","tags":["hkust","control"]},{"location":"2025/04/24/linear-quadratic-regulator/#summary","title":"Summary","text":"\\[ P_N=Q,\\quad \\begin{cases} u_k^* = L_kx_k \\\\ L_k = -(B^TP_{k+1}B+R)^{-1}B^TP_{k+1}A \\\\ P_k = A^TP_{k+1}A+Q-A^TP_{k+1}B(B^TP_{k+1}B+R)^{-1}B^TP_{k+1}A \\end{cases} \\]","tags":["hkust","control"]},{"location":"2025/04/24/linear-quadratic-regulator/#riccati-equation","title":"Riccati Equation","text":"<p>Define</p> \\[P_{k+1}=h(P_k)=A^TP_{k}A+Q-A^TP_{k}B(B^TP_{k}B+R)^{-1}B^TP_{k}A\\] <p>Assume \\((A,B)\\) is controllable, \\((A,\\sqrt{Q})\\) is observable, then the following holds</p> <ol> <li>\\(\\exists P\\succ0,\\forall P_0\\succeq0,\\lim_{k\\to\\infty}P_k=P\\)</li> <li>\\(P\\) is the unique solution to \\(P=h(P)\\)</li> <li>\\(D=A+BL\\) is stable, where \\(L=-(B^TPB+R)^{-1}B^TPA\\)</li> </ol>","tags":["hkust","control"]},{"location":"2025/04/24/linear-quadratic-regulator/#existence","title":"Existence","text":"<p>We firstly prove \\(\\exists P\\succ0,P=h(P)\\)</p> <p>Assume \\(P_0=0\\), then \\(P_k=h(P_{k-1})=h^{k-1}(P_1)=h^k(P_0)=h(0)\\succeq0\\). So for any control sequence</p> \\[ \\min_{i=0}^{k-1}\\underbrace{(x_i^TQx_i+u_i^TRu_i)}_{x_0^Tg^{k}(0)x_0}\\le\\min_{i=0}^{k}\\underbrace{(x_i^TQx_i+u_i^TRu_i)}_{x_0^Tg^{k+1}(0)x_0} \\] <p>If \\(P_0=0\\), then \\(\\forall X\\succeq Y,h(X)\\succeq h(Y)\\). For any specific control sequence \\(\\bar u_0,\\cdots\\bar u_k\\), there exist an associated cost \\(\\bar J_k=\\sum_{i=0}^{k-1}(x_i^TQx_i+\\bar u_i^TRu_i)\\) woule be a constant.</p> \\[ \\forall k, x_0^TP_kx_0=x_0^Th^k(P_0)x_0=x_0^Th^k(0)x_0\\le\\bar J_k \\] <p>So \\(P_k\\) converges when \\(P_0=0\\)</p>","tags":["hkust","control"]},{"location":"2025/04/24/linear-quadratic-regulator/#stability","title":"Stability","text":"\\[ \\begin{aligned} P &amp;= h(P) \\\\ &amp;= A^TP_{k}A+Q-A^TP_{k}B(B^TP_{k}B+R)^{-1}B^TP_{k}A \\\\ &amp;= D^TPD+Q+L^TRL \\end{aligned} \\] \\[ x_{k+1}=Ax_k+Bu_k=(A+BL)x_k = Dx_k \\] <p>To show \\(D\\) is stable, we only need to show that \\(\\forall x_0,x_k\\to0\\) as \\(k\\to\\infty\\)</p> \\[ \\begin{cases} x_{k+1}^TPx_{k+1} - x_k^TPx_k = x_k^TD^TPDx_k-x_k^TPx_k=-x_k^T(Q+L^TRL)x_k\\\\ x_{k}^TPx_{k} - x_{k-1}^TPx_{k-1} =-x_{k-1}^T(Q+L^TRL)x_{k-1} \\\\ \\qquad\\vdots \\\\ x_{1}^TPx_{1} - x_{0}^TPx_{0} =-x_{0}^T(Q+L^TRL)x_{0} \\\\ \\end{cases} \\] \\[ x_{k+1}^TPx_{k+1}=x_0^TPx_0-\\sum_{i=0}^kx_i^T(Q+L^TRL)x_i \\] <p>Because \\(P\\ge0\\), \\(x_{k+1}^TPx_{k+1}\\succeq0\\), hence</p> \\[ \\sum_{i=0}^kx_i^T(Q+L^TRL)x_i \\le x_0^TPx_0 &lt; \\infty \\] <p>This implies</p> \\[ \\lim_{k\\to\\infty}x_k^T(Q+L^TRL)x_k=0 \\] <p>While \\(Q+L^TRL\\succ0\\), we must have</p> \\[ \\lim_{k\\to\\infty}x_k=0 \\] <p>Hence, the stability is proved.</p>","tags":["hkust","control"]},{"location":"2025/04/24/linear-quadratic-regulator/#convergence","title":"Convergence","text":"<p>Next we prove \\(\\forall P\\succeq0,P=h(P)\\). Because \\(\\rm u^*\\) is the optimal solution, \\(\\bar J_k\\ge x_0^Th^k(P_0)x_0\\).</p> \\[ \\begin{aligned} \\bar J_k &amp;= x_k^TP_0x_k+\\sum_{i=0}^{k-1}(x_i^TQx_i+u_i^TRu_i) \\\\ &amp;= x_0^T(D^k)^TP_0D^kx_0+\\sum_{i=0}^{k-1}(x_i^T(Q+L^TRL)x_i) \\\\ &amp;= x_0^T\\left[(D^k)^TP_0D^k+\\sum_{i=0}^{k-1}(D^i)^T(Q+L^TRL)D^i\\right]x_0 \\\\ &amp;= x_0^T\\left[(D^k)^TP_0D^k+P-(D^k)^TPD^k\\right]x_0 \\\\ \\end{aligned} \\] \\[ \\lim_{k\\to\\infty}\\bar J_k = x_0^TPx_0 \\] \\[ \\forall P_0, \\lim_{k\\to\\infty}x_0^Th^k(P_0)x_0 = x_0^TPx_0 \\] \\[ \\lim_{k\\to\\infty}P_k=\\lim_{k\\to\\infty}h^k(P_0)=P \\]","tags":["hkust","control"]},{"location":"2025/04/24/linear-quadratic-regulator/#uniqueness","title":"Uniqueness","text":"<p>Assume \\(\\bar P\\) is another solution to \\(\\bar P=h(\\bar P)\\). If \\(P'\\ne P\\), then \\(\\exists x_0,x_0^TP'X_0&lt;x_0^TPx_0\\). Contradict to the optimal feature. Hence, \\(P'=P\\)</p>","tags":["hkust","control"]},{"location":"2025/04/24/linear-quadratic-regulator/#linear-quadratic-gaussian","title":"Linear Quadratic Gaussian","text":"\\[ \\begin{aligned} x_{k+1} &amp;= Ax_k+Bu_k+\\omega_k,&amp;\\omega_k\\sim\\mathcal N(0,W) \\\\ y_k &amp;= Cx_k + \\nu_k, &amp;\\nu_k\\sim\\mathcal N(0,V) \\end{aligned} \\] <p>We want to minimize the quadratic cost function:</p> \\[ J=\\mathbb E\\left[{x_N^TQx_N}+\\sum_{k=0}^{N-1}(x_k^TQx_k+u_k^TRu_k)\\right],\\quad Q\\succeq0,R\\succ0 \\] <p>By seperation principle, we can decomposed the problem to an optimal estimatior and an optimal controller.</p>","tags":["hkust","control"]},{"location":"2025/04/24/linear-quadratic-regulator/#kalman-filter","title":"Kalman Filter","text":"<p>The Kalman filter provides the optimal state estimate \\(\\hat{x}_k\\) with error covariance \\(\\Sigma_k\\):</p>","tags":["hkust","control"]},{"location":"2025/04/24/linear-quadratic-regulator/#time-update","title":"Time Update","text":"\\[ \\begin{aligned} \\hat{x}_{k|k-1} &amp;= A\\hat{x}_{k-1\\vert k-1} + Bu_{k-1} \\\\ \\hat{P}_{k|k-1} &amp;= A\\hat{P}_{k-1\\vert k-1}A^T + W \\end{aligned} \\]","tags":["hkust","control"]},{"location":"2025/04/24/linear-quadratic-regulator/#measurement-update","title":"Measurement Update","text":"\\[ \\begin{aligned} K_k &amp;= \\hat{P}_{k|k-1}C^T(C\\hat{P}_{k|k-1}C^T + V)^{-1} \\\\ \\hat{x}_{k\\vert k} &amp;= \\hat{x}_{k|k-1} + K_k(y_k - C\\hat{x}_{k|k-1}) \\\\ \\hat{P}_{k\\vert k} &amp;= (I - K_kC)\\hat{P}_{k|k-1} \\end{aligned} \\]","tags":["hkust","control"]},{"location":"2025/04/24/linear-quadratic-regulator/#linear-quadratic-regulator_1","title":"Linear Quadratic Regulator","text":"\\[ J = \\sum_{k=0}^{\\infty} (x_k^T Q x_k + u_k^T R u_k) \\] <p>Solve \\(P\\) from</p> \\[ P = A^T P A - A^T P B (R + B^T P B)^{-1} B^T P A + Q \\] <p>And solve \\(L\\) from</p> \\[ L = -(R + B^T P B)^{-1} B^T P A \\] \\[ u_k=Lx_k \\]","tags":["hkust","control"]},{"location":"2025/04/10/linear-algebra--system-theory/","title":"Linear Algebra &amp; System Theory","text":"","tags":["hkust","math"]},{"location":"2025/04/10/linear-algebra--system-theory/#linear-algebra-system-theory","title":"Linear Algebra &amp; System Theory","text":"<p>This is the lecture notes for \"ELEC 5650: Networked Sensing, Estimation and Control\" in the 2024-25 Spring semester, delivered by Prof. Ling Shi at HKUST. In this session, we will cover essential mathematical tools and concepts from linear algebra, matrix theory, and system theory that are fundamental to networked sensing, estimation, and control.</p>","tags":["hkust","math"]},{"location":"2025/04/10/linear-algebra--system-theory/#eigenvalues","title":"Eigenvalues","text":"<p>\\(A\\in\\mathbb R^{n\\times n}\\), \\(\\lambda\\) can be solved by</p> \\[\\text{det}(\\lambda I-A)=0\\] \\[ \\prod_{i=1}^n\\lambda_i=\\text{det}(A),\\quad\\sum_{i=1}^n\\lambda_i=\\text{Tr}(A),\\quad Av_i=\\lambda_iv_i \\]","tags":["hkust","math"]},{"location":"2025/04/10/linear-algebra--system-theory/#lemme","title":"Lemme","text":"<p>Let \\(A\\in\\mathbb R^{n\\times m}, B\\) the non-zero eigenvalues of \\(AB\\) and \\(BA\\) are the same</p>","tags":["hkust","math"]},{"location":"2025/04/10/linear-algebra--system-theory/#proof","title":"Proof","text":"\\[ \\underbrace{\\begin{bmatrix}I&amp;0\\\\B&amp;I\\end{bmatrix}}_{P}\\underbrace{\\begin{bmatrix}I&amp;0\\\\-B&amp;I\\end{bmatrix}}_{P^{-1}}=\\begin{bmatrix}I&amp;0\\\\0&amp;I\\end{bmatrix}=I \\] \\[ \\underbrace{\\begin{bmatrix}I&amp;0\\\\B&amp;I\\end{bmatrix}}_{P}\\begin{bmatrix}AB&amp;A\\\\0&amp;0\\end{bmatrix}\\underbrace{\\begin{bmatrix}I&amp;0\\\\-B&amp;I\\end{bmatrix}}_{P^{-1}}=\\begin{bmatrix}0&amp;A\\\\0&amp;BA\\end{bmatrix} \\]","tags":["hkust","math"]},{"location":"2025/04/10/linear-algebra--system-theory/#corollary","title":"Corollary","text":"\\[ \\text{Tr}(AB)=\\text{Tr}(BA),\\quad \\text{Tr}(ABC)=\\text{Tr}(BCA)=\\text{Tr}(CAB) \\] \\[ \\text{Tr}(ABC)\\neq\\text{Tr}(ACB) \\]","tags":["hkust","math"]},{"location":"2025/04/10/linear-algebra--system-theory/#cholesky-decomposition","title":"Cholesky Decomposition","text":"<p>If \\(A\\succeq0\\), then \\(\\exists\\) a lower triangular matrix \\(L\\) with real and non-negative diagonal entries such that</p> \\[ A=LL^T=\\begin{bmatrix} \\ddots &amp; &amp; 0 \\\\ &amp; \\ddots &amp; \\\\ \\ddots &amp; &amp; \\ddots \\end{bmatrix}\\begin{bmatrix} \\ddots &amp; &amp; \\ddots \\\\ &amp; \\ddots &amp; \\\\ 0 &amp; &amp; \\ddots \\end{bmatrix} \\] \\[ A=\\begin{bmatrix}a_{11}&amp;\\mathbf a_{12}\\\\\\mathbf a_{21}&amp;A_{22}\\end{bmatrix},\\quad L=\\begin{bmatrix}l_{11}&amp;\\mathbf 0\\\\\\mathbf l_{21}&amp;L_{22}\\end{bmatrix} \\] \\[ \\begin{bmatrix}a_{11}&amp;\\mathbf a_{12}\\\\\\mathbf a_{21}&amp;A_{22}\\end{bmatrix}=\\begin{bmatrix}l_{11}&amp;\\mathbf 0\\\\\\mathbf l_{21}&amp;L_{22}\\end{bmatrix}\\begin{bmatrix}l_{11}&amp;\\mathbf l_{21}^T\\\\\\mathbf 0&amp;L_{22}^T\\end{bmatrix}=\\begin{bmatrix}l_{11}^2 &amp; l_{11}\\mathbf l_{21}^T\\\\l_{11}\\mathbf l_{21}&amp;\\mathbf l_{21}\\mathbf l_{21}^T+L_{22} L_{22}^T\\end{bmatrix} \\] \\[ l_{11}=\\sqrt{a_{11}},\\quad\\mathbf l_{21}=\\frac1{l_{11}}\\mathbf a_{21},\\quad L_{22}L_{22}^T=A_{22}-\\mathbf l_{21}\\mathbf l_{21}^T \\] <p>Recursive Calculation !!!</p>","tags":["hkust","math"]},{"location":"2025/04/10/linear-algebra--system-theory/#matrix-inversion-lemma","title":"Matrix Inversion Lemma","text":"<p>For matrix \\(A\\) and \\(B\\)</p> \\[ (A+B)^{-1} = A^{-1}(I+BA^{-1})^{-1} \\] <p>For any matrix \\(A,B,C,D\\) with compatible dimensions, \\(A,C\\) nonsingular, then</p> \\[ \\begin{aligned} (A+BCD)^{-1} &amp;= [A(I+A^{-1}BCD)]^{-1} \\\\ &amp;= (I+A^{-1}BCD)^{-1}(I+A^{-1}BCD-A^{-1}BCD)A^{-1} \\\\ &amp;= [I-(I+A^{-1}BCD)^{-1}A^{-1}BCD]A^{-1} \\\\ &amp;= A^{-1}-(I+A^{-1}BCD)^{-1}A^{-1}BCDA^{-1} \\\\ &amp;= A^{-1}-(I+A^{-1}BCDA^{-1}A)^{-1}A^{-1}BCDA^{-1} \\\\ &amp;= A^{-1}-A^{-1}BCDA^{-1}(I+AA^{-1}BCDA^{-1})^{-1} \\\\ &amp;= A^{-1}-A^{-1}B[CDA^{-1}(I+BCDA^{-1})^{-1}] \\\\ &amp;= A^{-1}-A^{-1}B[(I+CDA^{-1}B)^{-1}CDA^{-1}] \\\\ &amp;= A^{-1}-A^{-1}B[CC^{-1}+CDA^{-1}B]^{-1}CDA^{-1} \\\\ &amp;= A^{-1}-A^{-1}B[C(C^{-1}+DA^{-1}B)]^{-1}CDA^{-1} \\\\ &amp;= A^{-1}-A^{-1}B(C+DA^{-1}B)^{-1}DA^{-1} \\\\ \\end{aligned} \\]","tags":["hkust","math"]},{"location":"2025/04/10/linear-algebra--system-theory/#schur-complement","title":"Schur Complement","text":"\\[ \\begin{bmatrix} \\rm A &amp; \\rm B \\\\ \\rm C &amp; \\rm D \\end{bmatrix}=\\begin{bmatrix} \\rm I &amp; 0 \\\\ \\rm CA^{-1} &amp; \\rm I \\end{bmatrix}\\begin{bmatrix} \\rm A &amp; 0 \\\\ 0 &amp; \\rm D-CA^{-1}B \\end{bmatrix}\\begin{bmatrix} \\rm I &amp; \\rm A^{-1}B \\\\ 0 &amp; \\rm I \\end{bmatrix} \\] \\[ \\begin{bmatrix} \\rm A &amp; \\rm B \\\\ \\rm C &amp; \\rm D \\end{bmatrix}=\\begin{bmatrix} \\rm I &amp; \\rm BD^{-1} \\\\ 0 &amp; \\rm I \\end{bmatrix}\\begin{bmatrix} \\rm A-BD^{-1}C &amp; 0 \\\\ 0 &amp; \\rm D \\end{bmatrix}\\begin{bmatrix} \\rm I &amp; 0 \\\\ \\rm D^{-1}C &amp; \\rm I \\end{bmatrix} \\]","tags":["hkust","math"]},{"location":"2025/04/10/linear-algebra--system-theory/#inner-product-space","title":"Inner Product Space","text":"<p>\\(\\mathbf u,\\mathbf v\\in\\mathcal V\\), the inner product \\(\\langle\\mathbf u,\\mathbf v\\rangle\\) satisfies</p> <ol> <li>Linearity: \\(\\langle\\alpha_1\\mathbf u_1+\\alpha_2\\mathbf u_2,\\mathbf v\\rangle=\\alpha_1\\langle\\mathbf u_1,\\mathbf v\\rangle+\\alpha_2\\langle\\mathbf u_2,\\mathbf v\\rangle\\)</li> <li>Conjugate Symmetry: \\(\\langle\\mathbf u,\\mathbf v\\rangle=\\langle\\mathbf v,\\mathbf u\\rangle^*\\), \\((\u00b7)^*\\) means transpose</li> <li>Positive Definiteness: \\(||\\mathbf u||^2=\\langle\\mathbf u,\\mathbf u\\rangle=0\\Leftrightarrow\\mathbf u=0\\)</li> </ol> <p>For two random variables \\(X,Y\\), define \\(\\langle X,Y\\rangle=E[XY^T]\\)</p>","tags":["hkust","math"]},{"location":"2025/04/10/linear-algebra--system-theory/#projection-theorem","title":"Projection Theorem","text":"<p>Let \\(\\mathcal H\\in\\mathbb R^{m}\\) be a linear subspace of \\(\\mathcal S\\in\\mathbb R^n,(m&lt;n)\\). For some vector \\(\\mathbf y\\in\\mathcal S\\), the projection of \\(\\mathbf y\\) onto \\(\\mathcal H\\) denoted as \\(\\hat{\\mathbf y}_{\\mathcal H}\\) is a uniyque element in \\(\\mathcal H\\), such that \\(\\forall\\mathbf x\\in\\mathcal H,\\langle\\mathbf y-\\hat{\\mathbf y}_{\\mathcal H},\\mathbf x\\rangle=0\\), in other word \\(\\mathbf y-\\hat{\\mathbf y}_{\\mathcal H}\\perp\\mathbf x\\).</p>","tags":["hkust","math"]},{"location":"2025/04/10/linear-algebra--system-theory/#gram-schmidt-process","title":"Gram-Schmidt Process","text":"<p>Let \\(\\set{\\mathbf v_1,\\mathbf v_2,...\\mathbf v_n}\\) be a set of linearly independent\u00a0vectors in an inner product space\u00a0VV. The Gram-Schmidt process constructs an\u00a0orthonormal basis \\(\\set{\\mathbf u_1,\\mathbf u_2,\\cdots,\\mathbf u_n}\\) for the subspace spanned by \\(\\set{\\mathbf v_1,\\mathbf v_2,...\\mathbf v_n}\\) as follows:</p> \\[ \\begin{aligned} \\mathbf u_1 &amp;= \\mathbf v_1 \\\\ \\mathbf u_2 &amp;= \\mathbf v_2 - \\text{proj}_{\\mathbf u_1}(\\mathbf v_2) \\\\ &amp;\\quad\\vdots \\\\ \\mathbf u_k &amp;= \\mathbf v_k - \\sum_{j=1}^{k-1}\\text{proj}_{\\mathbf u_j}(\\mathbf v_k) \\end{aligned}\\qquad\\mathbf e_i = \\frac{\\mathbf u_i}{||\\mathbf u_i||} \\]","tags":["hkust","math"]},{"location":"2025/04/10/linear-algebra--system-theory/#autonomous-system","title":"Autonomous System","text":"<p>A linear system \\(x_{k+1}=Ax_k\\) is said to be stable if</p> \\[ \\forall x_0,\\lim_{k\\to\\infty}|x_k|=0 \\] <p>The system is stable if and only if</p> \\[ \\max_i|\\lambda_i(A)|&lt;1 \\]","tags":["hkust","math"]},{"location":"2025/04/10/linear-algebra--system-theory/#controllability","title":"Controllability","text":"<p>A linear system \\(x_{k+1}=Ax_k+Bu_k\\) is said to be controllable if</p> \\[ \\forall x_0,x^*,\\exists k&gt;0,\\mathbf u_k=[u_{k-1},\\cdots,u_1,u_0],\\quad\\text{s.t.}\\quad x_k=x^*. \\] <p>\\((A,B)\\) is controllable is equivalent to the following</p> <ol> <li>\\(M_c=[B,AB,A^2B,\\cdots,A^{n-1}B]\\) is full rank</li> <li>\\(W_c=\\sum_{k=0}^{n-1}A^kBB^T(A^T)^k\\) is full rank</li> <li>PBH test: \\(\\forall\\lambda\\in\\mathbb C,[A-\\lambda I, B]\\) is full rank</li> </ol> <p>Assume \\((A,B)\\) is controllable, given \\(x_0, x^*\\), find \\(\\mathbf u_n\\) such that \\(x_n=x^*\\)</p> \\[ \\begin{aligned} x^* = x_n &amp;= Ax_{n-1} + Bu_{n-1} \\\\ &amp;= A(Ax_{n-2} + Bu_{n-2}) + Bu_{n-1} \\\\ &amp;= A^2x_{n-2} + ABu_{n-2} + Bu_{n-1} \\\\ &amp;= A^nx_0 + A^{n-1}Bu_0 + \\cdots + ABu_{n-2} + Bu_{n-1} \\\\ &amp;= A^nx_0 + M_c\\mathbf u_n \\end{aligned} \\] \\[ \\mathbf u_n = M_c^T(M_cM_c^T)^{-1}(x^*-A^nx_0) \\]","tags":["hkust","math"]},{"location":"2025/04/10/linear-algebra--system-theory/#observability","title":"Observability","text":"<p>A linear system \\(x_{k+1}=Ax_k, y_k=Cx_k\\) is said to be observable if \\(\\forall x_0,\\exists k&gt;0\\), such that \\(x_0\\) can be computed from \\(\\mathbf y_k=[y_0,y_1,\\cdots,y_{k-1}]^T\\).</p> <p>\\((A,C)\\) is observable is equivalent to the following</p> <ol> <li>\\(M_o=\\begin{bmatrix}C\\\\CA\\\\\\vdots\\\\CA^{n-1}\\end{bmatrix}\\) is full rank</li> <li>\\(W_o=\\sum_{k=0}^{n-1}(A^k)^TC^TCA^k\\) is full rank</li> <li>PBH test: \\(\\forall\\lambda\\in\\mathbb C,\\begin{bmatrix}A-\\lambda I\\\\C\\end{bmatrix}\\) is full rank</li> </ol> <p>Assume \\((A,C)\\) is observable, find \\(x_0\\) from \\(\\mathbf y_k\\).</p> \\[ \\begin{aligned} y_0 &amp;= Cx_0 \\\\ y_1 &amp;= Cx_1 = CAx_0 \\\\ &amp;\\ \\ \\vdots \\\\ y_{n-1} &amp;= CA^{n-1}x_0 \\end{aligned}\\Rightarrow \\mathbf y_n=\\begin{bmatrix}y_0\\\\y_1\\\\\\vdots\\\\y_{n-1}\\end{bmatrix}=\\begin{bmatrix}C\\\\CA\\\\\\vdots\\\\CA^{n-1}\\end{bmatrix}x_0=M_ox_0 \\] \\[ x_0=(M_o^TM_o)^{-1}M_o^T\\mathbf y_n \\]","tags":["hkust","math"]},{"location":"2025/04/10/linear-algebra--system-theory/#controllability-observability","title":"Controllability &amp; Observability","text":"<p>\\((A,C)\\) is observable if and only if\u00a0\\((A^T,C^T)\\)\u00a0is controllable.</p>","tags":["hkust","math"]},{"location":"2025/05/11/innovations-in-bim-based-localization/","title":"Innovations in BIM-based Localization","text":""},{"location":"2025/05/11/innovations-in-bim-based-localization/#innovations-in-bim-based-localization","title":"Innovations in BIM-based Localization","text":"<ul> <li>H. Yin, J. M. Liew, W. L. Lee, M. H. Ang, Ker-Wei Yeoh, and Justin, \u201cTowards BIM-based robot localization: a real-world case study,\u201d presented at the 39th International Symposium on Automation and Robotics in Construction, Jul. 2022. doi: 10.22260/ISARC2022/0012.</li> <li>H. Yin, Z. Lin, and J. K. W. Yeoh, \u201cSemantic localization on BIM-generated maps using a 3D LiDAR sensor,\u201d Automation in Construction, vol. 146, p. 104641, Feb. 2023, doi: 10.1016/j.autcon.2022.104641.</li> <li>Z. Qiao et al., \u201cSpeak the Same Language: Global LiDAR Registration on BIM Using Pose Hough Transform,\u201d IEEE Transactions on Automation Science and Engineering, pp. 1\u20131, 2025, doi: 10.1109/TASE.2025.3549176.</li> </ul> <p>In traditional SLAM, mapping and localization occur simultaneously, with maps built incrementally. In construction, maps are created once to support long-term operations, and BIM is increasingly favored over CAD. The following studies explore BIM-based localization, semantic consistency, and geometric consistency.</p> <p></p>"},{"location":"2025/05/11/innovations-in-bim-based-localization/#towards-bim-based-localization","title":"Towards BIM-based Localization","text":"<p>This work leverages a workflow that localizes robots in BIM-generated maps, freeing them from the computational complexity and global inconsistency of online SLAM.</p>"},{"location":"2025/05/11/innovations-in-bim-based-localization/#mapping","title":"Mapping","text":"<p>The authors propose a three-step pipeline for each individual floor:</p> <p></p>"},{"location":"2025/05/11/innovations-in-bim-based-localization/#localization","title":"Localization","text":"<p>A point-to-plane ICP-based pose estimation algorithm is used:</p> \\[ ({\\rm R,t})=\\arg\\min_{\\rm(R,t)}\\left(\\sum_{k=1}^K||({\\rm Rp}_k+{\\rm t-q}_k)\\cdot{\\rm n}_k||_2\\right) \\] <p>In the NUS dataset, localization achieves translation errors below\u00a00.2m\u00a0and rotation errors below\u00a02\u00b0\u00a0compared to DLO. However, deviations between as-planned and as-built conditions can lead to sudden drift.</p> <p></p> <p></p>"},{"location":"2025/05/11/innovations-in-bim-based-localization/#why-not-dlo","title":"Why not DLO ?","text":"<p>Direct LiDAR Odometry (DLO) achieves even higher accuracy than other LiDAR SLAM systems, to the point where the authors use it as\u00a0a proxy for ground truth\u00a0in their experiments. So why not rely entirely on DLO?</p> <p>Despite its superior accuracy, DLO still suffers from global inconsistency due to its scan-to-scan matching approach. More importantly, DLO cannot leverage the semantic information contained in BIM models.</p>"},{"location":"2025/05/11/innovations-in-bim-based-localization/#towards-semantic-consistency","title":"Towards Semantic Consistency","text":"<p>This work introduces Semantic ICP to improve localization accuracy in structured environments, which guides LiDAR-based localization to both geometric and semantic consist.</p> <p></p>"},{"location":"2025/05/11/innovations-in-bim-based-localization/#mapping_1","title":"Mapping","text":"<p>Each semantic object in BIM is represented by an axis-aligned bounding box which is parameterized by 2 points \\(\\rm d_{min}=[x_{min},y_{min},z_{min}]^T\\) and \\(\\rm d_{max}=[x_{max},y_{max},z_{max}]^T\\). A\u00a0point is labeled\u00a0if it lies within a box, assigning it the corresponding semantic class. While this approach may introduce minor inaccuracies - such as oversized boxes for non-axis-aligned walls or ambiguous labels for boundary points - the impact on overall localization performance is negligible.</p> <p></p>"},{"location":"2025/05/11/innovations-in-bim-based-localization/#localization_1","title":"Localization","text":"<p>With the guidance of coarse-to-fine, there are three steps to achieving the refined result. First, iterate by standard ICP to get a coarse result. Then, based on that result, the input points are labeled with those boxes. Finally, based on the label, a semantic ICP is applied to refined registration.</p> <p></p> \\[ \\mathbf T=\\arg\\min_{\\mathbf T}\\left(\\sum_{(i,j)} w(\\mathbf x_i^r,\\mathbf x_j^t)\\cdot r(\\mathbf x_i^r,\\mathbf x_j^t)\\right) \\] <p>Which</p> \\[ w_c=\\begin{cases} \\mu,&amp;\\text{same label} \\\\ 1-\\mu,&amp;\\text{otherwise} \\end{cases} \\] <p>In Huan's practice, \\(\\mu\\) is chosen to be \\(0.8\\) while \\(\\delta\\) is chosen to be \\(0.05\\rm (m)\\).</p> \\[ w_p=\\begin{cases} 1,&amp;e(\u00b7)&lt;\\delta\\\\ \\delta/e(\u00b7),&amp;e(\u00b7)\\ge\\delta \\end{cases} \\] <p></p> <p>In the experiment, a 2D offline SOTA SLAM Cartographer is chosen as a proxy for ground truth. Compared to standard ICP, the semantic filter improved the accuracy, while the gap between as-designed and as-built was still unsolved. The conversion from BIM to a semantic map may not be that accurate, which may reduce the robustness. The init guess of \\({\\rm T}_k\\) relies on \\({\\rm T}_{k-1}\\), a significant error in \\({\\rm T}_k\\) would lead to catastrophic sequence.</p>"},{"location":"2025/05/11/innovations-in-bim-based-localization/#towards-geometric-consistency","title":"Towards Geometric Consistency","text":"<p>Submap is composed by a sequence of pointcloud inputs</p> \\[ \\mathcal{S}=\\Gamma\\left(\\bigcup_{k=1}^n\\mathbf T_k\\mathcal P_k,r_v\\right) \\] <p>\\(\\Gamma(\\mathcal S, r)\\) is voxelization downsampling function\uff0c\\(r_v=0.8{\\rm m}\\) represents the voxel size\uff0c\\(n\\) is the minimum number of scans required such that traversed distance exceeds \\(d_s\\) meters.</p> <p>Every voxel is parameterized by Gaussian distribution</p> \\[ \\bar{\\mathbf{p}}=\\frac1N\\sum_{i=1}^N\\mathbf p_i,\\quad\\Sigma=\\frac1N\\sum_{i=1}^N(\\mathbf p_i-\\bar{\\mathbf p})(\\mathbf p_i-\\bar{\\mathbf p})^T \\] <p>To determine whether the points in a voxel belong to a plane, we analyze the eigenvalues of the covariance matrix \\(\\Sigma\\). The first eigenvector (associated with the largest eigenvalue) points towards the greatest variance, while the second and third eigenvectors correspond to the next greatest variances. Hence, \\(\\lambda_2/\\lambda_3\\) will be compared with the threshold \\(\\sigma_\\lambda=10\\) to determine whether the voxel belongs to a planar. If the ratio does not meet the threshold, the voxel is further subdivided into four sub-voxels (? Is it only divided in the horizontal direction? No corresponding code has been found yet), and the plane identification process is repeated until each sub-voxel contains fewer than four points.</p> <p>After this, those voxels with similar normal and similar point-to-plane distances will merge into the same planar. Then wall points are projected onto the ground plane with a pixel scale of \\(s_I=60{\\rm px/m}\\), with a Hough Transform-based line segment detector applied to extract the wall. Short 2D line segments (length under \\(L_\\text{min}=30{\\rm px}\\)) are discarded, while the remaining segments are merged based on endpoint proximity and directional similarity, then refitted to the original points for geometric accuracy. The refitted lines are extended a predefined distance to intersect and form corners, which are further refined using Non-Maximum Suppression (NMS) to avoid clustering.</p> <p></p> <p>The triangle descriptor \\(\\Delta=[||AB||,||BC||,||AC||,\\alpha,\\beta,\\gamma]\\) are quantized based on side length resolution \\(r_s=0.5{\\rm m}\\) and angle resolution \\(r_a=3.0{\\rm deg}\\). Although it leads to a high outlier ratio, the likelihood of candidates is also high. For every corner triplets \\((A,B,C)\\) within the range \\(L_{\\rm max}=30.0{\\rm m}\\text{ or }40.0{\\rm m}\\), a triangle descriptor will be calculated and stored in the hash table. When querying, it nearly costs a constant time.</p> <p></p> <p>Based on the resolution of \\(0.15{\\rm m}\\) for translation and \\(1.0{\\rm deg}\\) for rotation, an accumulated tensor is formed for transforming matching to \\(se(2)\\) parameter space. First, every match will contribute to some elements in that accumulated tensor. Then the highest \\(L=10000\\) elements will merge their neighborhoods' value. For the highest \\(K=5000\\) elements, non-maximum depression is applied to filter out \\(K'\\) elements. Finally, for the rest of \\(J=1500\\) elements, confidence score will calculated for them one by one.</p> <p></p> <p>To determine the optimal transformation aligning a LiDAR submap with a BIM model, an occupancy-aware confidence score is proposed. The BIM wall point cloud\u00a0\\(\\mathcal{P}_B\\)\u200b\u00a0is rasterized into a 2D grid\u00a0\\(\\mathcal{M}\\)\u00a0with resolution\u00a0\\(s_r\\)\u200b, where occupied cells are set to 1 and free cells to 0:\u00a0\\(\\mathcal{M}=\\phi(\\mathcal{P}_B,s_r)\\). To account for as-built deviations,\u00a0\\(\\mathcal{M}\\)\u00a0is dilated using a kernel of size\u00a0\\(k_d\\)\u200b, producing a score field\u00a0\\(\\mathcal{M}_d\\)\u200b\u00a0where cell values decrease linearly with distance to the nearest occupied cell. The score evaluates alignment using two components: an award score\u00a0sasa\u200b\u00a0and a penalty score\u00a0spsp\u200b. The award score rewards matches between non-ground submap points\u00a0\\(Q_{ng}\\)\u00a0and BIM walls:</p> \\[s_a=\\sum_{q\\in Q_{ng}}\\mathcal{M}_d(\u03d5(q,s_r))\\] <p>The penalty score penalizes ground submap points\u00a0\\(Q_g\\) overlapping with BIM walls:</p> \\[ s_p=\\sum_{q\\in Q_g}\\mathcal{M}_d(\\phi(q,s_r)) \\] <p>The final confidence score combines these components</p> \\[ s=\\frac{s_a-\\lambda\\cdot s_p}{|Q_{ng}|} \\] <p>where\u00a0\\(\\lambda\\)\u00a0balances their contributions, and normalization by\u00a0\\(|Q_{ng}\\) ensures robustness to submap size. This score avoids biases from unmatched free spaces or real-world objects not in the BIM, providing a reliable measure of alignment quality.</p> <p></p>"},{"location":"2025/05/11/innovations-in-bim-based-localization/#problems-unsolved","title":"Problems Unsolved","text":"<p>The authors use a clever confidence calculation method to avoid the difference between as-designed and as-built make some effects. It is an implicit way to express the gaps. Do we need to express it explicitly? Can we find an explicit expression for such gaps?</p> <p></p> <p>The second problem is lidar would degenerate in long corridor cases, which leads to inconsistency and failure estimation. Do we need to expand the size of the submap to avoid it? Can we use a topologic graph to avoid it explicitly? Or can we fuse more information from other sensors like cameras or radar?</p>"},{"location":"2025/06/20/derivation-of-on-manifold-imu-preintegration/","title":"Derivation of On-Manifold IMU Preintegration","text":"","tags":["slam","math"]},{"location":"2025/06/20/derivation-of-on-manifold-imu-preintegration/#derivation-of-on-manifold-imu-preintegration","title":"Derivation of On-Manifold IMU Preintegration","text":"<ul> <li>C. Forster, L. Carlone, F. Dellaert, and D. Scaramuzza, \u201cOn-Manifold Preintegration for Real-Time Visual--Inertial Odometry,\u201d IEEE Trans. Robot., vol. 33, no. 1, pp. 1\u201321, Feb. 2017, doi: 10.1109/TRO.2016.2597321.</li> <li>Z. Yang and S. Shen, \u201cMonocular Visual\u2013Inertial State Estimation With Online Initialization and Camera\u2013IMU Extrinsic Calibration,\u201d IEEE Trans. Automat. Sci. Eng., vol. 14, no. 1, pp. 39\u201351, Jan. 2017, doi: 10.1109/TASE.2016.2550621.</li> </ul> <p>IMU preintegration is a technique in visual-inertial odometry that efficiently fuses high-frequency IMU data between keyframes. Using Lie group theory on \\(SE(3)\\), it handles nonlinear 3D rotations and precomputes motion constraints for optimization. This method accounts for sensor biases, noise, and is essential for real-time state estimation.</p> <p></p>","tags":["slam","math"]},{"location":"2025/06/20/derivation-of-on-manifold-imu-preintegration/#special-orthology-group-so3","title":"Special Orthology Group \\(SO(3)\\)","text":"<p>The special orthology group \\(SO(3)\\) is defined as the set of all \\(\\mathbb{R}^{3\\times3}\\) rotation matrix:</p> \\[ SO(3) = \\set{R\\in\\mathbb{R}^{3\\times3}:R^TR=I,\\det(R)=1} \\] <p>And its tagent space on the manifold \\(\\mathfrak {so}(3)\\), consist of skew-symmetric matrices:</p> \\[ \\boldsymbol\\omega^\\wedge = \\begin{bmatrix} \\omega_1 \\\\ \\omega_2 \\\\ \\omega_3 \\end{bmatrix}^\\wedge = \\begin{bmatrix} 0 &amp; -\\omega_3 &amp; \\omega_2 \\\\ \\omega_3 &amp; 0 &amp; -\\omega_1 \\\\ -\\omega_2 &amp; \\omega_1 &amp; 0 \\\\ \\end{bmatrix} \\in \\mathfrak{so}(3) \\tag{1} \\] <p>The hat operator \\((\\cdot)^\\wedge\\) maps a vector \\(\\boldsymbol\\omega\\in\\mathbb{R}^3\\) to \\(\\mathfrak{so}(3)\\), while the vee operator \\((\\cdot)^\\vee\\) performs the inverse mapping.</p> <p>Rodrigues' Rotation Formula</p> <p>Let \\(\\bf v\\in\\mathbb{R}^3\\) be a 3D vector to be rotated by a unit vector axis \\(\\bf n=\\begin{pmatrix}n_x\\\\n_y\\\\n_z\\end{pmatrix}\\) by an angle \\(\\theta\\). The resulting rotated vector is denoted \\(\\bf v_\\text{rot}\\).</p> <p>\\(\\bf v\\) can be decomposed to two orthogonal components: the parallel to \\(\\bf n\\) part \\(\\bf v_\\parallel\\) and the perpendicular to \\(\\bf n\\) part \\(\\bf v_\\perp\\). Which satisfy,</p> \\[ \\begin{cases} \\mathbf v_\\parallel = (\\mathbf v\\cdot\\mathbf n)\\mathbf n \\\\ \\mathbf v_\\perp = \\mathbf v-(\\mathbf v\\cdot\\mathbf n)\\mathbf n = -\\mathbf n\\times(\\mathbf n\\times\\mathbf v) \\end{cases} \\] <p>The whole vector \\(\\bf v\\) is then rotated by:</p> \\[ \\begin{aligned} \\mathbf v_\\text{rot} &amp;= \\mathbf v_{\\parallel \\text{rot}} + \\mathbf v_{\\perp \\text{rot}} \\\\ &amp;= \\mathbf v_\\parallel + \\cos\\theta\\mathbf v_\\perp + \\sin\\theta\\mathbf n\\times\\bf v_\\perp \\\\ &amp;= (\\mathbf v\\cdot\\mathbf n)n + \\cos\\theta\\mathbf v_\\perp + \\sin\\theta\\mathbf n\\times\\mathbf v \\\\ &amp;= \\cos\\theta\\mathbf v+(1-\\cos\\theta)(\\mathbf n\\cdot\\mathbf v)\\mathbf n + \\sin\\theta\\mathbf n\\times\\mathbf v \\end{aligned} \\] <p>We get Rodrigues' Rotation Formula. Write it on the manifold, we then have:</p> \\[ R = \\cos(||\\phi||)I+[1-\\cos(||\\phi||)]\\frac{\\phi\\phi^T}{||\\phi||^2} + \\sin(||\\phi||)\\phi^\\wedge \\] <p>The exponential map \\(\\exp:\\mathfrak {so}(3)\\to SO(3)\\) converts a skew-symmetric matrix into a rotation matrix via the Rodrigues rotation formula:</p> \\[ R = \\exp(\\boldsymbol{\\phi}^\\wedge) = I + \\frac{\\sin \\|\\boldsymbol{\\phi}\\|}{\\|\\boldsymbol{\\phi}\\|} \\boldsymbol{\\phi}^\\wedge + \\frac{1 - \\cos \\|\\boldsymbol{\\phi}\\|}{\\|\\boldsymbol{\\phi}\\|^2} (\\boldsymbol{\\phi}^\\wedge)^2 \\tag{3} \\] <p>For small angles, this simplifies to \\(R \\approx I + \\boldsymbol{\\phi}^\\wedge\\).</p> <p>Exponential Mapping</p> \\[ \\begin{aligned} R &amp;= \\exp(\\phi^\\wedge) \\\\ &amp;= \\sum_{n=0}^\\infty\\frac{(\\phi^\\wedge)^n}{n!} \\\\ &amp;= I + \\bigg(||\\phi||-\\frac{||\\phi||^3}{3!}+\\frac{||\\phi||^5}{5!}+\\cdots\\bigg)\\phi^\\wedge+\\bigg(\\frac{||\\phi||^2}{2!}-\\frac{||\\phi||^4}{4!}+\\frac{||\\phi||^6}{6!}-\\cdots\\bigg)(\\phi^\\wedge)^2 \\\\ &amp;= I + \\frac{\\sin||\\phi||}{||\\phi||}\\phi^\\wedge+\\frac{1-\\cos||\\phi||}{||\\phi||^2}(\\phi^\\wedge)^2 \\\\ \\end{aligned} \\] <p>The logarithmic map \\(\\log : SO(3) \\to \\mathfrak{so}(3)\\) extracts the axis-angle representation from a rotation matrix:</p> \\[ \\boldsymbol{\\phi} = \\log(R)^\\vee = \\frac{\\|\\boldsymbol{\\phi}\\|}{2 \\sin \\|\\boldsymbol{\\phi}\\|} \\begin{bmatrix} r_{32} - r_{23} \\\\ r_{13} - r_{31} \\\\ r_{21} - r_{12} \\end{bmatrix} \\tag{5} \\] <p>Logarithmic Mapping</p> \\[ R= I + \\frac{\\sin||\\phi||}{||\\phi||}\\phi^\\wedge+\\frac{1-\\cos||\\phi||}{||\\phi||^2}(\\phi^\\wedge)^2 \\] <p>So the trace of the rotation matrix satisfied:</p> \\[ \\begin{aligned} {\\rm tr}(R) &amp;= {\\rm tr}(I) + \\frac{\\sin||\\phi||}{||\\phi||}{\\rm tr}(\\phi^\\wedge)+\\frac{1-\\cos||\\phi||}{||\\phi||^2}{\\rm tr}[(\\phi^\\wedge)^2] \\\\ &amp;= 3 + 0 + \\frac{1-\\cos||\\phi||}{||\\phi||^2}(-||\\phi^\\wedge||^2) \\\\ &amp;= 3 + 0 + \\frac{1-\\cos||\\phi||}{||\\phi||^2}(-2||\\phi||^2) \\\\ &amp;= 3 + 2\\cos||\\phi|| - 2 \\end{aligned} \\] <p>The angle \\(\\theta\\) is calculated by</p> \\[ ||\\phi|| = \\arccos\\bigg(\\frac{{\\rm tr}(R) - 1}2\\bigg) + 2k\\pi \\] <p>When \\(\\phi\\ne0\\Leftrightarrow R\\ne I\\), we construct a skew part and a non-skew part.</p> \\[ R=\\underbrace{\\frac{\\sin||\\phi||}{||\\phi||}\\phi^\\wedge}_{\\frac12(R-R^T)}+\\underbrace{I+\\frac{1-\\cos||\\phi||}{||\\phi||^2}(\\phi^\\wedge)^2}_{\\frac12(R+R^T)} \\] <p>So</p> \\[ \\begin{aligned} \\phi &amp;= \\log(R)^\\vee \\\\ &amp;= \\bigg[\\frac{||\\phi||(R-R^T)}{2\\sin||\\phi||}\\bigg]^\\vee \\\\ &amp;= \\frac{||\\phi||}{2\\sin||\\phi||}\\begin{bmatrix} r_{32} - r_{23} \\\\ r_{13} - r_{31} \\\\ r_{21} - r_{12} \\end{bmatrix} \\end{aligned} \\tag{5} \\] <p>For simplicity of notation, \\(\\text{Exp}\\) and \\(\\text{Log}\\) are defined as mappings between vector space \\(\\mathbb{R}^3\\) and Lie Group \\(SO(3)\\), while \\(\\exp\\) and \\(\\log\\) operate between Lie Algebra \\(\\mathfrak{so}(3)\\) and \\(SO(3)\\)</p> <p></p>","tags":["slam","math"]},{"location":"2025/06/20/derivation-of-on-manifold-imu-preintegration/#perturbation-models-and-jacobians","title":"Perturbation Models and Jacobians","text":"<p>For small perturbations \\(\\delta\\phi\\) of exponential and logarithm, we use first order approximation:</p> \\[ \\begin{cases} \\text{Exp}(\\phi+\\delta\\phi)\\approx\\text{Exp}(\\phi)\\text{Exp}(J_r(\\phi)\\delta\\phi) \\\\ \\text{Log}(\\text{Exp}(\\phi)\\text{Exp}(\\delta\\phi))\\approx\\phi+J_r^{-1}(\\phi)\\delta\\phi \\end{cases}\\tag{7,9} \\] <p>The right jacobian and its inverse are given by</p> \\[ \\begin{aligned} J_r(\\phi) &amp;= I - \\frac{1-\\cos(||\\phi||)}{||\\phi||^2}\\phi^\\wedge + \\frac{||\\phi||-\\sin(||\\phi||)}{||\\phi||^3}(\\phi^\\wedge)^2 \\\\ J_r^{-1}(\\phi) &amp;= I + \\frac12\\phi^\\wedge + \\left(\\frac1{||\\phi||^2} + \\frac{1+\\cos(||\\phi||)}{2||\\phi||\\sin(||\\phi||)}\\right)(\\phi^\\wedge)^2 \\\\ \\end{aligned} \\tag{8} \\] <p>Perturbation Jacobians</p> <p>Since a general increment cannot be defined on the special orthogonal group \\(R_1+R_2\\not\\in SO(3),R_1,R_2\\in SO(3)\\), we use perturbation models defined above.</p> \\[ \\text{Exp}(\\phi+\\Delta\\phi) = \\text{Exp}(\\mathbf  J_l\\Delta\\phi)\\text{Exp}(\\phi) = \\text{Exp}(\\phi) = \\text{Exp}(\\mathbf  J_r \\Delta\\phi) \\tag{7} \\] <p>The Lie bracket (binary operator on Lie groups) is defined as:</p> \\[ [A,B] = AB-BA \\] <p>Using the Baker-Campbell-Hausdorff (BCH) formula:</p> \\[ \\begin{aligned} &amp;\\log(\\text{Exp}(\\alpha)\\text{Exp}(\\beta))\\\\ =&amp;\\log(AB)\\\\=&amp;\\sum_{n=1}^\\infty\\frac{(-1)^{n-1}}n\\sum_{r_i+s_i&gt;0,i\\in[1,n]}\\frac{(\\sum_{i=1}^n(r_i+s_i))^{-1}}{\\Pi_{i=1}^n(r_i!s_i!)}[A^{r_1}B^{s_1}A^{r_2}B^{s_2}\\cdots A^{r_n}B^{s_n}]\\\\ =&amp;A+B+\\frac12[A,B]+\\frac1{12}[A,[A,B]]-\\frac1{12}[B,[A,B]]+\\cdots\\\\ \\approx&amp; \\begin{cases} \\beta+J_l(\\beta)^{-1}\\alpha,&amp;\\alpha\\to0\\\\ \\alpha+J_r(\\alpha)^{-1}\\beta,&amp;\\beta\\to0\\\\ \\end{cases} \\end{aligned}\\tag{9} \\] <p>Additional Jacobians:</p> \\[ \\begin{aligned} J_l(\\phi) &amp;= I + \\frac{1-\\cos(||\\phi||)}{||\\phi||^2}\\phi^\\wedge + \\frac{||\\phi||-\\sin(||\\phi||)}{||\\phi||^3}(\\phi^\\wedge)^2 \\\\ J_r(\\phi) &amp;= I - \\frac{1-\\cos(||\\phi||)}{||\\phi||^2}\\phi^\\wedge + \\frac{||\\phi||-\\sin(||\\phi||)}{||\\phi||^3}(\\phi^\\wedge)^2 \\\\ J_l^{-1}(\\phi) &amp;= I - \\frac12\\phi^\\wedge + \\left(\\frac1{||\\phi||^2} + \\frac{1+\\cos(||\\phi||)}{2||\\phi||\\sin(||\\phi||)}\\right)(\\phi^\\wedge)^2 \\\\ J_r^{-1}(\\phi) &amp;= I + \\frac12\\phi^\\wedge + \\left(\\frac1{||\\phi||^2} + \\frac{1+\\cos(||\\phi||)}{2||\\phi||\\sin(||\\phi||)}\\right)(\\phi^\\wedge)^2 \\\\ \\end{aligned} \\] <p>For any vector \\(v\\in\\mathbb{R}^3\\), using the properties of the cross product and the special orthogonal group, we have:</p> \\[ (Rp)^\\wedge v = (Rp) \\times v = (Rp) \\times (RR^{-1}v) = R[p\\times (R^{-1}v)] = Rp^\\wedge R^Tv \\] <p>Since \\(Rp^\\wedge R^T=(Rp)^\\wedge\\) holds for each term in the Taylor expansion of the exponential map, it follows that:</p> \\[ R\\exp(\\phi^\\wedge)R^T = \\exp((R\\phi)^\\wedge) \\] <p>Equivalently:</p> \\[ R\\text{Exp}(\\phi)R^T=\\text{Exp}(R\\phi) \\tag{10} \\]","tags":["slam","math"]},{"location":"2025/06/20/derivation-of-on-manifold-imu-preintegration/#uncertainty-description-in-so3","title":"Uncertainty Description in \\(SO(3)\\)","text":"<p>An intuitive way to define uncertainty on rotation matrices is to right-multiply the matrix by a small perturbation that follows a normal distribution:</p> \\[ \\tilde R=R\\cdot\\text{Exp}(\\epsilon),\\ \\epsilon\\in\\mathcal N(0,\\Sigma)\\tag{12} \\] <p>For Gaussian distributions, we have the normalization condition:</p> \\[ \\int_{\\mathbb{R}^3}p(\\epsilon){\\rm d}\\epsilon = \\int_{\\mathbb{R}^3}\\frac1{\\sqrt{(2\\pi)^3\\det(\\Sigma)}}\\cdot\\exp\\bigg({-\\frac12||\\epsilon||^2_\\Sigma}\\bigg){\\rm d}\\epsilon=1 \\tag{13} \\] <p>Substituting \\(\\epsilon=\\text{Log}(R^{-1}\\tilde R)\\), we obtain</p> \\[ \\int_{SO(3)}\\frac1{\\sqrt{(2\\pi)^3\\det(\\Sigma)}}\\cdot\\exp\\bigg({-\\frac12\\big|\\big|\\text{Log}(R^{-1}\\tilde R)\\big|\\big|^2_\\Sigma}\\bigg)\\left|\\frac{{\\rm d}\\epsilon}{{\\rm d}\\tilde R}\\right|{\\rm d}\\tilde R=1 \\] <p>The scaling factor, known as the Jacobian determinant, is given by the right-perturbation model:</p> \\[ \\left|\\frac{{\\rm d}\\epsilon}{{\\rm d}\\tilde R}\\right| = \\left|\\frac1{J_r(\\text{Log}(R^{-1}\\tilde R))}\\right| \\] <p>Rewriting gives:</p> \\[ \\int_{SO(3)}\\frac1{\\sqrt{(2\\pi)^3\\det(\\Sigma)}}\\cdot\\left|\\frac1{J_r(\\text{Log}(R^{-1}\\tilde R))}\\right|\\cdot\\exp\\bigg({-\\frac12\\big|\\big|\\text{Log}(R^{-1}\\tilde R)\\big|\\big|^2_\\Sigma}\\bigg){\\rm d}\\tilde R=1\\tag{14} \\] <p>From this, the probability density function on \\(SO(3)\\) is:</p> \\[ p(\\tilde R) =\\frac1{\\sqrt{(2\\pi)^3\\det(\\Sigma)}}\\cdot\\left|\\frac1{J_r(\\text{Log}(R^{-1}\\tilde R))}\\right|\\cdot\\exp\\bigg({-\\frac12\\big|\\big|\\text{Log}(R^{-1}\\tilde R)\\big|\\big|^2_\\Sigma}\\bigg) \\tag{15} \\] <p>For small perturbations, the normalization term \\(\\frac1{\\sqrt{(2\\pi)^3\\det(\\Sigma)}}\\cdot\\left|\\frac1{J_r(\\text{Log}(R^{-1}\\tilde R))}\\right|\\) can be approximated as constant, leading to the following expression for the negative log-likelihood:</p> \\[ \\begin{aligned} \\mathcal L(R) &amp;= \\frac12\\big|\\big|\\text{Log}(R^{-1}\\tilde R)\\big|\\big|^2_\\Sigma + c \\\\ &amp;= \\frac12\\big|\\big|\\text{Log}(\\tilde R^{-1}R)\\big|\\big|^2_\\Sigma + c \\end{aligned} \\tag{16} \\]","tags":["slam","math"]},{"location":"2025/06/20/derivation-of-on-manifold-imu-preintegration/#gauss-newton-method-on-manifolds","title":"Gauss-Newton Method on Manifolds","text":"<p>For standard Gauss-Newton optimization:</p> \\[ \\mathbf x^* = \\arg\\min_{\\mathbf x} f(\\mathbf x) \\Rightarrow \\mathbf x^* = \\mathbf x + \\arg\\min_{\\Delta\\mathbf x}f(\\mathbf x+\\Delta\\mathbf x) \\] <p>On manifolds, this becomes:</p> \\[ x^*=\\arg\\min_{x\\in\\mathcal M} f(x) \\Rightarrow x^* = \\mathcal R_x\\cdot\\arg\\min_{\\delta x\\in\\mathbb{R}^n} f(\\mathcal R_x(\\delta x))\\tag{18} \\] <p>Where \\(R_x(\\cdot)\\) is a retraction mapping from the tangent space to the manifold.</p> <p>In the case of the \\(SO(3)\\) group, the retraction is defined as:</p> \\[ \\mathcal R_R(\\delta\\phi) = R\\cdot\\text{Exp}(\\delta\\phi),\\ \\delta\\phi\\in\\mathbb{R}^3\\tag{20} \\] <p>For the \\(SE(3)\\) group, it is:</p> \\[ \\mathcal R_T(\\delta\\phi,\\delta\\mathbf p)=\\begin{bmatrix}R\\cdot\\text{Exp}(\\delta\\phi) &amp; \\mathbf p+R\\cdot\\delta\\mathbf p\\end{bmatrix},\\ \\begin{bmatrix}\\delta\\phi \\\\ \\delta\\mathbf p\\end{bmatrix}\\in\\mathbb{R}^6\\tag{21} \\]","tags":["slam","math"]},{"location":"2025/06/20/derivation-of-on-manifold-imu-preintegration/#imu-preintegration","title":"IMU Preintegration","text":"<p>The state of the system at time \\(k\\) is represented by the IMU's orientation, position, velocity, and sensor biases</p> \\[ {\\rm x}_k = [{\\rm R}_{wb_k}(\\mathbf{q}_{wb_k}),\\mathbf{p}_{wb_k},\\mathbf{v}_k^w,\\mathbf{b}_g^{b_k},\\mathbf{b}_a^{b_k}]\\tag{22} \\] <p>Let \\(\\hat{\\mathbf{a}}^b(t)\\) and \\(\\hat{\\boldsymbol{\\omega}}^b(t)\\) denote the measurements from the three-axis accelerometer and gyroscope, respectively. These are corrupted by noise and time-varying biases:</p> \\[ \\begin{aligned} \\hat{\\boldsymbol\\omega}^{b}(t) &amp;= \\boldsymbol\\omega^{b}(t) + \\mathbf{b}_g^{b}(t)+\\mathbf{n}_g^{b}(t) \\\\ \\hat{\\mathbf{a}}^{b}(t) &amp;= {\\rm R}_{bw}(t)[\\mathbf{a}^w(t)+\\mathbf g^w] + \\mathbf{b}_a^{b}(t)+\\mathbf{n}_a^{b}(t) \\\\ \\end{aligned}\\tag{27, 28} \\] <p>In this notation, the superscript \\(w\\) refers to the world (inertial) frame, while \\(b\\) denotes the body (sensor) frame. The subscripts \\(a\\) and \\(g\\) refer to the accelerometer and gyroscope, respectively.</p> <p>The time dirivatives of \\({\\rm R},\\mathbf p,\\mathbf v\\) are given as:</p> \\[ \\begin{aligned} \\dot{\\mathbf{p}}_{wb}(t) &amp;= \\mathbf v^w(t) \\\\ \\dot{\\mathbf{v}}^w(t) &amp;= \\mathbf a^w(t) \\\\ \\dot{\\rm R}_{wb}(t) &amp;= {\\rm R}_{wb}(t) \\cdot\\text{Exp}[\\boldsymbol\\omega^{b}(t)] \\\\ \\dot{\\mathbf{q}}_{wb}(t) &amp;= \\mathbf{q}_{wb}(t)\\otimes\\begin{bmatrix}0\\\\\\frac12\\boldsymbol\\omega^{b}(t)\\end{bmatrix} \\end{aligned}\\tag{29} \\] <p>Using these dynamics, we can express the system state at time \\(t+\\Delta t\\) as follows:</p> \\[ \\begin{aligned} % position \\mathbf{p}_{wb}(t+\\Delta t) &amp;= \\mathbf p_{wb}(t) + \\mathbf{v}^w(t)\\cdot\\Delta t + \\iint_t^{t+\\Delta t}\\mathbf{a}^w(\\tau){\\rm d}\\tau^2 \\\\ &amp;= \\mathbf p_{wb}(t) + \\mathbf{v}^w(t)\\cdot\\Delta t + \\iint_t^{t+\\Delta t}\\left[{\\rm R}_{wb}(\\tau)(\\hat{\\mathbf{a}}^b(\\tau)-\\mathbf b^b_a(\\tau)-\\mathbf{n}^b_a(\\tau))-\\mathbf{g}^w\\right]{\\rm d}\\tau^2 \\\\ &amp;= \\mathbf p_{wb}(t) + \\mathbf{v}^w(t)\\cdot\\Delta t - \\frac12\\mathbf{g}^w\\cdot(\\Delta t)^2 + \\iint_t^{t+\\Delta t}{\\rm R}_{wb}(\\tau)\\left[\\hat{\\mathbf{a}}^b(\\tau)-\\mathbf b^b_a(\\tau)-\\mathbf{n}^b_a(\\tau)\\right]{\\rm d}\\tau^2 \\\\ % velocity \\mathbf{v}^w(t+\\Delta t) &amp;= \\mathbf{v}^w(t) + \\int_t^{t+\\Delta t}\\mathbf{a}^w(\\tau){\\rm d}\\tau \\\\ &amp;= \\mathbf{v}^w(t) + \\int_t^{t+\\Delta t}\\left[{\\rm R}_{wb}(\\tau)(\\hat{\\mathbf{a}}^b(\\tau)-\\mathbf b^b_a(\\tau)-\\mathbf{n}^b_a(\\tau))-\\mathbf{g}^w\\right]{\\rm d}\\tau^2 \\\\ &amp;= \\mathbf{v}^w(t) - \\mathbf{g}^w\\cdot\\Delta t + \\int_t^{t+\\Delta t}{\\rm R}_{wb}(\\tau)\\left[\\hat{\\mathbf{a}}^b(\\tau)-\\mathbf b^b_a(\\tau)-\\mathbf{n}^b_a(\\tau)\\right]{\\rm d}\\tau^2 \\\\ % rotation matrix {\\rm R}_{wb}(t+\\Delta t) &amp;= \\int_t^{t+\\Delta t}{\\rm R}_{wb}(\\tau)\\cdot\\text{Exp}(\\boldsymbol\\omega^b(\\tau)){\\rm d}\\tau \\\\ &amp;= \\int_t^{t+\\Delta t} {\\rm R}_{wb}(\\tau) \\cdot \\text{Exp}\\left(\\hat{\\boldsymbol\\omega}^b(\\tau) - \\mathbf{b}^b_g(\\tau) - \\mathbf{n}^b_g(\\tau)\\right) {\\rm d}\\tau \\\\ % quaternion \\mathbf{q}_{wb}(t+\\Delta t) &amp;= \\int_t^{t+\\Delta t} \\mathbf{q}_{wb}(\\tau) \\otimes \\begin{bmatrix} 0 \\\\ \\frac12\\boldsymbol\\omega^b(t) \\end{bmatrix} {\\rm d}\\tau \\\\ &amp;= \\int_t^{t+\\Delta t} \\mathbf{q}_{wb}(\\tau) \\otimes \\begin{bmatrix} 0 \\\\ \\frac12\\left(\\hat{\\boldsymbol\\omega}^b(\\tau) - \\mathbf{b}^b_g(\\tau) - \\mathbf{n}^b_g(\\tau)\\right) \\end{bmatrix} {\\rm d}\\tau \\end{aligned} \\] <p>However, recomputing \\({\\rm R}_{wb}(\\tau)\\) at each step leads to repeated integration and unnecessary computational cost. To mitigate this, we use the identity:</p> \\[ {\\rm R}_{wb}(\\tau) = {\\rm R}_{wb_t} \\cdot {\\rm R}_{b_tb}(\\tau) \\] <p>This allows us to factor out \\({\\rm R}_{wb_i}\\) from the integrals over the interval \\([t_i,t_j]\\), resulting in:</p> \\[ \\begin{aligned} {\\rm R}_{wb_j} &amp;= {\\rm R}_{wb_i} \\cdot \\int_{t_i}^{t_j} {\\rm R}_{b_ib}(\\tau) \\cdot \\text{Exp}\\left(\\hat{\\boldsymbol\\omega}^b(\\tau) - \\mathbf{b}^b_g(\\tau) - \\mathbf{n}^b_g(\\tau)\\right) {\\rm d}\\tau \\\\ \\mathbf{q}_{wb_j} &amp;= \\mathbf{q}_{wb_i} \\otimes \\int_{t_i}^{t_j} \\mathbf{q}_{b_ib}(\\tau) \\otimes \\begin{bmatrix} 0 \\\\ \\frac12\\left(\\hat{\\boldsymbol\\omega}^b(\\tau) - \\mathbf{b}^b_g(\\tau) - \\mathbf{n}^b_g(\\tau)\\right) \\end{bmatrix} {\\rm d}\\tau \\\\ \\mathbf{p}_{wb_j} &amp;= \\mathbf{p}_{wb_i} + \\mathbf{v}^w_i\\cdot\\Delta t - \\frac12\\mathbf{g}^w\\cdot(\\Delta t)^2 + {\\rm R}_{wb_i}\\iint_{t_i}^{t_j}{\\rm R}_{b_ib}(\\tau)\\left[\\hat{\\mathbf{a}}^b(\\tau)-\\mathbf b^b_a(\\tau)-\\mathbf{n}^b_a(\\tau)\\right]{\\rm d}\\tau^2 \\\\ \\mathbf{v}^w_j &amp;= \\mathbf{v}^w_i - \\mathbf{g}^w\\cdot\\Delta t + {\\rm R}_{wb_i}\\int_{t_i}^{t_j}{\\rm R}_{b_ib}(\\tau)\\left[\\hat{\\mathbf{a}}^b(\\tau)-\\mathbf b^b_a(\\tau)-\\mathbf{n}^b_a(\\tau)\\right]{\\rm d}\\tau \\\\ \\end{aligned} \\] <p>We define the following preintegrated terms:</p> \\[ \\begin{aligned} \\boldsymbol\\alpha_{b_ib_j} &amp;= \\iint_{t_i}^{t_j}{\\rm R}_{b_ib}(\\tau)\\left[\\hat{\\mathbf{a}}^b(\\tau)-\\mathbf b^b_a(\\tau)-\\mathbf{n}^b_a(\\tau)\\right]{\\rm d}\\tau^2 \\\\ \\boldsymbol\\beta_{b_ib_j} &amp;= \\int_{t_i}^{t_j}{\\rm R}_{b_ib}(\\tau)\\left[\\hat{\\mathbf{a}}^b(\\tau)-\\mathbf b^b_a(\\tau)-\\mathbf{n}^b_a(\\tau)\\right]{\\rm d}\\tau \\\\ \\boldsymbol\\gamma_{b_ib_j} &amp;= \\int_{t_i}^{t_j} {\\rm R}_{b_ib}(\\tau) \\cdot \\text{Exp}\\left(\\hat{\\boldsymbol\\omega}^b(\\tau) - \\mathbf{b}^b_g(\\tau) - \\mathbf{n}^b_g(\\tau)\\right) {\\rm d}\\tau \\\\ \\end{aligned} \\] <p>Thus, the final update equations become:</p> \\[ \\begin{aligned} \\mathbf{p}_{wb_j} &amp;= \\mathbf{p}_{wb_i} + \\mathbf{v}_i^w\\Delta t - \\frac12\\mathbf{g}^w\\Delta t^2 + {\\rm R}_{wb_i}\\boldsymbol{\\alpha}_{b_ib_j} \\\\ \\mathbf{v}_j^w &amp;= \\mathbf{v}_i^w - \\mathbf{g}^w\\Delta t + {\\rm R}_{wb_i}\\boldsymbol{\\beta}_{b_ib_j} \\\\ {\\rm R}_{wb_j} &amp;= {\\rm R}_{wb_i} \\cdot \\boldsymbol{\\gamma}_{b_ib_j} \\end{aligned} \\] <p>Finally, since the gyroscope and accelerometer biases are modeled as Gaussian white noise processes with zero mean:</p> \\[ \\dot{\\mathbf{b}}\\sim\\mathcal{N}(0,\\Sigma) \\] <p>we assume that the bias remains approximately constant between two consecutive time steps:</p> \\[ \\mathbf{b}_g^{b_i}=\\mathbf{b}_g^{b_j},\\quad\\mathbf{b}_a^{b_i}=\\mathbf{b}_a^{b_j},\\quad\\forall{i,j} \\]","tags":["slam","math"]},{"location":"2025/04/30/sparsity-extended-information-filter-slam/","title":"Sparsity Extended Information Filter SLAM","text":"","tags":["slam","math"]},{"location":"2025/04/30/sparsity-extended-information-filter-slam/#sparsity-extended-information-filter-slam","title":"Sparsity Extended Information Filter SLAM","text":"<p>M. R. Walter, R. M. Eustice, and J. J. Leonard, \u201cExactly Sparse Extended Information Filters for Feature-based SLAM,\u201d The International Journal of Robotics Research, vol. 26, no. 4, pp. 335\u2013359, Apr. 2007, doi: 10.1177/0278364906075026.</p> <p>\u7a00\u758f\u6269\u5c55\u4fe1\u606f\u6ee4\u6ce2\u5668\uff08SEIF\uff09SLAM \u662f\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u540c\u65f6\u5b9a\u4f4d\u4e0e\u5730\u56fe\u6784\u5efa\uff08SLAM\uff09\u65b9\u6cd5\u3002\u901a\u8fc7\u5229\u7528\u4fe1\u606f\u77e9\u9635\u7684\u7a00\u758f\u6027\uff0cSEIF \u964d\u4f4e\u4e86\u72b6\u6001\u4f30\u8ba1\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u73af\u5883\u3002</p> <p></p>","tags":["slam","math"]},{"location":"2025/04/30/sparsity-extended-information-filter-slam/#gaussian-probability","title":"Gaussian Probability","text":"","tags":["slam","math"]},{"location":"2025/04/30/sparsity-extended-information-filter-slam/#duality-of-covariance-and-information","title":"Duality of Covariance and Information","text":"<p>\u5bf9\u670d\u4ece\u9ad8\u65af\u5206\u5e03\u7684\u591a\u5143\u968f\u673a\u53d8\u91cf \\(\\mathbf\\xi_t\\sim\\mathcal N(\\boldsymbol\\mu_t,\\Sigma_t)\\) \u53ef\u4ee5\u901a\u8fc7\u5747\u503c\u5411\u91cf \\(\\boldsymbol\\mu_t\\) \u548c\u534f\u65b9\u5dee\u77e9\u9635 \\(\\Sigma_t\\) \u53c2\u6570\u5316\uff0c\u540c\u65f6\u4e5f\u53ef\u4ee5\u88ab\u89c4\u8303\u5f62\u5f0f \\(\\mathcal N^{-1}(\\boldsymbol\\eta_t,\\Lambda_t)\\) \u6240\u8868\u793a\uff0c\u5176\u4e2d \\(\\Lambda_t=\\Sigma_t^{-1},\\boldsymbol\\eta_t=\\Sigma^{-1}_t\\boldsymbol\\mu_t\\).</p> \\[ \\begin{aligned} p(\\xi_t) &amp;= N(\\mu_t, \\Sigma_t) \\\\ &amp;\\propto \\exp\\left\\{-\\frac{1}{2}(\\xi_t - \\mu_t)^\\top \\Sigma_t^{-1}(\\xi_t - \\mu_t)\\right\\} \\\\ &amp;= \\exp\\left\\{-\\frac{1}{2}(\\xi_t^\\top \\Sigma_t^{-1} \\xi_t - 2\\mu_t^\\top \\Sigma_t^{-1} \\xi_t + \\mu_t^\\top \\Sigma_t^{-1} \\mu_t)\\right\\} \\\\ &amp;\\propto \\exp\\left\\{-\\frac{1}{2}\\xi_t^\\top \\Sigma_t^{-1} \\xi_t + \\mu_t^\\top \\Sigma_t^{-1} \\xi_t\\right\\} \\\\ &amp;= \\exp\\left\\{-\\frac{1}{2}\\xi_t^\\top \\Lambda_t \\xi_t + \\eta_t^\\top \\xi_t\\right\\} \\propto N^{-1}(\\eta_t, \\Lambda_t) \\end{aligned} \\] <p>\u5728\u6807\u51c6\u5f62\u5f0f\u4e2d\uff0c\u8fb9\u7f18\u5316\u64cd\u4f5c\u53ea\u9700\u4ece\u5747\u503c\u5411\u91cf\u548c\u534f\u65b9\u5dee\u77e9\u9635\u4e2d\u79fb\u9664\u76f8\u5e94\u7684\u5143\u7d20\u3002\u7136\u800c\u5728\u89c4\u8303\u5f62\u5f0f\u4e2d\uff0c\u8fb9\u7f18\u5316\u64cd\u4f5c\u9700\u8981\u8ba1\u7b97\u8212\u5c14\u8865\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u8f83\u9ad8\u3002\u6761\u4ef6\u5316\u5219\u76f8\u53cd\uff0c\u5728\u6807\u51c6\u5f62\u5f0f\u4e2d\u64cd\u4f5c\u8f83\u4e3a\u590d\u6742\uff0c\u5728\u89c4\u8303\u5f62\u5f0f\u4e0b\u5219\u76f8\u5bf9\u7b80\u5355\u3002</p> <p></p>","tags":["slam","math"]},{"location":"2025/04/30/sparsity-extended-information-filter-slam/#implied-conditional-independence","title":"Implied Conditional Independence","text":"\\[ \\begin{aligned} p(\\boldsymbol\\xi)&amp;\\propto\\exp\\left\\{-\\frac12\\boldsymbol\\xi^T\\Lambda\\boldsymbol\\xi+\\boldsymbol\\eta^T\\boldsymbol\\xi\\right\\} \\\\ &amp;= \\exp\\left\\{\\sum_i\\left(\\eta_i\\xi_i-\\frac12\\sum_j\\xi_i\\lambda_{ij}\\xi_j\\right)\\right\\} \\\\ &amp;=\\prod_i\\exp\\left\\{-\\frac12\\lambda_{ii}\\xi_i^2+\\eta_i\\xi_i\\right\\}\\cdot\\prod_{i\\neq j}\\exp\\left\\{-\\frac12\\xi_i\\lambda_{ij}\\xi_j\\right\\} \\\\ &amp;= \\prod_i\\Psi_i(\\xi_i)\\cdot\\prod_{i\\ne j}\\Psi_{ij}(\\xi_i,\\xi_j) \\end{aligned} \\] <p>\u5176\u4e2d</p> \\[ \\begin{aligned} \\Psi_i(\\xi_i) &amp;= \\exp\\left\\{-\\frac12\\lambda_{ii}\\xi_i^2+\\eta_i\\xi_i\\right\\} \\\\ \\Psi_{ij}(\\xi_i,\\xi_j) &amp;= \\exp\\left\\{-\\frac12\\xi_i\\lambda_{ij}\\xi_j\\right\\} \\end{aligned} \\] <p>\u201cThe meaning of a zero in an inverse covariance matrix (at location \\(i, j\\)) is conditional on all the other variables, these two variables \\(i\\) and \\(j\\) are independent. ... So positive off-diagonal terms in the covariance matrix always describe positive correlation; but the off-diagonal terms in the inverse covariance matrix can\u2019t be interpreted that way. The sign of an element \\((i, j)\\) in the inverse covariance matrix does not tell you about the correlation between those two variables.\u201d (MacKay and Cb, 2006, p. 4)</p> <p></p> <p>\u5982\u679c\u4fe1\u606f\u77e9\u9635\u4e2d\u7684\u975e\u5bf9\u89d2\u5143\u7d20\u4e3a\u96f6\uff0c\u5373 \\(\\lambda_{ij} = 0 \\Leftrightarrow \\Psi_{ij}(\\xi_i, \\xi_j) = 1\\)\uff0c\u8fd9\u610f\u5473\u7740\u4e24\u4e2a\u8282\u70b9\u4e4b\u95f4\u6ca1\u6709\u8fb9\u7ea6\u675f\uff0c\u8868\u660e \\(\\xi_i\\) \u548c \\(\\xi_j\\) \u6761\u4ef6\u72ec\u7acb\u3002\u76f8\u53cd\uff0c\u5982\u679c\u975e\u5bf9\u89d2\u5143\u7d20\u4e0d\u4e3a\u96f6\uff0c\u5219\u8868\u660e \\(\\xi_i\\) \u548c \\(\\xi_j\\) \u4e4b\u95f4\u5b58\u5728\u4e00\u6761\u8fb9\u7ea6\u675f\uff0c\u5176\u5f3a\u5ea6\u6b63\u6bd4\u4e8e \\(\\lambda_{ij}\\)\u3002\u8fd9\u79cd\u5173\u7cfb\u5f88\u597d\u5730\u4f53\u73b0\u5728\u65e0\u5411\u56fe\u4e2d\uff0c\u76f4\u89c2\u5730\u53cd\u6620\u4e86\u53d8\u91cf\u4e4b\u95f4\u7684\u6761\u4ef6\u72ec\u7acb\u6027\u3002\u4f7f\u7528\u89c4\u8303\u5f62\u5f0f\u7684\u4e00\u4e2a\u4e3b\u8981\u597d\u5904\u662f\uff0c\u4fe1\u606f\u77e9\u9635 \\(\\Lambda\\)\u00a0 \u63d0\u4f9b\u4e86\u9a6c\u5c14\u53ef\u592b\u573a\u7684\u663e\u5f0f\u7ed3\u6784\u8868\u793a\uff0c\u6e05\u6670\u5730\u63ed\u793a\u4e86\u53d8\u91cf\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002\u5173\u4e8e\u534f\u65b9\u5dee\u77e9\u9635\u548c\u4fe1\u606f\u77e9\u9635\u7684\u66f4\u591a\u6df1\u5165\u7406\u89e3\uff0c\u53ef\u4ee5\u53c2\u8003 David J.C. MacKay 2006 \u5e74\u7684\u624b\u7a3f The Humble Gaussian Distribution.</p>","tags":["slam","math"]},{"location":"2025/04/30/sparsity-extended-information-filter-slam/#extended-information-filter","title":"Extended Information Filter","text":"\\[ p(\\boldsymbol\\xi_t|\\mathbf z^t,\\mathbf u^t)=\\mathcal N(\\boldsymbol\\mu_t,\\Sigma_t)=\\mathcal N^{-1}(\\boldsymbol\\eta_t,\\Lambda_t) \\] <p>\u8bb0\u72b6\u6001 \\(\\boldsymbol\\xi_t=[\\mathbf x_t^T\\quad\\mathbf M_t^T]^T\\) \u4e3a\u673a\u5668\u4eba\u4f4d\u59ff\u4e3a \\(\\mathbf x_t\\) \u548c\u5730\u56fe\u7279\u5f81 \\(\\mathbf M=\\set{\\mathbf m_1,\\cdots,\\mathbf m_n}\\) \u7684\u7ec4\u5408\uff0c\\(\\mathbf z^{1:t}\\) \u548c \\(\\mathbf u^{1:t}\\) \u8868\u793a\u89c2\u6d4b\u6570\u636e\u548c\u8f93\u5165\u7684\u5386\u53f2\u3002\u5730\u56fe\u57fa\u4e8e\u4fe1\u606f\u77e9\u9635\u7684\u7ed3\u6784\u88ab\u5212\u5206\u4e3a\u4e24\u4e2a\u96c6\u5408\uff0c\\(\\mathbf M = (\\mathbf m^+,\\mathbf m^-)\\)\uff0c\u5176\u4e2d \\(\\mathbf m^+\\) \u5305\u542b\u90a3\u4e9b\u4e0e\u673a\u5668\u4eba\u5b58\u5728\u975e\u96f6\u975e\u5bf9\u89d2\u9879\u8fde\u63a5\u7684\u5730\u56fe\u5143\u7d20\uff0c\u800c \\(\\mathbf m^-\\) \u5219\u8868\u793a\u4e0e\u8f66\u8f86\u4f4d\u59ff\u6761\u4ef6\u72ec\u7acb\u7684\u7279\u5f81\u3002</p>","tags":["slam","math"]},{"location":"2025/04/30/sparsity-extended-information-filter-slam/#measurement-update-step","title":"Measurement Update Step","text":"<p>\u89c2\u6d4b\u5bf9\u51cf\u5c0f\u673a\u5668\u4f4d\u59ff\u548c\u5730\u56fe\u7684\u4f30\u8ba1\u7684\u4e0d\u786e\u5b9a\u6027\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u5728\u5747\u503c\u5904\u5bf9\u975e\u7ebf\u6027\u89c2\u6d4b\u6a21\u578b\u505a\u4e00\u9636\u8fd1\u4f3c</p> \\[ \\begin{aligned} \\mathbf z_t &amp;= \\mathbf h(\\boldsymbol\\xi_t)+\\mathbf v_t \\\\ &amp;\\approx\\mathbf h(\\bar{\\boldsymbol\\mu}_t)+H(\\boldsymbol\\xi_t-\\bar{\\boldsymbol\\mu}_t)+\\mathbf v_t,\\quad\\mathbf v_t\\sim\\mathcal N(0,R) \\end{aligned} \\] <p>\u6839\u636e\u9a6c\u5c14\u79d1\u592b\u5047\u8bbe \\(p(\\mathbf z_t|\\boldsymbol\\xi_t,\\mathbf z_{1:t-1},\\mathbf u_{1:t})=p(\\mathbf z_t|\\boldsymbol\\xi_t)\\) \u548c \\(\\forall\\boldsymbol\\xi_t,p(\\mathbf z_t|\\mathbf z_{1:t-1}\\mathbf u_{1:t})=\\frac1\\eta\\)\uff0c\u8d1d\u53f6\u65af\u5b9a\u7406\u7ed9\u51fa</p> \\[ \\begin{aligned} p(\\boldsymbol\\xi_t|\\mathbf z_{1:t},\\mathbf u_{1:t}) &amp;= p(\\boldsymbol\\xi_t|\\mathbf z_t,\\mathbf z_{1:t-1},\\mathbf u_{1:t}) \\\\ &amp;= \\frac{p(\\mathbf z_t|\\boldsymbol\\xi_t,\\mathbf z_{1:t-1},\\mathbf u_{1:t})\\cdot p(\\boldsymbol\\xi_t|\\mathbf z_{1:t-1},\\mathbf u_{1:t})}{p(\\mathbf z_t|\\mathbf z_{1:t-1},\\mathbf u_{1:t})} \\\\ &amp;= \\eta\\cdot p(\\mathbf z_t|\\boldsymbol\\xi_t)\\cdot p(\\boldsymbol\\xi_t|\\mathbf z_{1:t-1},\\mathbf u_{1:t})\\\\ &amp;= \\mathcal N^{-1}(\\boldsymbol\\eta_t,\\Lambda_t) \\end{aligned} \\] <p>\u5728\u66f4\u65b0\u65f6\uff0cEIF \u4f30\u8ba1\u89c4\u8303\u5f62\u5f0f\u7684\u65b0\u7684\u540e\u9a8c\u6982\u7387</p> \\[ \\begin{aligned} \\Lambda_t &amp;= \\bar\\Lambda_t+H^TR^{-1}H \\\\ \\boldsymbol\\eta_t &amp;= \\bar{\\boldsymbol\\eta}_t + H^TR^{-1}(\\mathbf z_t-\\mathbf h(\\bar{\\boldsymbol\\mu}_t)+H\\bar{\\boldsymbol\\mu}_t) \\end{aligned} \\] <p>\u5176\u4e2d\u6d4b\u91cf\u6a21\u578b\u662f\u4e00\u4e2a\u53ea\u5305\u542b\u673a\u5668\u5f53\u524d\u4f4d\u59ff\u4ee5\u53ca\u73b0\u5728\u89c2\u6d4b\u5230\u7684\u5730\u6807\u7684\u51fd\u6570\uff0c\u5728\u96c5\u53ef\u6bd4\u4e2d\u8868\u73b0\u4e3a\u6781\u5ea6\u7a00\u758f\uff08\u6ca1\u89c2\u6d4b\u5230\u7684\u5730\u6807\u7684\u68af\u5ea6\u4e3a \\(0\\)\uff09</p> \\[ H=\\begin{bmatrix} \\frac{\\partial h_1}{\\partial\\boldsymbol\\xi_t} &amp; \\cdots &amp; \\mathbf 0 &amp; \\cdots &amp; \\frac{\\partial h_1}{\\partial\\mathbf m_i} &amp; \\cdots &amp; \\mathbf 0 \\\\ \\vdots &amp; &amp; &amp; \\ddots &amp; &amp; &amp; \\vdots \\\\ \\frac{\\partial h_m}{\\partial\\boldsymbol\\xi_t} &amp; \\cdots &amp; \\frac{\\partial h_m}{\\partial\\mathbf m_j} &amp; \\cdots &amp; \\mathbf 0 &amp; \\cdots &amp; \\mathbf 0 \\\\ \\end{bmatrix} \\] <p>\u4fe1\u606f\u77e9\u9635\u901a\u8fc7\u7a00\u758f\u77e9\u9635 \u00a0\\(H^TR^{-1}H\\) \u8fdb\u884c\u66f4\u65b0\uff0c\u4ec5\u4fee\u6539\u4e0e\u673a\u5668\u4eba\u4f4d\u59ff\u548c\u89c2\u6d4b\u5730\u6807\u76f8\u5173\u7684\u9879\uff0c\u4ece\u800c\u52a0\u5f3a\u6216\u5efa\u7acb\u65b0\u7684\u7ea6\u675f\uff1b\u7531\u4e8e \u00a0\\(H\\)\u00a0 \u7684\u7a00\u758f\u6027\u548c\u673a\u5668\u4eba\u89c6\u91ce\u7684\u9650\u5236\uff0c\u66f4\u65b0\u65f6\u95f4\u4e0e\u89c2\u6d4b\u6570\u91cf \u00a0\\(m\\)\u00a0 \u76f8\u5173\uff0c\u590d\u6742\u5ea6\u4e3a \u00a0\\(\\mathcal O(m^2)\\)\uff0c\u4e14\u4e0d\u968f\u5730\u56fe\u89c4\u6a21\u589e\u957f\uff1b\u5728\u975e\u7ebf\u6027\u60c5\u51b5\u4e0b\uff0c\u8ba1\u7b97\u5747\u503c\u9700\u8981 \u00a0\\(\\mathcal O(n^3)\\) \u7684\u77e9\u9635\u6c42\u9006\uff0c\u800c\u5728\u7ebf\u6027\u60c5\u51b5\u4e0b\uff0c\u66f4\u65b0\u53ef\u4ee5\u5728\u5e38\u6570\u65f6\u95f4\u5185\u5b8c\u6210\uff0c\u65e0\u9700\u5747\u503c\u8ba1\u7b97\u3002</p>","tags":["slam","math"]},{"location":"2025/04/30/sparsity-extended-information-filter-slam/#time-projection-step","title":"Time Projection Step","text":"<p>\u65f6\u95f4\u9884\u6d4b\u5305\u62ec\u72b6\u6001\u6269\u5c55\u548c\u8fb9\u7f18\u5316\u3002\u9996\u5148\u53c2\u8003\u89c2\u6d4b\u6a21\u578b\uff0c\u5bf9\u4e8e\u8fd0\u52a8\u5b66\u6a21\u578b\u6211\u4eec\u540c\u6837\u7ed9\u51fa\u4e00\u9636\u8fd1\u4f3c</p> \\[ \\begin{aligned} \\mathbf x_{t+1} &amp;= \\mathbf f(\\mathbf x_t,\\mathbf u_{t+1})+\\mathbf w_t \\\\ &amp;\\approx \\mathbf f(\\boldsymbol\\mu_{\\mathbf x_t},\\mathbf u_{t+1}) + F(\\mathbf x_t - \\boldsymbol\\mu_{\\mathbf x_t}) + \\mathbf w_t,\\quad\\mathbf w_t\\sim\\mathcal N(0,Q) \\end{aligned} \\] <p>\u9996\u5148\u5c06\u65b0\u7684\u673a\u5668\u4eba\u4f4d\u59ff\u52a0\u5165\u72b6\u6001\u5411\u91cf\uff0c\u5e76\u540c\u6b65\u6269\u5c55\u4fe1\u606f\u77e9\u9635\u548c\u4fe1\u606f\u5411\u91cf\u3002\u5176\u4e2d\u72b6\u6001\u5411\u91cf \\(\\hat{\\boldsymbol\\xi}_{t+1}=[\\mathbf x_t^T,\\mathbf x_{t+1}^T,\\mathbf M^T]^T\\) \u9075\u5faa\u540e\u9a8c\u5206\u5e03 \\(p(\\boldsymbol\\xi_t|\\mathbf z_{1:t},\\mathbf u_{1:t}) = \\mathcal N^{-1}(\\boldsymbol\\eta_t,\\Lambda_t)\\). \u5982\u56fe\u6240\u793a\uff0c\u6839\u636e\u9a6c\u5c14\u79d1\u592b\u6027\u8d28\uff0c\u65b0\u7684\u4f4d\u59ff\u53ea\u548c\u4e0a\u4e00\u6b65\u4f4d\u59ff\u76f8\u5173\u800c\u4e0e\u5730\u56fe\u65e0\u5173\u3002</p> \\[ \\begin{aligned} p(\\hat{\\boldsymbol\\xi}_{t+1}|\\mathbf z_{1:t},\\mathbf u_{1:t+1}) &amp;= p(\\mathbf x_{t+1},\\boldsymbol\\xi_t|\\mathbf z_{1:t},\\mathbf u_{1:t+1}) \\\\ &amp;= p(\\mathbf x_{t+1}|\\mathbf x_t,\\mathbf u_{t+1})\\cdot p(\\boldsymbol\\xi_t|\\mathbf z_{1:t},\\mathbf u_{1:t}) \\end{aligned} \\] <p></p> <p>\u65b0\u7684\u72b6\u6001\u4f30\u8ba1\u670d\u4ece\u66f4\u65b0\u540e\u7684\u9ad8\u65af\u5206\u5e03</p> \\[ p(\\mathbf x_t,\\mathbf x_{t+1},\\mathbf M|\\mathbf z_{1:t},\\mathbf u_{1:t+1})=\\mathcal N(\\hat{\\boldsymbol\\mu}_{t+1},\\hat\\Sigma_{t+1})=\\mathcal N^{-1}(\\hat{\\boldsymbol\\eta}_{t+1},\\hat\\Lambda_{t+1}) \\] \\[ \\begin{aligned} \\hat\\Sigma_{t+1} &amp;= \\left[\\begin{array} {c | c c} \\Sigma_{\\mathbf x_t\\mathbf x_t} &amp; \\rm F\\Sigma_{\\mathbf x_t\\mathbf x_t} &amp; \\rm F\\Sigma_{\\mathbf x_t\\mathbf M} \\\\ \\hline \\rm\\Sigma_{\\mathbf x_t\\mathbf x_t}F^T &amp; \\rm F\\Sigma_{\\mathbf x_t\\mathbf x_t}F^T+Q &amp; \\Sigma_{\\mathbf x_t\\mathbf M} \\\\ \\rm\\Sigma_{\\mathbf M\\mathbf x_t}F^T &amp; \\Sigma_{\\mathbf M\\mathbf x_t} &amp; \\Sigma_{\\mathbf M\\mathbf M} \\end{array}\\right]=\\left[\\begin{array} {c | c} \\hat\\Sigma_{t+1}^{11} &amp; \\hat\\Sigma_{t+1}^{12} \\\\ \\hline \\hat\\Sigma_{t+1}^{21} &amp; \\hat\\Sigma_{t+1}^{22} \\end{array}\\right]\\\\ \\hat{\\boldsymbol\\mu}_{t+1} &amp;= \\left[\\begin{array} c \\boldsymbol\\mu_{\\mathbf x_t} \\\\ \\hline \\mathbf f(\\boldsymbol\\mu_{\\mathbf x_t,\\mathbf u_{t+1}}) \\\\ \\boldsymbol\\mu_{\\mathbf M} \\end{array}\\right]=\\left[\\begin{array} c \\hat{\\boldsymbol\\mu}_{t+1}^1\\\\\\hline\\hat{\\boldsymbol\\mu}_{t+1}^2 \\end{array}\\right] \\end{aligned} \\] <p>\u7531\u534f\u65b9\u5dee\u77e9\u9635\u548c\u4fe1\u606f\u77e9\u9635\u7684\u5bf9\u5076\u6027\u5f97\u5230</p> \\[ \\begin{aligned} \\hat\\Lambda_{t+1} &amp;= \\left[\\begin{array} {c | c c} \\Lambda_{\\mathbf x_t\\mathbf x_t}+\\rm FQ^{-1}F &amp; \\rm-F^TQ^{-1} &amp; \\Lambda_{\\mathbf x_t\\mathbf M} \\\\ \\hline \\rm-Q^{-1}F &amp; \\rm Q^{-1} &amp; 0 \\\\ \\Lambda_{\\mathbf M\\mathbf x_t} &amp; 0 &amp; \\Lambda_{\\mathbf M\\mathbf M} \\end{array}\\right] = \\left[\\begin{array} {c | c} \\hat\\Lambda_{t+1}^{11} &amp; \\hat\\Lambda_{t+1}^{12} \\\\ \\hline \\hat\\Lambda_{t+1}^{21} &amp; \\hat\\Lambda_{t+1}^{22} \\end{array}\\right]\\\\ \\hat{\\boldsymbol\\eta}_{t+1} &amp;= \\left[\\begin{array} c \\boldsymbol\\eta_{\\mathbf x_t}-\\rm F^TQ^{-1}[\\mathbf f(\\boldsymbol\\mu_{\\mathbf x_t,\\mathbf u_{t+1}})-F\\boldsymbol\\mu_{\\mathbf x_t}] \\\\ \\hline \\rm Q^{-1}[\\mathbf f(\\boldsymbol\\mu_{\\mathbf x_t,\\mathbf u_{t+1}})-F\\boldsymbol\\mu_{\\mathbf x_t}] \\\\ \\boldsymbol\\eta_{\\mathbf M} \\end{array}\\right]=\\left[\\begin{array} c \\hat{\\boldsymbol\\eta}_{t+1}^1\\\\\\hline\\hat{\\boldsymbol\\eta}_{t+1}^2 \\end{array}\\right] \\end{aligned} \\] <p>\u7b2c\u4e8c\u6b65\u662f\u8fb9\u7f18\u5316 \\(\\mathbf x_t\\), \u4f7f\u72b6\u6001\u5411\u91cf\u53d8\u4e3a \\(\\boldsymbol\\xi_{t+1}=[\\mathbf x_{t+1}^T,\\mathbf M^T]^T\\).</p> \\[ \\begin{aligned} p(\\mathbf x_{t+1},\\mathbf M|\\mathbf z_{1:t},\\mathbf u_{1:t+1}) &amp;= \\int_{\\mathbf x_t}p(\\mathbf x_t,\\mathbf x_{t+1},\\mathbf M|\\mathbf z_{1:t},\\mathbf u_{1:t+1}){\\rm d}\\mathbf x_t \\\\ p(\\boldsymbol\\xi_{t+1}|\\mathbf z_{1:t},\\mathbf u_{1:t+1}) &amp;=\\mathcal N^{-1}(\\bar{\\boldsymbol\\eta}_{t+1},\\bar\\Lambda_{t+1}) \\end{aligned} \\] <p>\\(\\bar{\\boldsymbol\\eta}_{t+1}\\) \u548c \\(\\bar\\Lambda_{t+1}\\) \u7531\u524d\u9762\u8868\u4e2d\u7ed9\u51fa\u7684\u8fb9\u7f18\u5316\u516c\u5f0f\u5f97\u5230</p> \\[ \\begin{aligned} \\bar\\Lambda_{t+1} &amp;= \\hat\\Lambda_{t+1}^{22} - \\hat\\Lambda_{t+1}^{21}(\\hat\\Lambda_{t+1}^{11})^{-1}\\hat\\Lambda_{t+1}^{12} \\\\ \\bar{\\boldsymbol\\eta}_{t+1} &amp;= \\hat{\\boldsymbol\\eta}_{t+1}^2 - \\hat\\Lambda_{t+1}^{21}(\\hat\\Lambda_{t+1}^{11})^{-1}\\hat{\\boldsymbol\\eta}_{t+1}^1 \\end{aligned} \\] <p>\u867d\u7136 EIF \u80fd\u9ad8\u6548\u5904\u7406\u65b0\u89c2\u6d4b\u7684\u589e\u91cf\u66f4\u65b0\uff0c\u4f46\u901a\u8fc7\u8fb9\u7f18\u5316\u65e7\u4f4d\u59ff\u65f6\u4ea7\u751f\u7684\u5168\u8fde\u63a5\u95ee\u9898\u4f1a\u5bfc\u81f4\u4fe1\u606f\u77e9\u9635\u8fc5\u901f\u7a20\u5bc6\u5316\uff0c\u4f7f\u5f97\u8fd0\u52a8\u9884\u6d4b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u8fbe\u5230 \\(\\mathcal O(n^2)\\) \u91cf\u7ea7\u3002\u8fb9\u7f18\u5316\u8fc7\u7a0b\u4f1a\u5728\u65b0\u4f4d\u59ff\u4e0e\u88ab\u79fb\u9664\u65e7\u4f4d\u59ff\u5173\u8054\u7684\u6240\u6709\u7279\u5f81 \\(\\mathbf m^+\\) \u4e4b\u95f4\u5efa\u7acb\u65b0\u7684\u4fe1\u606f\u8fde\u63a5\uff0c\u5bfc\u81f4\u4fe1\u606f\u77e9\u9635\u7a20\u5bc6\u5316\u3002\u4f46\u7531\u4e8e\u8fd9\u4e9b\u65b0\u8fde\u63a5\u901a\u5e38\u5177\u6709\u8f83\u5f31\u7684\u5173\u8054\u5f3a\u5ea6\uff0c\u8fd9\u4e3a\u901a\u8fc7\u7a00\u758f\u5316\u8fd1\u4f3c\u6765\u7ef4\u6301\u8ba1\u7b97\u6548\u7387\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u5373\u4fdd\u7559\u5f3a\u8fde\u63a5\u820d\u5f03\u5f31\u8fde\u63a5\u3002</p>","tags":["slam","math"]},{"location":"2025/04/30/sparsity-extended-information-filter-slam/#sparse-extended-information-filter","title":"Sparse Extended Information Filter","text":"","tags":["slam","math"]},{"location":"2025/04/30/sparsity-extended-information-filter-slam/#active-sparsity-maintenance","title":"Active Sparsity Maintenance","text":"<p>\u8bb0\u539f\u5148\u6fc0\u6d3b\u540e\u7eed\u53d8\u4e3a\u88ab\u52a8\u7684\u7279\u5f81\u4e3a \\(\\mathbf m^0\\)\uff0c\u5219\u5730\u56fe\u88ab\u5212\u5206\u4e3a \\(\\mathbf M=\\set{\\mathbf m^0,\\mathbf m^+,\\mathbf m^-}\\) \u4e09\u90e8\u5206\u3002\u4e0b\u56fe\u8868\u660e\u901a\u8fc7\u4e3b\u52a8\u63a7\u5236\u6fc0\u6d3b\u7279\u5f81\u65ad\u5f00\uff0c\u53ef\u4ee5\u6709\u6548\u63a7\u5236\u4fe1\u606f\u77e9\u9635\u7684\u7a00\u758f\u6027\u3002\u800c\u5bf9\u4e0e\u673a\u5668\u65e0\u5173\u8054\u7684\u5730\u6807 \\(\\mathcal m^-\\)\uff0c\u6211\u4eec\u53ef\u4ee5\u4efb\u610f\u7ed9\u51fa\u4f30\u8ba1 \\(\\boldsymbol\\phi\\)\uff0c\u4f46\u5b9e\u9645\u4e0a\u5982\u679c\u7528\u4e86\u975e\u5747\u503c\u7684\u4f30\u8ba1\u4f1a\u4f7f SEIF \u5931\u51c6\u3002</p> <p>SEIF \u7ed9\u51fa\u53bb\u9664 \\(\\mathbf m^0\\) \u540e\u7684\u8fd1\u4f3c\u540e\u9a8c\u4f30\u8ba1</p> \\[ \\begin{aligned} \\tilde p_\\text{SEIF}(\\boldsymbol\\xi_t|\\mathbf z_{1:t},\\mathbf u_{1:t}) &amp;= \\tilde p_\\text{SEIF}(\\mathbf x_t,\\mathbf m^0,\\mathbf m^+,\\mathbf m^-|\\mathbf z_{1:t},\\mathbf u_{1:t}) \\\\ &amp;= p(\\mathbf x_t|\\mathbf m^+,\\mathbf m^-=\\boldsymbol\\phi,\\mathbf z_{1:t},\\mathbf u_{1:t})\\cdot p(\\mathbf m^0,\\mathbf m^+,\\mathbf m^-,\\mathbf z_{1:t},\\mathbf u_{1:t}) \\end{aligned} \\] <p></p>","tags":["slam","math"]},{"location":"2025/04/30/sparsity-extended-information-filter-slam/#discussion-on-overconfidence","title":"Discussion on Overconfidence","text":"<p>\u5047\u8bbe\u4e09\u5143\u53d8\u91cf \\([a,b,c]\\) \u670d\u4ece\u9ad8\u65af\u5206\u5e03</p> \\[ \\begin{aligned} p(a,b,c) &amp;= \\mathcal N\\left(\\begin{bmatrix}\\mu_a\\\\\\mu_b\\\\\\mu_c\\end{bmatrix},\\begin{bmatrix}\\sigma_a^2&amp;\\rho_{ab}\\sigma_a\\sigma_b&amp;\\rho_{ac}\\sigma_a\\sigma_c\\\\\\rho_{ab}\\sigma_a\\sigma_b&amp;\\sigma_b^2&amp;\\rho_{bc}\\sigma_b\\sigma_c\\\\\\rho_{ac}\\sigma_a\\sigma_c&amp;\\rho_{bc}\\rho_b\\rho_c&amp;\\sigma_c^2\\end{bmatrix}\\right) \\\\ &amp;= \\mathcal N^{-1}\\left(\\begin{bmatrix}\\eta_a\\\\\\eta_b\\\\\\eta_c\\end{bmatrix},\\begin{bmatrix}\\lambda_{aa}&amp;\\lambda_{ab}&amp;\\lambda_{ac}\\\\\\lambda_{ab}&amp;\\lambda_{bb}&amp;\\lambda_{bc}\\\\\\lambda_{ac}&amp;\\lambda_{bc}&amp;\\lambda_{cc}\\end{bmatrix}\\right) \\end{aligned} \\] <p>\u5e76\u4e14\u53d8\u91cf \\(a,b\\) \u5728\u6761\u4ef6 \\(c\\) \u4e0b\u72ec\u7acb\uff0c\u5c06\u8fd1\u4f3c\u7ed3\u679c\u8bb0\u4e3a \\(\\tilde p(a,b,c)\\)</p> \\[ p(a,b,c)=p(a,b|c)\\cdot p(c)\\approx p(a|c)\\cdot p(b|c)\\cdot p(c)=\\tilde p(a,b,c) \\] <p>\u5bf9\u4e8e\u5728 \\(c\\) \u6761\u4ef6\u4e0b\u6761\u4ef6\u72ec\u7acb\u7684 \\(a\\) \u548c \\(b\\), \u524d\u9762\u8ba8\u8bba\u8fc7\u5408\u9002\u7684\u505a\u6cd5\u662f\u628a\u4fe1\u606f\u77e9\u9635\u7684 \\(\\lambda_{ab}\\) \u8bbe\u4e3a \\(0\\), \u8fd9\u7b49\u4ef7\u4e8e\u534f\u65b9\u5dee\u77e9\u9635\u53d8\u4e3a</p> \\[ \\tilde p(a,b,c) = \\mathcal N\\left(\\begin{bmatrix}\\mu_a\\\\\\mu_b\\\\\\mu_c\\end{bmatrix},\\begin{bmatrix}\\sigma_a^2&amp;\\rho_{ac}\\rho_{bc}\\sigma_a\\sigma_b&amp;\\rho_{ac}\\sigma_a\\sigma_c\\\\\\rho_{ac}\\rho_{bc}\\sigma_a\\sigma_b&amp;\\sigma_b^2&amp;\\rho_{bc}\\sigma_b\\sigma_c\\\\\\rho_{ac}\\sigma_a\\sigma_c&amp;\\rho_{bc}\\rho_b\\rho_c&amp;\\sigma_c^2\\end{bmatrix}\\right) \\\\ \\] <p>\u4e3a\u4e86\u4fdd\u8bc1\u8fd1\u4f3c\u540e\u7684\u4e00\u81f4\u6027\uff0c\u4e00\u4e2a\u5145\u8981\u6761\u4ef6\u662f \\(\\bar\\Sigma-\\Sigma\\) \u534a\u6b63\u5b9a</p> \\[ \\bar\\Sigma-\\Sigma=\\begin{bmatrix} 0 &amp; (\\rho_{ac}\\rho_{bc}-\\rho_{ab})\\sigma_a\\sigma_b &amp; 0 \\\\ (\\rho_{ac}\\rho_{bc}-\\rho_{ab})\\sigma_a\\sigma_b &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}\\succeq0 \\] <p>\u5176\u4e2d\uff0c\u4e00\u4e2a\u4f7f \\(\\bar\\Sigma-\\Sigma\\) \u534a\u6b63\u5b9a\u7684\u5fc5\u8981\u6761\u4ef6\u662f\u5de6\u4e0a\u7684 \\(2\\times2\\) \u5b50\u77e9\u9635\u975e\u8d1f\u3002</p> \\[ \\text{det}\\left(\\begin{bmatrix} 0 &amp; (\\rho_{ac}\\rho_{bc}-\\rho_{ab})\\sigma_a\\sigma_b \\\\ (\\rho_{ac}\\rho_{bc}-\\rho_{ab})\\sigma_a\\sigma_b &amp; 0 \\\\ \\end{bmatrix}\\right)=-[(\\rho_{ac}\\rho_{bc}-\\rho_{ac})\\sigma_a\\sigma_b]^2\\le0 \\] <p>\u53ea\u6709\u5728 \\(\\rho_{ab}=\\rho_{ac}\\rho_{bc}\\) \u65f6 \\(\\bar\\Sigma-\\Sigma\\) \u534a\u6b63\u5b9a\uff0c\u5426\u5219\u5f3a\u5236\u7a00\u758f\u5316\u4f1a\u5bfc\u81f4\u4fe1\u606f\u77e9\u9635\u8fc7\u4e8e\u81ea\u4fe1\uff0c\u5373\u4fe1\u606f\u77e9\u9635\u88ab\u8fc7\u5ea6\u5f3a\u5316\uff0c\u5bfc\u81f4\u4f30\u8ba1\u7684\u534f\u65b9\u5dee\u6bd4\u5b9e\u9645\u5c0f\u3002</p>","tags":["slam","math"]},{"location":"2025/02/11/classic-visual-feature-descriptors/","title":"Classic Visual Feature Descriptors","text":"","tags":["slam"]},{"location":"2025/02/11/classic-visual-feature-descriptors/#classic-visual-feature-descriptors","title":"Classic Visual Feature Descriptors","text":"<p>Experiments show that different visual descriptors tend to have similar rates of outliers in feature matching, and the precision differences brought by the application of different feature descriptors in Visual SLAM are negligible.</p> <p></p>","tags":["slam"]},{"location":"2025/02/11/classic-visual-feature-descriptors/#sift-scale-invariant-feature-transform","title":"SIFT: Scale-invariant Feature Transform","text":"<p>Scale invariance is mainly ensured by the introduction of convolution kernels of varying sizes during feature detection, which will not be discussed here. How, then, is rotation invariance achieved?</p> <p>Firstly, during detection, the magnitude and direction of the gradient for each pixel are pre-calculated.</p> \\[ \\begin{aligned} m(x,y) &amp;= \\sqrt{[L(x+1,y)-L(x-1,y)]^2+[L(x,y+1)-L(x,y-1)]^2} \\\\ \\theta(x,y) &amp;= \\arctan\\frac{L(x,y+1)-L(x,y-1)}{L(x+1,y)-L(x-1,y)} \\end{aligned} \\] <p>Next, according to the gradient direction of the pixels, the product of the gradient magnitudes is added to the histogram with weights based on the normal distribution probability of the distance from the center point. The direction in the histogram that corresponds to the interval from the maximum value to the 80% of maximum value is identified as the orientation of the feature point (if there are multiple candidate directions within this interval, then multiple feature points with the same other attributes are created and assigned these directions).</p> <p></p> <p>\"First the image gradient magnitudes and orientations are sampled around the keypoint location, using the scale of the keypoint to select the level of Gaussian blur for the image. In order to achieve orientation invariance, the coordinates of the descriptor and the gradient orientations are rotated relative to the keypoint orientation.\" (Lowe, 2004, p. 15)</p> <p>To pursue rotation invariance, all gradients are rotated so that the main gradient (the one with the longest magnitude) points upward. At the same time, to avoid errors caused by luminance changes, gradient magnitudes that exceed a certain threshold are clipped and then normalized.</p> <p></p>","tags":["slam"]},{"location":"2025/02/11/classic-visual-feature-descriptors/#surf-speeded-up-robust-features","title":"SURF: Speeded Up Robust Features","text":"<p>Compared to SIFT, which uses histograms to find the main orientation, SURF determines the main orientation of the feature point by calculating the Harr wavelet responses within a radius of \\(6s\\) around the feature point, weighing and accumulating the \\(x\\) and \\(y\\) direction responses within \\(60\\) degree sectors.</p> <p></p> <p>Around the feature point, a square box with a side length of \\(20s\\) is taken, and its orientation is aligned with the main orientation of the feature point. This box is divided into \\(16\\) sub-regions, and for each sub-region, the Haar wavelet features of \\(25\\) pixels relative to the main orientation are calculated, including the sum of the horizontal and vertical values and absolute values.</p> <p></p>","tags":["slam"]},{"location":"2025/02/11/classic-visual-feature-descriptors/#brief-binary-robust-independent-elementary-features","title":"BRIEF: Binary Robust Independent Elementary Features","text":"<p>The idea behind BRIEF is very straightforward.</p> <p>\"Our approach is inspired by earlier work [9, 15] that showed that image patches could be effectively classified on the basis of a relatively small number of pairwise intensity comparisons.\" (Hutchison et al., 2010, p. 3)</p> <p>Define the binary intensity comparison test as</p> \\[ \\tau(\\mathbf p;\\mathbf x,\\mathbf y) = \\begin{cases} 1 &amp; \\mathbf p(\\mathbf x)&lt;\\mathbf p(\\mathbf y)\\\\ 0 &amp; \\mathbf p(\\mathbf x)\\ge\\mathbf p(\\mathbf y)\\\\ \\end{cases} \\] <p>The results of the binary intensity tests are combined in a predetermined order (which can be randomly selected like <code>G I</code>, <code>G II</code>, <code>G III</code>, or systematically selected like <code>G V</code>) to form an intensity code:</p> \\[ f_{n_d}(\\mathbf p) = \\sum_{1\\le i\\le n_d}2^{i-1}\\cdot\\tau(\\mathbf p;\\mathbf x_i,\\mathbf y_i) \\] <p></p>","tags":["slam"]},{"location":"2025/02/11/classic-visual-feature-descriptors/#brisk-binary-robust-invariant-scalable-keypoints","title":"BRISK: Binary Robust Invariant Scalable Keypoints","text":"<p>BRISK basically continues the idea of binary intensity tests.</p> <p></p> <p>For \\(N\\) feature points, consider forming \\(N\\cdot(N-1)/2\\) pairs of points, and their gradients are given by the luminance difference:</p> \\[ \\mathbf g(\\mathbf p_i,\\mathbf p_j) = (\\mathbf p_j-\\mathbf p_i)\\cdot\\frac{I(\\mathbf p_j,\\sigma_j)-I(\\mathbf p_i,\\sigma_i)}{||\\mathbf p_j-\\mathbf p_i||^2} \\] <p>All point pairs constitute a set \\(\\mathcal A\\), which is further divided into short-range pairs and long-range pairs based on the distance between the points:</p> \\[ \\begin{aligned} \\mathcal S &amp;= \\set{(\\mathbf p_i,\\mathbf p_j)\\in\\mathcal A\\ \\vert ||\\mathbf p_i-\\mathbf p_j||&lt;\\delta_\\max} \\subseteq \\mathcal A \\\\ \\mathcal L &amp;= \\set{(\\mathbf p_i,\\mathbf p_j)\\in\\mathcal A\\ \\vert ||\\mathbf p_i-\\mathbf p_j||&gt;\\delta_\\min} \\subseteq \\mathcal A \\end{aligned} \\] <p>The gradient of the feature points is given by the average gradient of the long-range point pairs:</p> \\[ \\mathbf g=\\begin{pmatrix} g_x\\\\g_y \\end{pmatrix}=\\frac1L\\cdot\\sum_{(\\mathbf p_i,\\mathbf p_j)\\in\\mathcal L}\\mathbf g(\\mathbf p_i,\\mathbf p_j) \\] <p>Similarly, to pursue rotation invariance, all points are rotated around the feature point, and then a boolean descriptor is constructed from the short-range pairs based on binary intensity tests, similar to BRIEF.</p>","tags":["slam"]},{"location":"2025/02/11/classic-visual-feature-descriptors/#experimental-results","title":"Experimental Results","text":"<p>Several studies have evaluated the performance of different feature descriptors in Visual SLAM applications. Here we summarize some key findings:</p>","tags":["slam"]},{"location":"2025/02/11/classic-visual-feature-descriptors/#performance-evaluation-of-visual-slam-using-several-feature-extractors","title":"Performance Evaluation of Visual SLAM Using Several Feature Extractors","text":"<p>In a study by Klippenstein and Zhang (2009), Harris, KLT, and SIFT were evaluated using average normalized error and accumulated uncertainty metrics:</p> \\[ \\bar\\epsilon_k = \\frac1N\\sum_{i=1}^N|| \\mathbf r_k^{(i)} - \\hat{\\mathbf r}_k^{(i)} ||_{\\Sigma_{r,k}^{-1}}^2 \\] \\[ \\overline{\\text{AU}} = \\frac1N\\sum_{i=1}^N\\sum_{k=1}^{N_s}\\frac43\\pi\\sqrt{\\det(\\Sigma_{k,r})} \\] <p></p> <p></p> <p>The results showed that in most indoor scenarios, there were no significant differences in accumulated uncertainty among the three feature extractors. The average normalized error followed similar trends, with nearly identical performance. SIFT showed slightly better performance in terms of accumulated uncertainty, though the study did not include feature matching comparisons.</p>","tags":["slam"]},{"location":"2025/02/11/classic-visual-feature-descriptors/#evaluation-of-rgb-d-slam-system","title":"Evaluation of RGB-D SLAM System","text":"<p>Endres et al. (2012) conducted experiments on the FR1 dataset and found that:</p> <ul> <li>SIFT performed well but was computationally expensive</li> <li>ORB was computationally efficient and handled viewpoint changes well</li> <li>SURF required careful threshold adjustment to maintain an appropriate number of feature points</li> <li>Too few SURF features led to inaccurate motion estimation and failures</li> <li>Too many features slowed down matching and increased false positives</li> </ul> <p></p> <p>The study also revealed that when there were incorrect edges in the graph, the mapping results deteriorated. The authors suggested future improvements in keypoint matching strategies, such as adding feature dictionaries, pruning unmatched features, and directly using keypoints as landmarks for nonlinear optimization.</p>","tags":["slam"]},{"location":"2025/02/11/classic-visual-feature-descriptors/#comparison-of-feature-descriptors-for-visual-slam","title":"Comparison of Feature Descriptors for Visual SLAM","text":"<p>Hartmann et al. (2013) conducted experiments on both RGBD datasets and their own datasets, finding that:</p> <ul> <li>SIFT achieved the best performance but was the most computationally intensive</li> <li>BRIEF performed best among binary descriptors</li> <li>In most cases, the choice of descriptor had minimal impact on accuracy</li> </ul> <p></p> <p>These experimental results suggest that while different feature descriptors have their own characteristics, their impact on the overall performance of Visual SLAM systems might be less significant than initially thought. The choice of descriptor should be based on a balance between computational efficiency and the specific requirements of the application.</p>","tags":["slam"]},{"location":"2025/02/11/classic-visual-feature-descriptors/#reference","title":"Reference","text":"Image Link https://miro.medium.com/v2/0*HrkMSiOEvpWjyq2N.jpg https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6keFD3eBZBqtWStykos5pZimIdojq2hIfJJEdOIneS7ssXf2YyNvlkMuVcXK-SE7gCp2VO1Aqj3-eGme-Z1lN_FW9KqT3mS-29c0PbEqbEBY5OonC089GRDemZfn92-W6Mm_OSg/s1600/sift_pic https://miro.medium.com/v2/0*UDe_M_7xSVNM8_h_.jpg https://miro.medium.com/v2/0*nda8uDh7EYfGtbJW.png https://miro.medium.com/v2/0*bTfQfO4qOxk3qL78 https://juliaimages.org/ImageFeatures.jl/v0.0.3/img/brisk_pattern.png","tags":["slam"]},{"location":"2025/12/04/java-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E8%BF%9C%E5%BE%81/","title":"Java \u9762\u5411\u5bf9\u8c61\u8fdc\u5f81","text":"","tags":["bilibili","java","oop"]},{"location":"2025/12/04/java-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E8%BF%9C%E5%BE%81/#java","title":"Java \u9762\u5411\u5bf9\u8c61\u8fdc\u5f81","text":"<p>\u8fd9\u662f\u4e00\u7cfb\u5217\u5173\u4e8e Java \u9762\u5411\u5bf9\u8c61\u7f16\u7a0b\u7684\u7cfb\u5217\u89c6\u9891\uff0c\u6db5\u76d6\u4e86\u4ece\u57fa\u7840\u6982\u5ff5\u5230\u9ad8\u7ea7\u5e94\u7528\u7684\u5404\u4e2a\u65b9\u9762\u3002\u6bcf\u4e2a\u89c6\u9891\u65f6\u957f\u7ea6\u4e3a 10 \u5206\u949f\uff0c\u65e8\u5728\u5e2e\u52a9\u521d\u5b66\u8005\u548c\u6709\u7ecf\u9a8c\u7684\u5f00\u53d1\u8005\u66f4\u597d\u5730\u7406\u89e3\u548c\u638c\u63e1 Java \u9762\u5411\u5bf9\u8c61\u7f16\u7a0b\u7684\u6838\u5fc3\u539f\u7406\u548c\u5b9e\u8df5\u6280\u5de7\uff0c\u4e3a\u6784\u5efa\u5927\u578b\u8f6f\u4ef6\u7cfb\u7edf\u6253\u4e0b\u575a\u5b9e\u7684\u57fa\u7840\u3002\u5e0c\u671b\u4f60\u559c\u6b22\uff01</p> \u7c7b\u548c\u5bf9\u8c61 \u4e09\u5927\u7279\u6027 \u62bd\u8c61\u4e0e\u63a5\u53e3 Java \u6838\u5fc3\u7c7b \u51fd\u6570\u5f0f\u7f16\u7a0b \u7c7b\u7684\u672c\u8d28 \u5c01\u88c5 \u62bd\u8c61\u7c7b Object \u7c7b \u884c\u4e3a\u53c2\u6570\u5316 \u5b9e\u4f8b\u5316\u5bf9\u8c61 \u7ee7\u627f \u63a5\u53e3 Enum \u7c7b \u51fd\u6570\u5f0f\u63a5\u53e3 \u65b9\u6cd5\u91cd\u8f7d \u591a\u6001 \u63a5\u53e3 vs \u62bd\u8c61\u7c7b Record \u7c7b Lambda \u8868\u8fbe\u5f0f","tags":["bilibili","java","oop"]},{"location":"2025/12/04/learn-like-a-kindergartener/","title":"Learn Like a Kindergartener","text":"","tags":["bilibili"]},{"location":"2025/12/04/learn-like-a-kindergartener/#learn-like-a-kindergartener","title":"Learn Like a Kindergartener","text":"<p>This is a Bilibili series designed to make advanced topics accessible to everyone. Each episode is 6-10 minutes long and covers topics such as control, estimation, and optimization. The series explores the mathematical beauty of classical theories from perspectives beyond standard textbooks. I hope you enjoy it!</p>","tags":["bilibili"]},{"location":"2025/12/04/learn-like-a-kindergartener/#episodes","title":"Episodes","text":"<ul> <li>Optimal Estimation<ol> <li>Kalman Filter in 3 Ways</li> <li>EKF, UKF, and Particle Filter</li> <li>Information Filter and Factor Graph</li> </ol> </li> <li>Optimal Control<ol> <li>LQR in 3 Ways</li> <li>The duality between LQR and KF</li> <li>Reinforcement Learning is Adaptive Optimal Control</li> </ol> </li> <li>Optimization<ol> <li>LBFGS as Quasi-Newton Method</li> <li>From Lagrange Multiplier to KKT Condition</li> <li>Augmented Lagrangian Method for Conic Optimization</li> </ol> </li> </ul>","tags":["bilibili"]},{"location":"2025/12/04/phr-conic-augmented-lagrangian-method/","title":"PHR Conic Augmented Lagrangian Method","text":"","tags":["bilibili","optimization"]},{"location":"2025/12/04/phr-conic-augmented-lagrangian-method/#phr-conic-augmented-lagrangian-method","title":"PHR Conic Augmented Lagrangian Method","text":"<p>Slide: Starting from the penalty method, we extend to the augmented Lagrangian method for improved stability. By introducing \\(s\\), symmetric cone constraints are integrated, forming a unified framework for solving constrained optimization problems iteratively. Inspired by Dr. Zhepei Wang's Lecture \"Numerical Optimization for Robotics\".</p> <p></p>","tags":["bilibili","optimization"]},{"location":"2025/12/04/phr-conic-augmented-lagrangian-method/#introduction","title":"Introduction","text":"","tags":["bilibili","optimization"]},{"location":"2025/12/04/phr-conic-augmented-lagrangian-method/#penalty-method","title":"Penalty Method","text":"<p>Consider the constrained optimization problem:</p> \\[ \\min_xf(x)\\quad\\text{s.t.}\\quad h(x)=0 \\] <p>Penalty method solves a series of unconstrained problems:</p> \\[ Q_\\rho(x) = f(x) + \\frac\\rho2||h(x)||^2 \\] <p>Challenge:</p> <ul> <li>Requires \\(\\rho\\to\\infty\\) for exact solution, causing ill-conditioned Hessian.</li> <li>Finite \\(\\rho\\) leads to constraint violation \\(h(x)\\neq 0\\).</li> </ul>","tags":["bilibili","optimization"]},{"location":"2025/12/04/phr-conic-augmented-lagrangian-method/#lagrangian-relaxation","title":"Lagrangian Relaxation","text":"<p>The Lagrangian is defined as:</p> \\[ \\mathcal{L}(x,\\lambda) = f(x) + \\lambda^\\top h(x) \\] <p>At the optimal solution \\(x^*\\), there exists \\(\\lambda^*\\) such that \\(\\nabla_x \\mathcal{L}(x^*,\\lambda^*) = 0\\).</p> <p>Uzawa's method iteratively updates \\(x\\) and \\(\\lambda\\):</p> \\[ \\begin{cases} x^{k+1} = \\arg\\min_x \\mathcal{L}(x,\\lambda^k) \\\\ \\lambda^{k+1} = \\lambda^k + \\alpha_k h(x^{k+1}) \\end{cases} \\] <p>where \\(\\alpha_k &gt; 0\\) is a step size.</p> <p>However, when \\(\\lambda\\) is fixed and one attempts to minimize \\(\\mathcal{L}(x,\\lambda)\\):</p> <ul> <li>\\(\\min_x\\mathcal{L}(x,\\lambda)\\) can be non-smooth even for smooth \\(f\\) and \\(h\\).</li> <li>\\(\\min_x \\mathcal{L}(x,\\lambda)\\) may be unbounded or have no finite solution.</li> </ul>","tags":["bilibili","optimization"]},{"location":"2025/12/04/phr-conic-augmented-lagrangian-method/#equality-constraint","title":"Equality Constraint","text":"","tags":["bilibili","optimization"]},{"location":"2025/12/04/phr-conic-augmented-lagrangian-method/#phr-augmented-lagrangian-method","title":"PHR Augmented Lagrangian Method","text":"<p>Consider the optimization problem with a penalty on the deviation from a prior \\(\\bar{\\lambda}\\):</p> \\[ \\min_x\\max_\\lambda f(x) + \\lambda^\\top h(x) - \\frac1{2\\rho}||\\lambda-\\bar{\\lambda}||^2 \\] <p>The inner problem:</p> \\[ \\nabla_\\lambda = h(x) - \\frac1\\rho(\\lambda-\\bar\\lambda)\\quad\\Rightarrow\\quad\\lambda^*(\\bar\\lambda)=\\bar\\lambda+\\rho h(x) \\] <p>The outer problem:</p> \\[ \\begin{aligned} &amp; \\min_x\\max_\\lambda f(x)+\\lambda^\\top h(x) - \\frac1{2\\rho}||\\lambda-\\bar\\lambda||^2 \\\\ =&amp; \\min_x f(x)+[\\lambda^*(\\bar\\lambda)]^\\top h(x) - \\frac1{2\\rho}||\\lambda^*(\\bar\\lambda)-\\bar\\lambda||^2 \\\\ =&amp; \\min_x f(x)+[\\bar\\lambda+\\rho h(x)]^\\top h(x) - \\frac\\rho2||h(x)||^2 \\\\ =&amp; \\min_x f(x)+\\bar\\lambda^\\top h(x) + \\frac\\rho2||h(x)||^2 \\end{aligned} \\]","tags":["bilibili","optimization"]},{"location":"2025/12/04/phr-conic-augmented-lagrangian-method/#phr-augmented-lagrangian-method-cont","title":"PHR Augmented Lagrangian Method (cont.)","text":"<p>To increase precision:</p> <ul> <li>Reduce the penalty weight \\(1/\\rho\\)</li> <li>Update the prior multiplier \\(\\bar\\lambda\\leftarrow\\lambda^*(\\bar\\lambda)\\)</li> </ul> <p>Uzawa's method for the augmented Lagrangian function is:</p> <ol> <li>\\(x\\leftarrow\\arg\\min_x f(x)+\\bar\\lambda^\\top h(x)+\\frac\\rho2||h(x)||^2\\)</li> <li>\\(\\bar\\lambda\\leftarrow\\bar\\lambda+\\rho h(x)\\)</li> </ol>","tags":["bilibili","optimization"]},{"location":"2025/12/04/phr-conic-augmented-lagrangian-method/#penalty-method-perspective","title":"Penalty Method Perspective","text":"<p>The corresponding primal problem of the augumented Lagrangian Function is obviously:</p> \\[ \\begin{aligned} \\min_x\\ &amp; f(x)+\\frac\\rho2||h(x)||^2 \\\\ \\text{s.t. } &amp; h(x) = 0 \\end{aligned} \\] <p>Advantages:</p> <ul> <li>Even without \\(\\rho \\to \\infty\\), the constraints can be exactly satisfied in the limit through multiplier updates.</li> <li>For large \\(\\rho\\), the penalty term \\(\\frac{\\rho}{2}||h(x)||^2\\) dominates, ensuring \\(\\min_x \\mathcal{L}_\\rho(x, \\lambda)\\) has a local solution.</li> <li>The augmented dual function \\(q_\\rho(\\lambda)\\) is smooth in proper conditions, with \\(\\nabla q_\\rho(\\lambda) \\approx h(x(\\lambda))\\).</li> </ul>","tags":["bilibili","optimization"]},{"location":"2025/12/04/phr-conic-augmented-lagrangian-method/#practical-phr-alm","title":"Practical PHR-ALM","text":"<p>In practical, we use its equivalent form:</p> \\[ \\mathcal{L}_\\rho(x,\\lambda) = f(x) + \\frac\\rho2\\left\\lVert h(x)+\\frac\\lambda\\rho\\right\\rVert^2 - \\underbrace{\\frac1{2\\rho}||\\lambda||^2}_{x\\text{-independent}} \\] <p>The KKT solution can be solved via:</p> \\[ \\begin{cases} x^{k+1} = \\arg\\min_x\\mathcal{L}_{\\rho^k}(x,\\lambda^k) \\\\ \\lambda^{k+1} = \\lambda^k + \\rho^k\\;h(x^{k+1}) \\\\ \\rho^{k+1} = \\min[(1+\\gamma)\\rho^k, \\rho_\\max] \\end{cases} \\] <p>where \\(\\rho^k\\) can be any nondecreasing positive sequence.</p>","tags":["bilibili","optimization"]},{"location":"2025/12/04/phr-conic-augmented-lagrangian-method/#inequality-constraint","title":"Inequality Constraint","text":"","tags":["bilibili","optimization"]},{"location":"2025/12/04/phr-conic-augmented-lagrangian-method/#slack-variables-relaxation","title":"Slack Variables Relaxation","text":"<p>Consider the optimization problem with inequality constraints:</p> \\[ \\min_xf(x)\\quad\\text{s.t.}\\quad g(x)\\le0 \\] <p>We use the equivalent formulation using slack variables:</p> \\[ \\min_{x,s} f(x)\\quad\\text{s.t.}\\quad g(x)+[s]^2=0 \\] <p>where \\([\\cdot]^2\\) means element-wise squaring.</p> <p>We can directly form Lagrangian like equality-constrained case</p> \\[ \\begin{aligned} &amp;\\min_{x,s}\\left\\{f(x)+\\frac\\rho2\\left\\lVert g(x)+[s]^2+\\frac\\lambda\\rho\\right\\rVert^2\\right\\}\\\\=&amp;\\min_xf(x)+\\min_x\\min_s\\frac\\rho2\\left\\lVert g(x)+[s]^2+\\frac\\lambda\\rho\\right\\rVert^2\\\\=&amp;\\min_x f(x)+\\frac\\rho2\\left\\lVert\\max[g(x)+\\frac\\lambda\\rho,0]\\right\\rVert^2 \\end{aligned} \\]","tags":["bilibili","optimization"]},{"location":"2025/12/04/phr-conic-augmented-lagrangian-method/#simplified-form","title":"Simplified Form","text":"<p>Summing over all components gives the final form:</p> \\[ \\mathcal{L}_\\rho(x,\\mu) = f(x) + \\frac\\rho2\\left\\lVert\\max[g(x)+\\frac\\mu\\rho,0]\\right\\rVert^2 - \\underbrace{\\frac1{2\\rho}||\\mu||^2}_{x\\text{-independent}} \\] <p>For the dual update, from the optimality condition:</p> \\[ \\begin{aligned} \\mu^{k+1} &amp;= \\mu^k + \\rho\\left(g(x^{k+1})+[s^{k+1}]^2\\right) \\\\ &amp;= \\max\\left[\\mu^k+\\rho g(x^{k+1}), 0\\right] \\end{aligned} \\]","tags":["bilibili","optimization"]},{"location":"2025/12/04/phr-conic-augmented-lagrangian-method/#summary","title":"Summary","text":"<p>PHR Augmented Lagrangian Method for General Nonconvex cases:</p> \\[ \\begin{aligned} \\min_x\\; &amp; f(x) \\\\ \\text{s.t. } &amp; h(x) = 0 \\\\ &amp; g(x) \\le 0 \\end{aligned} \\] <p>Its PHR Augmented Lagrangian is defined as</p> \\[ \\mathcal{L}_\\rho=f(x)+\\frac\\rho2\\left\\lVert h(x)+\\frac\\lambda\\rho\\right\\rVert^2+\\frac\\rho2\\left\\lVert\\max[g(x)+\\frac\\mu\\rho,0]\\right\\rVert^2-\\frac1{2\\rho}\\left\\{||\\lambda||^2+||\\mu||^2\\right\\} \\] <p>The PHR-ALM is simply repeating the primal descent and dual ascent iterations:</p> \\[ \\begin{cases} x^{k+1} = \\arg\\min_x\\mathcal{L}_{\\rho^k}(x,\\lambda^k,\\mu^k) \\\\ \\lambda^{k+1} = \\lambda^k + \\rho^k\\;h(x^{k+1}) \\\\ \\mu^{k+1} = \\max[\\mu^k+\\rho^k g(x^{k+1}), 0] \\\\ \\rho^{k+1} = \\min[(1+\\gamma)\\rho^k, \\rho_\\max] \\end{cases} \\] <p>Courtesy: Z. Wang</p>","tags":["bilibili","optimization"]},{"location":"2025/12/04/phr-conic-augmented-lagrangian-method/#symmetric-cone-constraint","title":"Symmetric Cone Constraint","text":"","tags":["bilibili","optimization"]},{"location":"2025/12/04/phr-conic-augmented-lagrangian-method/#extension-to-symmetric-cone-constraints","title":"Extension to Symmetric Cone Constraints","text":"<p>Consider the symmetric cone constrained optimization problem:</p> \\[ \\begin{aligned} \\min_x\\; &amp; f(x) \\\\ \\text{s.t. } &amp; h(x) = 0 \\\\ &amp; g(x) \\in \\mathcal{K} \\end{aligned} \\] Second Order Cone Positive Definite Cone","tags":["bilibili","optimization"]},{"location":"2025/12/04/phr-conic-augmented-lagrangian-method/#generalized-inequality-constraint","title":"Generalized Inequality Constraint","text":"<p>For the symmetric cone constraint \\(x \\in \\mathcal{K}\\), we can equivalently express it as:</p> \\[ g(x) = -x \\preceq_{\\mathcal{K}} 0 \\] <p>The standard inequality constraint \\(g(x)\\le0\\)\u00a0corresponds to the\u00a0nonnegative orthant cone:</p> \\[ \\mathcal{K}=\\mathbb{R}^n_+=\\set{x\\in\\mathbb{R}^n:x_i\\ge0,\\;i=1,\\dots,n} \\] <p>t's projection operator is exactly element-wise max function:</p> \\[ \\Pi_{\\mathbb{R}^n_+}(v) = \\max[v,0],\\quad(\\mathbb{R}^n_+)^*=\\mathbb{R}^n_+ \\]","tags":["bilibili","optimization"]},{"location":"2025/12/04/phr-conic-augmented-lagrangian-method/#slack-variables-relaxation_1","title":"Slack Variables Relaxation","text":"<p>Consider the optimization problem with inequality constraints:</p> \\[ \\min_xf(x)\\quad\\text{s.t.}\\quad g(x)\\in\\mathcal{K} \\] <p>By Euclidean Jordan algebra, the conit program is equivalent to</p> \\[ \\min_{x,s} f(x)\\quad\\text{s.t.}\\quad g(x)=s\\circ s \\] <p>We can directly form Lagrangian like equality-constrained case</p> \\[ \\begin{aligned} &amp;\\min_{x,s}\\left\\{f(x)+\\frac\\rho2\\left\\lVert g(x)-s\\circ s+\\frac\\lambda\\rho\\right\\rVert^2\\right\\}\\\\=&amp;\\min_xf(x)+\\min_x\\min_s\\frac\\rho2\\left\\lVert g(x)-s\\circ s+\\frac\\lambda\\rho\\right\\rVert^2\\\\=&amp;\\min_x f(x)+\\frac\\rho2\\left\\lVert\\Pi_{\\mathcal{K}}\\left(-g(x)-\\frac\\lambda\\rho\\right)\\right\\rVert^2 \\end{aligned} \\]","tags":["bilibili","optimization"]},{"location":"2025/12/04/phr-conic-augmented-lagrangian-method/#simplified-form_1","title":"Simplified Form","text":"<p>Let \\(\\mu=-\\lambda\\), we get the final form:</p> \\[ \\mathcal{L}_\\rho(x,\\mu) = f(x) + \\frac\\rho2\\left\\lVert\\Pi_{\\mathcal{K^*}}\\left(-g(x)+\\frac\\mu\\rho\\right)\\right\\rVert^2 - \\underbrace{\\frac1{2\\rho}||\\mu||^2}_{x\\text{-independent}} \\] <p>For the dual update, from the optimality condition:</p> \\[ \\begin{aligned} \\mu^{k+1} &amp;= \\mu^k + \\rho\\left[g(x^{k+1})-s^{k+1}\\circ s^{k+1}\\right] \\\\ &amp;= \\Pi_{\\mathcal{K^*}}[\\mu^k-\\rho\\cdot g(x^{k+1})]\\end{aligned} \\] <p>where \\(\\mathcal{K}^*\\) is the dual cone of \\(\\mathcal{K}\\).</p>","tags":["bilibili","optimization"]},{"location":"2025/12/04/phr-conic-augmented-lagrangian-method/#summary_1","title":"Summary","text":"<p>PHR Augmented Lagrangian Method for General Nonconvex cases:</p> \\[ \\begin{aligned} \\min_x\\; &amp; f(x) \\\\ \\text{s.t. } &amp; h(x) = 0 \\\\ &amp; g(x) \\in \\mathcal{K} \\end{aligned} \\] <p>Its PHR Augmented Lagrangian is defined as</p> \\[ \\mathcal{L}_\\rho=f(x)+\\frac\\rho2\\left\\lVert h(x)+\\frac\\lambda\\rho\\right\\rVert^2+\\frac\\rho2\\left\\lVert\\Pi_{\\mathcal{K}}\\left(-g(x)+\\frac\\mu\\rho\\right)\\right\\rVert^2-\\frac1{2\\rho}\\left\\{||\\lambda||^2+||\\mu||^2\\right\\} \\] <p>The PHR-ALM is simply repeating the primal descent and dual ascent iterations:</p> \\[ \\begin{cases} x^{k+1} = \\arg\\min_x\\mathcal{L}_{\\rho^k}(x,\\lambda^k,\\mu^k) \\\\ \\lambda^{k+1} = \\lambda^k + \\rho^k\\;h(x^{k+1}) \\\\ \\mu^{k+1} = \\Pi_\\mathcal{K^*}(\\mu^k-\\rho^kx^{k+1}) \\\\ \\rho^{k+1} = \\min[(1+\\gamma)\\rho^k, \\rho_\\max] \\end{cases} \\]","tags":["bilibili","optimization"]},{"location":"2025/12/04/ekf-ukf--partical-filter/","title":"EKF, UKF &amp; Partical Filter","text":"","tags":["bilibili","estimation"]},{"location":"2025/12/04/ekf-ukf--partical-filter/#ekf-ukf-partical-filter","title":"EKF, UKF &amp; Partical Filter","text":"<p>Slide: Evolving from the classic Kalman Filter, the EKF, UKF, and Particle Filter address nonlinear estimation through local linearization, deterministic sampling, and stochastic sampling, respectively, forming the cornerstone of modern state estimation.</p> <p></p>","tags":["bilibili","estimation"]},{"location":"2025/12/04/ekf-ukf--partical-filter/#recap","title":"Recap","text":"","tags":["bilibili","estimation"]},{"location":"2025/12/04/ekf-ukf--partical-filter/#kalman-filter-in-3-ways","title":"Kalman Filter in 3 Ways","text":"<p>The previous video has been well received. In response to fans' requests, we updated the Kalman filter visualization and unscented Kalman filter.</p>","tags":["bilibili","estimation"]},{"location":"2025/12/04/ekf-ukf--partical-filter/#kalman-filter-algorithm-step","title":"Kalman Filter Algorithm Step","text":"<p>1. Prediction</p> \\[ \\begin{aligned} \\hat{x}_{k|k-1} &amp;= \\mathbb{E}[x_k|z_{1:k-1}] \\\\ P_{k|k-1} &amp;= \\text{cov}[x_k-\\hat{x}_{k|k-1}|z_{1:k-1}] \\end{aligned} \\] <p>2. Update</p> \\[ \\begin{aligned} \\hat{x}_{k|k} &amp;= \\mathbb{E}[x_k|z_{1:k}] \\\\ P_{k|k} &amp;= \\text{cov}[x_k-\\hat{x}_{k|k}|z_{1:k}] \\end{aligned} \\]","tags":["bilibili","estimation"]},{"location":"2025/12/04/ekf-ukf--partical-filter/#ekf","title":"EKF","text":"","tags":["bilibili","estimation"]},{"location":"2025/12/04/ekf-ukf--partical-filter/#nonlinear-system-model","title":"Nonlinear System Model","text":"<p>The Extended Kalman Filter (EKF) linearizes nonlinear system models using first-order Taylor series expansion around the current estimate, then applies standard Kalman Filter equations.</p> \\[ \\begin{aligned} x_k &amp;= f(x_{k-1}, u_{k-1}, w_{k-1}), \\quad &amp; w_k \\sim \\mathcal{N}(0, Q_k) \\\\ z_k &amp;= h(x_k, v_k), \\quad &amp; v_k \\sim \\mathcal{N}(0, R_k) \\end{aligned} \\] <p>Define the Jacobian matrices:</p> \\[ \\begin{aligned} F_{k-1} &amp;= \\frac{\\partial f}{\\partial x} \\Big|_{\\hat{x}_{k-1|k-1}, u_{k-1}} \\\\ H_k &amp;= \\frac{\\partial h}{\\partial x} \\Big|_{\\hat{x}_{k|k-1}} \\end{aligned} \\]","tags":["bilibili","estimation"]},{"location":"2025/12/04/ekf-ukf--partical-filter/#ekf-algorithm-steps","title":"EKF Algorithm Steps","text":"<p>1. Prediction</p> \\[ \\begin{aligned} \\hat{x}_{k|k-1} &amp;= f(\\hat{x}_{k-1|k-1}, u_{k-1}, 0) \\\\ P_{k|k-1} &amp;= F_{k-1} P_{k-1|k-1} F_{k-1}^T + Q_k \\end{aligned} \\] <p>2. Update</p> \\[ \\begin{aligned} K_k &amp;= P_{k|k-1} H_k^T (H_k P_{k|k-1} H_k^T + R_k)^{-1} \\\\ \\hat{x}_{k|k} &amp;= \\hat{x}_{k|k-1} + K_k (z_k - h(\\hat{x}_{k|k-1}, 0)) \\\\ P_{k|k} &amp;= (I - K_k H_k) P_{k|k-1} \\end{aligned} \\]","tags":["bilibili","estimation"]},{"location":"2025/12/04/ekf-ukf--partical-filter/#pros-and-cons-of-the-ekf","title":"Pros and Cons of the EKF","text":"<ul> <li>Pros: Intuitive concept, relatively low computational cost, performs well in many applications.</li> <li>Cons:</li> <li>Only suitable for mildly nonlinear systems; linearization errors can be large for strong nonlinearities.</li> <li>Requires calculation of Jacobian matrices, which can be complex and error-prone.</li> <li>Can diverge (due to accumulation of linearization errors).</li> </ul>","tags":["bilibili","estimation"]},{"location":"2025/12/04/ekf-ukf--partical-filter/#ukf","title":"UKF","text":"<p>The Unscented Kalman Filter takes a different approach: Instead of approximating the nonlinear function, approximate the probability distribution.</p> <p>Unscented Transform</p> <ol> <li>Select Sigma Points: Choose \\(2n+1\\) points (\\(n\\) is the state dimension) based on the current state's mean and covariance.</li> <li>Propagate Sigma Points: Pass each Sigma point through the nonlinear function \\(f\\) or \\(h\\).</li> <li>Recalculate Statistics: Compute the new mean and covariance from the propagated points.</li> </ol> <p></p> <p>Courtesy: Zhenhui Zhang</p>","tags":["bilibili","estimation"]},{"location":"2025/12/04/ekf-ukf--partical-filter/#sigma-point-selection","title":"Sigma Point Selection","text":"<p>For an \\(n\\)-dimensional state vector \\(\\hat{x}\\) and covariance \\(P\\), the Sigma points \\(\\mathcal{X}^{(i)}\\) are chosen as follows:</p> \\[ \\begin{aligned} \\mathcal{X}^{(0)} &amp;= \\hat{x} \\\\ \\mathcal{X}^{(i)} &amp;= \\hat{x} + \\left( \\sqrt{(n+\\lambda)P} \\right)_i, \\quad &amp; i=1,\\dots,n \\\\ \\mathcal{X}^{(i)} &amp;= \\hat{x} - \\left( \\sqrt{(n+\\lambda)P} \\right)_{i-n}, \\quad &amp; i=n+1,\\dots,2n \\end{aligned} \\] <p>Where \\(\\lambda = \\alpha^2 (n+\\kappa) - n\\) is a scaling parameter, \\(\\alpha\\) controls the spread of the sigma points, and \\(\\kappa\\) is typically set to \\(3-n\\).</p> <p>Each point has an associated weight:</p> \\[ \\begin{aligned} W_m^{(0)} &amp;= \\frac{\\lambda}{n+\\lambda} \\\\ W_c^{(0)} &amp;= \\frac{\\lambda}{n+\\lambda} + (1-\\alpha^2+\\beta) \\\\ W_m^{(i)} = W_c^{(i)} &amp;= \\frac{1}{2(n+\\lambda)}, \\quad i=1,\\dots,2n \\end{aligned} \\] <p>Here, \\(\\beta\\) is used to incorporate prior knowledge of the distribution (optimal \\(\\beta=2\\) for Gaussian).</p>","tags":["bilibili","estimation"]},{"location":"2025/12/04/ekf-ukf--partical-filter/#ukf-prediction-step","title":"UKF Prediction Step","text":"<ul> <li>Generate Sigma points \\(\\mathcal{X}_{k-1}^{(i)}\\) from \\((\\hat{x}_{k-1|k-1}, P_{k-1|k-1})\\).</li> <li>Propagate Sigma points through the process model: \\(\\mathcal{X}_{k|k-1}^{*(i)} = f(\\mathcal{X}_{k-1}^{(i)}, u_{k-1})\\).</li> <li>Compute the predicted mean and covariance:   $$   \\begin{aligned}   \\hat{x}{k|k-1} &amp;= \\sum}^{2n} W_m^{(i)} \\mathcal{X{k|k-1}^{(i)} \\   P_{k|k-1} &amp;= \\sum_{i=0}^{2n} W_c^{(i)} \\left( \\mathcal{X}_{k|k-1}^{(i)} - \\hat{x}} \\right) \\left( \\mathcal{X{k|k-1}^{*(i)} - \\hat{x} \\right)^T + Q_k   \\end{aligned}   $$</li> </ul>","tags":["bilibili","estimation"]},{"location":"2025/12/04/ekf-ukf--partical-filter/#ukf-update-step","title":"UKF Update Step","text":"<ul> <li>Generate a new set of Sigma points \\(\\mathcal{X}_{k|k-1}^{(i)}\\) from \\((\\hat{x}_{k|k-1}, P_{k|k-1})\\) (or reuse the prediction points).</li> <li>Propagate points through the observation model: \\(\\mathcal{Z}_k^{(i)} = h(\\mathcal{X}_{k|k-1}^{(i)})\\).</li> <li>Calculate the predicted observation mean, its covariance, and the cross-covariance between state and observation:   $$   \\begin{aligned}   \\hat{z}{k|k-1} &amp;= \\sum}^{2n} W_m^{(i)} \\mathcal{Zk^{(i)} \\   P} &amp;= \\sum_{i=0}^{2n} W_c^{(i)} \\left( \\mathcal{Zk^{(i)} - \\hat{z}} \\right) \\left( \\mathcal{Zk^{(i)} - \\hat{z} \\right)^T + R_k \\   P_{x z} &amp;= \\sum_{i=0}^{2n} W_c^{(i)} \\left( \\mathcal{X}{k|k-1}^{(i)} - \\hat{x}} \\right) \\left( \\mathcal{Zk^{(i)} - \\hat{z} \\right)^T   \\end{aligned}   $$</li> <li>Compute the Kalman gain and update the state:   $$   \\begin{aligned}   K_k &amp;= P_{x z} P_{z z}^{-1} \\   \\hat{x}{k|k} &amp;= \\hat{x}} + K_k (z_k - \\hat{z{k|k-1}) \\   P K_k^T   \\end{aligned}   $$} &amp;= P_{k|k-1} - K_k P_{z z</li> </ul>","tags":["bilibili","estimation"]},{"location":"2025/12/04/ekf-ukf--partical-filter/#pros-and-cons-of-the-ukf","title":"Pros and Cons of the UKF","text":"<ul> <li>Pros:</li> <li>No need to calculate Jacobian matrices, making implementation simpler.</li> <li>Higher approximation accuracy for nonlinear systems (can achieve third-order Taylor series accuracy).</li> <li>Generally more stable and less prone to divergence.</li> <li>Cons:</li> <li>Slightly higher computational cost than EKF (requires propagating \\(2n+1\\) points).</li> <li>Parameters (\\(\\alpha\\), \\(\\beta\\), \\(\\kappa\\)) need to be tuned.</li> </ul>","tags":["bilibili","estimation"]},{"location":"2025/12/04/ekf-ukf--partical-filter/#particle-filter","title":"Particle Filter","text":"","tags":["bilibili","estimation"]},{"location":"2025/12/04/ekf-ukf--partical-filter/#particles-approximation","title":"Particles Approximation","text":"<p>Like UKF, Particle Filter approximates distributions through sampling. But while UKF uses deterministic sigma points, PF employs many random particles, using importance sampling and resampling to handle arbitrary nonlinear, non-Gaussian systems:</p> \\[ p(x_k | z_{1:k}) \\approx \\sum_{i=1}^N w_k^{(i)} \\delta(x_k - x_k^{(i)}) \\] <p>where \\(\\{x_k^{(i)}, w_k^{(i)}\\}_{i=1}^N\\) is the set of weighted particles, and \\(\\delta(\\cdot)\\) is the Dirac delta function.</p>","tags":["bilibili","estimation"]},{"location":"2025/12/04/ekf-ukf--partical-filter/#basic-algorithm-steps","title":"Basic Algorithm Steps","text":"<p>1. Initialization</p> <ul> <li>Sample \\(N\\) particles \\(\\{x_0^{(i)}\\}_{i=1}^N\\) from the prior distribution \\(p(x_0)\\), with initial weights \\(w_0^{(i)} = 1/N\\).</li> </ul> <p>2. Recursive Process (for each time step \\(k\\))</p> <ul> <li>Importance Sampling:   Sample new particles from a proposal distribution (often the state transition prior):</li> </ul> <p>$$   x_k^{(i)} \\sim p(x_k | x_{k-1}^{(i)})   $$</p> <ul> <li>Weight Update:   Update particle weights based on the observation likelihood:</li> </ul> <p>$$   w_k^{(i)} = w_{k-1}^{(i)} \\cdot p(z_k | x_k^{(i)})   $$</p> <ul> <li>Weight Normalization:</li> </ul> <p>$$   \\tilde{w}_k^{(i)} = \\frac{w_k<sup>{(i)}}{\\sum_{j=1}</sup>N w_k^{(j)}}   $$</p> <ul> <li>Resampling (Optional but critical):   Resample particles based on their weights, replicating high-weight particles and eliminating low-weight particles, resulting in a new set of equally weighted particles.</li> </ul> <p>3. State Estimation</p> \\[ \\hat{x}_k = \\sum_{i=1}^N \\tilde{w}_k^{(i)} x_k^{(i)} \\]","tags":["bilibili","estimation"]},{"location":"2025/12/04/ekf-ukf--partical-filter/#pros-and-cons-of-the-particle-filter","title":"Pros and Cons of the Particle Filter","text":"<ul> <li> <p>Pros:</p> </li> <li> <p>Can handle highly nonlinear and non-Gaussian systems.</p> </li> <li>Solid theoretical foundation (based on Bayesian estimation and Monte Carlo methods).</li> <li>Suitable for complex multi-modal distributions.</li> <li> <p>Relatively intuitive implementation.</p> </li> <li> <p>Cons:</p> </li> <li>High computational cost: Requires a large number of particles to ensure accuracy.</li> <li>Particle degeneracy problem: After a few iterations, only a few particles have significant weights, while most particles have negligible weights.</li> <li>Sample impoverishment problem: Resampling can lead to a loss of particle diversity.</li> </ul>","tags":["bilibili","estimation"]},{"location":"2025/12/04/ekf-ukf--partical-filter/#summary","title":"Summary","text":"","tags":["bilibili","estimation"]},{"location":"2025/12/04/ekf-ukf--partical-filter/#comparison","title":"Comparison","text":"Feature Kalman Filter (KF) EKF UKF Particle Filter Theoretical Basis Optimal Linear Estimator Local Linearization + KF Unscented Transform + KF Monte Carlo + Bayesian Estimation Computational Complexity \\(O(n^3)\\) \\(O(n^3)\\) \\(O(n^3)\\) \\(O(N)\\), where \\(N \\gg n\\) Applicable Systems Linear, Gaussian Mildly Nonlinear, ~Gaussian Moderate to Highly Nonlinear, ~Gaussian Arbitrarily Complex Nonlinear, Non-Gaussian Memory Requirements Low Low Low High (proportional to number of particles) Key Challenge Limited to Linear Systems Linearization Errors, Divergence Parameter Tuning Particle Degeneracy, Computational Cost","tags":["bilibili","estimation"]},{"location":"2025/12/04/ekf-ukf--partical-filter/#visualization-of-kalman-filter","title":"Visualization of Kalman Filter","text":"<p>See https://zhangzrjerry.github.io/html/kf.html</p>","tags":["bilibili","estimation"]},{"location":"2025/12/04/factor-graph-for-pose-estimation/","title":"Factor Graph for Pose Estimation","text":"","tags":["bilibili","slam"]},{"location":"2025/12/04/factor-graph-for-pose-estimation/#factor-graph-for-pose-estimation","title":"Factor Graph for Pose Estimation","text":"<p>Slide: Overview of factor graphs in pose estimation, emphasizing their benefits over Kalman Filters for handling complex dependencies. Covers dynamic Bayesian networks, information forms, and smoothing for efficient state estimation.</p> <p></p>","tags":["bilibili","slam"]},{"location":"2025/12/04/factor-graph-for-pose-estimation/#kalman-filter","title":"Kalman Filter","text":"","tags":["bilibili","slam"]},{"location":"2025/12/04/factor-graph-for-pose-estimation/#markov-chain","title":"Markov Chain","text":"<p>The classic Kalman Filter corresponds to a simple\u00a0Markov Chain:</p> <p></p> <p>Core Assumptions:</p> <ul> <li>Markov Property:\u00a0The current state\u00a0\\(x_k\\)\u00a0depends\u00a0only\u00a0on the immediate previous state\u00a0\\(x_{k-1}\\) and input \\(u_k\\)</li> <li>Conditional Independence:\u00a0Given the current state\u00a0\\(x_k\\), the observation\u00a0\\(y_k\\)\u200b\u00a0is independent of all other states and observations.</li> </ul>","tags":["bilibili","slam"]},{"location":"2025/12/04/factor-graph-for-pose-estimation/#scenario-1-spatio-temporal-constraints","title":"Scenario 1: Spatio-Temporal Constraints","text":"<p>A robot revisits a location, observing the same landmark\u00a0\\(l\\)\u00a0at two different times,\u00a0\\(k_1\\)\u200b\u00a0and\u00a0\\(k_2\\):</p> \\[ \\underset{\\text{obs. }l}{x_{k_1}}\\to x_{k_1+1} \\to \\cdots {x_{k_2-1}\\to \\underset{\\text{obs. }l}{x_{k_2}}} \\] <p>These two observations create a\u00a0direct constraint\u00a0between pose\u00a0\\(x_{k_1}\\)\u200b\u200b\u00a0and pose\u00a0\\(x_{k_2}\\)\u200b\u200b. In the Kalman Filter, this connection is not direct. We need a way to directly link \\(x_{k_1}\\) and \\(x_{k_2}\\).</p>","tags":["bilibili","slam"]},{"location":"2025/12/04/factor-graph-for-pose-estimation/#scenario-2-physical-constraints","title":"Scenario 2: Physical Constraints","text":"<p>When tracking multiple objects, they may be subject to physical interaction constraints</p> \\[ \\begin{aligned} \\text{Object A: }\\; x_0^A\\to\\underbrace{x_1^A \\to x_2^A}_\\text{interact with B} \\to \\cdots \\\\ \\text{Object B: }\\; x_0^B\\to\\underbrace{x_1^B \\to x_2^B}_\\text{interact with A} \\to \\cdots \\end{aligned} \\] <p>The Kalman Filter, designed for a single Markov chain, cannot natively represent this cross-object dependency.</p>","tags":["bilibili","slam"]},{"location":"2025/12/04/factor-graph-for-pose-estimation/#factor-graph","title":"Factor Graph","text":"","tags":["bilibili","slam"]},{"location":"2025/12/04/factor-graph-for-pose-estimation/#dynamic-bayesian-networks","title":"Dynamic Bayesian Networks","text":"<p>Dynamic Bayesian Networks (DBNs)\u00a0provide a more flexible framework than a simple Markov chain\u00a0for representing probabilistic dependencies\u00a0across time.</p> <p></p> <p>Extensions:</p> <ol> <li>Arbitrary Time-Span Dependencies:\u00a0States can depend on states several steps back.</li> <li>Complex Inter-Variable Constraints:\u00a0Multiple variables within a time slice can be interconnected.</li> <li>Hierarchical State Representations:\u00a0States can be decomposed into sub-states with their own dependencies.</li> </ol>","tags":["bilibili","slam"]},{"location":"2025/12/04/factor-graph-for-pose-estimation/#dynamic-bayesian-networks-cont","title":"Dynamic Bayesian Networks (cont.)","text":"<p>It is a\u00a0bipartite graph\u00a0consisting of two types of nodes:</p> <ol> <li>Variable Nodes:\u00a0Represent the unknown quantities we wish to estimate.</li> <li>Factor Nodes:\u00a0Represent a\u00a0constraint\u00a0or a\u00a0measurement\u00a0on the set of variables they are connected to.</li> </ol> <p>Goal: Find the most probable assignment of the variables that maximizes the product of all factors.</p> \\[ \\mathbf{X}^*=\\arg\\max_{\\mathbf X}\\prod_i f_i(\\mathcal{X}_i) \\] <p>Under Gaussian assumptions, this becomes a nonlinear least-squares problem:</p> \\[ \\mathbf{X}^*=\\arg\\min_{\\mathbf{X}}\\sum_i||h_i(\\mathcal{X}_i)-z_i||^2_{\\mathbf S_i} \\]","tags":["bilibili","slam"]},{"location":"2025/12/04/factor-graph-for-pose-estimation/#unified-view","title":"Unified View","text":"","tags":["bilibili","slam"]},{"location":"2025/12/04/factor-graph-for-pose-estimation/#the-dual-information-form","title":"The Dual Information Form","text":"\\[ p(\\boldsymbol\\alpha,\\boldsymbol\\beta)=\\mathcal{N}\\left(\\begin{bmatrix}\\boldsymbol\\mu_{\\boldsymbol\\alpha}\\\\\\boldsymbol\\mu_{\\boldsymbol\\beta}\\end{bmatrix}, \\begin{bmatrix}\\boldsymbol\\Sigma_{\\boldsymbol{\\alpha\\alpha}}&amp;\\boldsymbol\\Sigma_{\\boldsymbol{\\alpha\\beta}}\\\\\\boldsymbol\\Sigma_{\\boldsymbol{\\beta\\alpha}}&amp;\\boldsymbol\\Sigma_{\\boldsymbol{\\beta\\beta}}\\end{bmatrix}\\right)=\\mathcal{N}^{-1}\\left(\\begin{bmatrix}\\boldsymbol\\eta_{\\boldsymbol\\alpha}\\\\\\boldsymbol\\eta_{\\boldsymbol\\beta}\\end{bmatrix}, \\begin{bmatrix}\\boldsymbol\\Lambda_{\\boldsymbol{\\alpha\\alpha}}&amp;\\boldsymbol\\Lambda_{\\boldsymbol{\\alpha\\beta}}\\\\\\boldsymbol\\Lambda_{\\boldsymbol{\\beta\\alpha}}&amp;\\boldsymbol\\Lambda_{\\boldsymbol{\\beta\\beta}}\\end{bmatrix}\\right) \\] Operation Covariance Form Information Form Marginalization \\(p(\\boldsymbol\\alpha) = \\int p(\\boldsymbol\\alpha, \\boldsymbol\\beta) \\, {\\rm d}\\boldsymbol\\beta\\) \\(\\begin{aligned} \\boldsymbol\\mu &amp;= \\boldsymbol\\mu_{\\boldsymbol\\alpha} \\\\ \\boldsymbol\\Sigma &amp;= \\boldsymbol\\Sigma_{\\boldsymbol{\\alpha\\alpha}} \\end{aligned}\\) \\(\\begin{aligned} \\boldsymbol\\eta &amp;= \\boldsymbol\\eta_{\\boldsymbol\\alpha} - \\boldsymbol\\Lambda_{\\boldsymbol{\\alpha\\beta}} \\boldsymbol\\Lambda_{\\boldsymbol{\\beta\\beta}}^{-1} \\boldsymbol\\eta_{\\boldsymbol\\beta} \\\\ \\boldsymbol\\Lambda &amp;= \\boldsymbol\\Lambda_{\\boldsymbol{\\alpha\\alpha}} - \\boldsymbol\\Lambda_{\\boldsymbol{\\alpha\\beta}} \\boldsymbol\\Lambda_{\\boldsymbol{\\beta\\beta}}^{-1} \\boldsymbol\\Lambda_{\\boldsymbol{\\beta\\alpha}} \\end{aligned}\\) Conditioning \\(p(\\boldsymbol\\alpha \\mid \\boldsymbol\\beta) = {p(\\boldsymbol\\alpha, \\boldsymbol\\beta)}/{p(\\boldsymbol\\beta)}\\) \\(\\begin{aligned} \\boldsymbol\\mu' &amp;= \\boldsymbol\\mu_{\\boldsymbol\\alpha} + \\boldsymbol\\Sigma_{\\boldsymbol{\\alpha\\beta}} \\boldsymbol\\Sigma_{\\boldsymbol{\\beta\\beta}}^{-1} (\\boldsymbol\\beta - \\boldsymbol\\mu_{\\boldsymbol\\beta}) \\\\ \\boldsymbol\\Sigma' &amp;= \\boldsymbol\\Sigma_{\\boldsymbol{\\alpha\\alpha}} - \\boldsymbol\\Sigma_{\\boldsymbol{\\alpha\\beta}} \\boldsymbol\\Sigma_{\\boldsymbol{\\beta\\beta}}^{-1} \\boldsymbol\\Sigma_{\\boldsymbol{\\beta\\alpha}} \\end{aligned}\\) \\(\\begin{aligned} \\boldsymbol\\eta' &amp;= \\boldsymbol\\eta_{\\boldsymbol\\alpha} - \\boldsymbol\\Lambda_{\\boldsymbol{\\alpha\\beta}} \\boldsymbol\\beta \\\\ \\boldsymbol\\Lambda' &amp;= \\boldsymbol\\Lambda_{\\boldsymbol{\\alpha\\alpha}} \\end{aligned}\\)","tags":["bilibili","slam"]},{"location":"2025/12/04/factor-graph-for-pose-estimation/#the-dual-information-filter","title":"The Dual Information Filter","text":"Kalman Filter Information Filter Prediction Step \\(\\begin{aligned}\\boldsymbol\\mu_{t\\vert t-1} &amp;= \\mathbf{A}_t \\boldsymbol\\mu_{t-1} \\\\ \\boldsymbol\\Sigma_{t\\vert t-1} &amp;= \\mathbf{A}_t \\boldsymbol\\Sigma_{t-1} \\mathbf{A}_t^\\top + \\mathbf{Q}_t \\end{aligned}\\) \\(\\begin{aligned} \\boldsymbol\\Lambda_{t\\vert t-1} &amp;= (\\mathbf{A}_t \\boldsymbol\\Lambda_{t-1}^{-1} \\mathbf{A}_t^\\top + \\mathbf{Q}_t)^{-1} \\\\ \\boldsymbol\\eta_{t\\vert t-1} &amp;= \\boldsymbol\\Lambda_{t\\vert t-1} \\mathbf{A}_t \\boldsymbol\\Lambda_{t-1}^{-1} \\boldsymbol\\eta_{t-1} \\end{aligned}\\) Update Step \\(\\begin{aligned} \\mathbf{K}_t &amp;= \\boldsymbol\\Sigma_{t\\vert t-1} \\mathbf{H}_t^\\top (\\mathbf{H}_t \\boldsymbol\\Sigma_{t\\vert t-1} \\mathbf{H}_t^\\top + \\mathbf{R}_t)^{-1} \\\\ \\boldsymbol\\mu_t &amp;= \\boldsymbol\\mu_{t\\vert t-1} + \\mathbf{K}_t (\\mathbf{z}_t - \\mathbf{H}_t \\boldsymbol\\mu_{t\\vert t-1}) \\\\ \\boldsymbol\\Sigma_t &amp;= (\\mathbf{I} - \\mathbf{K}_t \\mathbf{H}_t) \\boldsymbol\\Sigma_{t\\vert t-1} \\end{aligned}\\) \\(\\begin{aligned} \\boldsymbol\\Lambda_t &amp;= \\boldsymbol\\Lambda_{t\\vert t-1} + \\mathbf{H}_t^\\top \\mathbf{R}_t^{-1} \\mathbf{H}_t \\\\ \\boldsymbol\\eta_t &amp;= \\boldsymbol\\eta_{t\\vert t-1} + \\mathbf{H}_t^\\top \\mathbf{R}_t^{-1} \\mathbf{z}_t \\end{aligned}\\)","tags":["bilibili","slam"]},{"location":"2025/12/04/factor-graph-for-pose-estimation/#factor-graph-with-information-form","title":"Factor Graph with Information Form","text":"<p>The global nonlinear optimization problem</p> \\[ \\mathbf{X}^*=\\arg\\min_{\\mathbf{X}}\\sum_i||h_i(\\mathcal{X}_i)-z_i||^2_{\\mathbf S_i} \\] <p>can be linearized at current estimation \\(\\mathbf{X}_0\\):</p> \\[ h_i(\\mathcal{X}_i)\\approx h_i(\\mathcal{X}_{i,0})+\\mathbf{J}_i\\cdot\\delta\\mathcal{X}_i \\] <p>where:</p> <ul> <li>\\(\\mathbf{J}_i = \\frac{\\partial h_i}{\\partial \\mathcal{X}_i}\\big|_{\\mathcal{X}_{i,0}}\\) is the\u00a0Jacobian matrix\u00a0of measurement function \\(h_i\\)</li> <li>\\(\\mathbf{r}_i = z_i - h_i(\\mathcal{X}_{i,0})\\) is the\u00a0residual vector</li> </ul>","tags":["bilibili","slam"]},{"location":"2025/12/04/factor-graph-for-pose-estimation/#factor-graph-with-information-form-cont","title":"Factor Graph with Information Form (cont.)","text":"Local Information Form Global Information Form \\(\\begin{aligned} \\boldsymbol{\\Lambda}_i &amp;= \\mathbf{J}_i^\\top \\mathbf{S}_i^{-1} \\mathbf{J}_i \\\\ \\boldsymbol{\\eta}_i &amp;= \\mathbf{J}_i^\\top \\mathbf{S}_i^{-1} \\mathbf{r}_i \\end{aligned}\\) \\(\\begin{aligned} \\boldsymbol{\\Lambda} &amp;= \\sum_i \\mathbf{A}_i^\\top \\boldsymbol{\\Lambda}_i \\mathbf{A}_i \\\\ \\boldsymbol{\\eta} &amp;= \\sum_i \\mathbf{A}_i^\\top \\boldsymbol{\\eta}_i \\end{aligned}\\) <p>where \\(\\mathbf{A}_i\\) is the\u00a0selection matrix\u00a0that maps local variables \\(\\mathcal{X}_i\\) to the global state vector \\(\\mathbf{X}\\).</p> <p>The optimal update is then:</p> \\[ \\mathbf{X}^*=\\mathbf{X}_0 + (\\boldsymbol\\Lambda)^{-1}\\boldsymbol\\eta \\]","tags":["bilibili","slam"]},{"location":"2025/12/04/factor-graph-for-pose-estimation/#from-filtering-to-smoothing","title":"From Filtering to Smoothing","text":"<p>Kalman Filter, Information Filter, and Factor Graph are fundamentally solving the same problem:\u00a0state estimation under Gaussian assumptions. They are probabilistically equivalent.</p> <p>Despite mathematical equivalence, FG-based smoothing dominates modern applications because:</p> <ol> <li>It naturally encodes arbitrary constraints;</li> <li>It exploits sparse structure for efficient solving;</li> <li>It's batch-based update enabling non-linear optimization;</li> <li>It corrects past states by future evidence.</li> </ol>","tags":["bilibili","slam"]},{"location":"2025/12/04/kalman-filter-in-3-ways/","title":"Kalman Filter in 3 Ways","text":"","tags":["bilibili","estimation"]},{"location":"2025/12/04/kalman-filter-in-3-ways/#kalman-filter-in-3-ways","title":"Kalman Filter in 3 Ways","text":"<p>Slide: Discover the Kalman Filter through the Geometric perspective of orthogonal projection, the Probabilistic perspective of Bayesian filtering, and the Optimization perspective of weighted least squares. Inspired by Ling Shi's 2024-25 Spring lecture \"Networked Sensing, Estimation and Control\".</p> <p></p>","tags":["bilibili","estimation"]},{"location":"2025/12/04/kalman-filter-in-3-ways/#introduction","title":"Introduction","text":"","tags":["bilibili","estimation"]},{"location":"2025/12/04/kalman-filter-in-3-ways/#system-model-and-assumptions","title":"System Model and Assumptions","text":"<p>Consider a discrete-time linear Gaussian system with initial condition \\(x_0\\) and \\(P_0\\):</p> \\[ \\begin{aligned} x_{k+1} &amp;= A_kx_k+B_ku_k+\\omega_k,&amp;\\omega_k\\sim\\mathcal N(0,Q_k) \\\\ y_k &amp;= C_kx_k + \\nu_k, &amp;\\nu_k\\sim\\mathcal N(0,R_k) \\end{aligned} \\] <p>Assumptions:</p> <ul> <li>\\((A_k,B_k)\\) is controllable and \\((A_k,C_k)\\) is observable</li> <li>\\(Q_k\\succeq0,R_k\\succeq0,P_0\\succeq0\\)</li> <li>\\(\\omega_k\\), \\(\\nu_k\\) and \\(x_0\\) are mutually uncorrelated</li> <li>The future state of the system is conditionally independent of the past states given the current state</li> </ul> <p>Goal: Find \\(\\hat x_{k|k}=\\mathbb{E}[x_k|y_{1:k}]\\) (MMSE estimator)</p>","tags":["bilibili","estimation"]},{"location":"2025/12/04/kalman-filter-in-3-ways/#geometric-perspective-orthogonal-projection","title":"Geometric Perspective: Orthogonal Projection","text":"","tags":["bilibili","estimation"]},{"location":"2025/12/04/kalman-filter-in-3-ways/#hilbert-space-of-random-variables","title":"Hilbert Space of Random Variables","text":"<p>Key Idea:</p> <ul> <li>View random variables as vectors in Hilbert space</li> <li>Inner product: \\(\\langle\\xi,\\eta\\rangle=\\mathbb{E}[\\xi\\eta]\\)</li> <li>Orthogonality: \\(\\xi\\perp\\eta\\Leftrightarrow\\mathbb{E}[\\xi\\eta]=0\\)</li> <li>Optimal estimate is orthogonal projection onto observation space</li> </ul> <p>Geometric Interpretation:</p> <p></p>","tags":["bilibili","estimation"]},{"location":"2025/12/04/kalman-filter-in-3-ways/#time-update","title":"Time Update","text":"<p>State Prediction:</p> \\[ \\begin{align*} \\hat{x}_{k|k-1} &amp;= \\mathbb{E}[x_k \\mid y_{1:k-1}] \\\\ &amp;= \\mathbb{E}[A_{k-1} x_{k-1} + B_{k-1}u_{k-1} + w_{k-1} \\mid y_{1:k-1}] \\\\ &amp;= A_{k-1} \\hat{x}_{k-1|k-1} + B_{k-1}u_{k-1} \\qquad \\text{(since $w_{k-1} \\perp y_{1:k-1}$)} \\end{align*} \\] <p>Covariance Prediction:</p> \\[ \\begin{align*} P_{k|k-1} &amp;= \\text{cov}(x_k - \\hat{x}_{k|k-1}) \\\\ &amp;= \\text{cov}[A_{k-1}(x_{k-1} - \\hat{x}_{k-1|k-1}) + w_{k-1}] \\\\ &amp;= A_{k-1}\\cdot\\text{cov}(x_{k-1} - \\hat{x}_{k-1|k-1})\\cdot A_{k-1}^\\top + 2A_{k-1}\\cdot\\text{cov}(x_k-\\hat{x}_{k|k-1},\\omega_{k-1}) + \\text{cov}(w_{k-1}) \\\\ &amp;= A_{k-1} P_{k-1|k-1} A_{k-1}^\\top + Q_{k-1} \\end{align*} \\]","tags":["bilibili","estimation"]},{"location":"2025/12/04/kalman-filter-in-3-ways/#innovation-process","title":"Innovation Process","text":"<p>Definition:</p> \\[ \\begin{align*} e_k &amp;= y_k - \\hat{y}_{k\\vert k-1} \\\\ &amp;= y_k - \\text{proj}_{\\mathcal{Y}_{k-1}}(y_k) \\\\ &amp;= y_k - \\text{proj}_{\\mathcal{Y}_{k-1}}(C_kx_k+\\nu_k) \\\\ &amp;= y_k - C_k\\cdot\\text{proj}_{\\mathcal{Y}_{k-1}}(x_k)-\\text{proj}_{\\mathcal{Y}_{k-1}}(\\nu_k) \\\\ &amp;= y_k - C_k\\hat{x}_{k\\vert k-1} \\end{align*} \\] <p>Properties:</p> <ul> <li>Zero Mean: \\(\\mathbb{E}[e_k]=0\\)</li> <li>White Sequence: \\(\\mathbb{E}[e_ke_j^\\top]=0\\) for \\(k\\ne j\\)</li> <li>Orthogonality Principle: \\(\\mathbb{E}[e_ky_j^\\top]=0\\) for \\(j&lt;k\\)</li> </ul>","tags":["bilibili","estimation"]},{"location":"2025/12/04/kalman-filter-in-3-ways/#measurement-update","title":"Measurement Update","text":"<p>State Update:</p> \\[ \\begin{align*} \\hat{x}_{k\\vert k} &amp;= \\text{proj}_{\\mathcal{Y}_{k}}(x_k) \\\\ &amp;= \\hat{x}_{k\\vert k-1}+K_ke_k \\\\ &amp;= \\hat{x}_{k\\vert k-1}+K_k(y_k - C_k\\hat{x}_{k\\vert k-1}) \\end{align*} \\] <p>Covariance Update:</p> \\[ \\begin{align*} P_{k|k} &amp;= \\text{cov}(x_k - \\hat{x}_{k|k}) \\\\ &amp;= \\text{cov}(x_k - \\hat{x}_{k|k-1} - K_k e_k) \\\\ &amp;= \\text{cov}(x_k - \\hat{x}_{k|k-1}) - 2 K_k \\text{cov}(x_k - \\hat{x}_{k|k-1}, e_k) + K_k \\text{cov}(e_k) K_k^\\top \\\\ &amp;= \\text{cov}(x_k - \\hat{x}_{k|k-1}) - 2 K_k \\text{cov}(x_k - \\hat{x}_{k|k-1}, y_k - C_k\\hat{x}_{k|k-1}) + K_k \\text{cov}(y_k - C_k\\hat{x}_{k|k-1}) K_k^\\top \\\\ &amp;= P_{k|k-1} - K_kC_kP_{k|k-1} -P_{k|k-1}C_k^\\top K^\\top+ K_k (C_k P_{k|k-1} C_k^\\top + R_{k}) K_k^\\top \\\\ \\end{align*} \\]","tags":["bilibili","estimation"]},{"location":"2025/12/04/kalman-filter-in-3-ways/#kalman-gain-derivation","title":"Kalman Gain Derivation","text":"<p>Optimal Kalman Gain: $\\(\\frac{\\partial\\text{tr}(P_{k\\vert k})}{\\partial K_k} = -2P_{k\\vert k-1}C_k^\\top + 2K_k(C_kP_{k\\vert k-1}C_k^\\top+R_{k}) = 0\\)$ $\\(K_k = P_{k\\vert k-1}C_k^\\top(C_kP_{k\\vert k-1}C_k^\\top+R_{k})^{-1}\\)$</p> <p>Covariance Derivation: $\\(P_{k|k} = P_{k\\vert k-1}-K_kC_kP_{k\\vert k-1} = (P_{k\\vert k-1}^{-1}+C_k^\\top R_{k}^{-1}C_k)^{-1}\\)$</p>","tags":["bilibili","estimation"]},{"location":"2025/12/04/kalman-filter-in-3-ways/#probabilistic-perspective-bayesian-filtering","title":"Probabilistic Perspective: Bayesian Filtering","text":"","tags":["bilibili","estimation"]},{"location":"2025/12/04/kalman-filter-in-3-ways/#bayesian-filtering-framework","title":"Bayesian Filtering Framework","text":"\\[ \\begin{aligned} &amp;p(x_{k}|y_{1:k}, u_{1:k})\\\\=\\ &amp;p(x_{k}|y_k,y_{1:k-1}, u_{1:k})\\\\=\\ &amp;\\frac{p(y_{k}|x_k,y_{1:k-1}, u_{1:k})\\cdot p(x_{k}|y_{1:k-1}, u_{1:k})}{p(y_{k}|y_{1:k-1}, u_{1:k})}\\\\ =\\ &amp;\\eta\\cdot p(y_k|x_k)\\cdot p(x_{k}|y_{1:k-1}, u_{1:k}) \\\\ =\\ &amp;\\eta\\cdot p(y_k|x_k)\\cdot\\int p(x_{k},x_{k-1}|y_{1:k-1}, u_{1:k})\\ {\\rm d}x_{k-1} \\\\ =\\ &amp;\\eta\\cdot p(y_k|x_k)\\cdot\\int p(x_{k}|x_{k-1},y_{1:k-1},u_{1:k})\\cdot p(x_{k-1}|y_{1:k-1},u_{1:k})\\ {\\rm d}x_{k-1} \\\\ =\\ &amp;\\eta\\cdot\\underbrace{p(y_k|x_k)}_\\text{observation model}\\cdot\\int\\underbrace{p(x_{k}|x_{k-1},u_{k})}_\\text{motion model}\\cdot\\underbrace{p(x_{k-1}|y_{1:k-1},u_{1:k-1})}_\\text{previous belief}\\ {\\rm d}x_{k-1} \\end{aligned} \\]","tags":["bilibili","estimation"]},{"location":"2025/12/04/kalman-filter-in-3-ways/#prediction-step-gaussian-propagation","title":"Prediction Step: Gaussian Propagation","text":"\\[ p(x_{k}|y_{1:k}, u_{1:k}) = \\eta\\cdot\\mathcal N(y_k;C_kx_k,R_k)\\cdot\\int\\mathcal{N}(x_k;A_{k-1}x_{k-1}+B_{k-1}u_{k-1},Q_{k-1})\\cdot\\mathcal{N}(x_{k-1};\\hat{x}_{k-1},P_{k-1})\\ {\\rm d}x_{k-1} \\] <p>Predicted Mean:</p> \\[ \\begin{aligned} \\hat{x}_{k|k-1} &amp;= \\mathbb{E}[A_{k-1}x_{k-1} + B_{k-1}u_{k-1} + w_{k-1}] \\\\ &amp;= A_{k-1}\\mathbb{E}[x_{k-1}] + B_{k-1}u_{k-1} + \\mathbb{E}[w_{k-1}] \\\\ &amp;= A_{k-1}\\hat{x}_{k-1} + B_{k-1}u_{k-1} \\end{aligned} \\] <p>Predicted Covariance:</p> \\[ \\begin{aligned} P_{k|k-1} &amp;= \\text{cov}[A_{k-1}x_{k-1} + B_{k-1}u_{k-1} + w_{k-1}] \\\\ &amp;= \\text{cov}[A_{k-1}x_{k-1}] + \\text{cov}[w_{k-1}] \\\\ &amp;= A_{k-1}\\text{cov}[x_{k-1}]A_{k-1}^\\top + Q_{k-1} \\\\ &amp;= A_{k-1}P_{k-1}A_{k-1}^\\top + Q_{k-1} \\end{aligned} \\]","tags":["bilibili","estimation"]},{"location":"2025/12/04/kalman-filter-in-3-ways/#update-step-gaussian-product","title":"Update Step: Gaussian Product","text":"\\[ p(x_{k}|y_{1:k}, u_{1:k}) = \\eta\\cdot\\mathcal N(y_k;C_kx_k,R_k)\\cdot\\mathcal{N}(x_k;\\hat x_{k|k-1},P_{k|k-1}) \\] <p>Gaussian Product:</p> \\[ \\mathcal{N}(x;\\mu,\\Sigma)\\propto\\mathcal{N}(x;\\mu_1,\\Sigma_1)\\cdot\\mathcal{N}(x,\\mu_2,\\Sigma_2) \\] \\[ \\begin{align*} \\Sigma^{-1} &amp;= \\Sigma_1^{-1} + \\Sigma_2^{-1} \\\\ \\mu &amp;= \\Sigma(\\Sigma_1^{-1}\\mu_1+\\Sigma_2^{-1}\\mu_2) \\end{align*} \\] <p>Posterior Result:</p> \\[ \\begin{align*} \\hat{x}_{k|k} &amp;= \\hat{x}_{k|k-1} + K_k (y_k - C_k \\hat{x}_{k|k-1}) \\\\ K_k &amp;= P_{k|k-1} C_k^\\top (C_k P_{k|k-1} C_k^\\top + R)^{-1} \\\\ P_{k|k} &amp;= (I - K_k C_k) P_{k|k-1} \\end{align*} \\]","tags":["bilibili","estimation"]},{"location":"2025/12/04/kalman-filter-in-3-ways/#optimization-perspective-map-estimation","title":"Optimization Perspective: MAP Estimation","text":"","tags":["bilibili","estimation"]},{"location":"2025/12/04/kalman-filter-in-3-ways/#maximum-a-posteriori-formulation","title":"Maximum A Posteriori Formulation","text":"<p>MAP Estimation:</p> \\[ \\begin{align*} \\hat{x}_{k|k} &amp;= \\arg\\max_{x_k} p(x_k \\mid y_{1:k}) \\\\ &amp;= \\arg\\min_{x_k} \\left[ -\\log p(x_k \\mid y_{1:k}) \\right] \\end{align*} \\] <p>Weighted Least Square:</p> \\[ \\mathcal E(x) = ||Mx-n||^2_\\Sigma=x^\\top M^\\top\\Sigma^{-1}Mx-2n^\\top\\Sigma^{-1}Mx+n^\\top\\Sigma^{-1}n \\] \\[ \\nabla\\mathcal E = 2M^\\top\\Sigma^{-1}Mx - 2M^\\top\\Sigma^{-1}n \\] \\[ \\hat x = (M^\\top\\Sigma^{-1}M)^{-1}M^\\top\\Sigma^{-1}n \\]","tags":["bilibili","estimation"]},{"location":"2025/12/04/kalman-filter-in-3-ways/#map-as-weighted-least-squares","title":"MAP as Weighted Least Squares","text":"<p>Posterior Distribution:</p> \\[ p(x_k \\mid y_{1:k}) \\propto p(y_k \\mid x_k) p(x_k \\mid y_{1:k-1}) \\] <p>Assume Gaussian Distributions:</p> \\[ \\begin{align*} p(x_k \\mid y_{1:k-1}) &amp;= \\mathcal{N}(x_k; \\hat{x}_{k|k-1}, P_{k|k-1}) \\\\ p(y_k \\mid x_k) &amp;= \\mathcal{N}(y_k; C_k x_k, R_k) \\end{align*} \\] <p>Negative Log-Posterior:</p> \\[ \\begin{align*} -\\log p(x_k \\mid y_{1:k}) &amp;\\propto \\frac{1}{2} \\|y_k - C_k x_k\\|_{R_k^{-1}}^2 + \\frac{1}{2} \\|x_k - \\hat{x}_{k|k-1}\\|_{P_{k|k-1}^{-1}}^2 \\\\ &amp;= \\frac{1}{2} \\left\\| \\begin{bmatrix} C_k \\\\ I \\end{bmatrix} x_k - \\begin{bmatrix} y_k \\\\ \\hat{x}_{k|k-1} \\end{bmatrix} \\right\\|_{\\Sigma^{-1}}^2 \\end{align*} \\] <p>where \\(\\Sigma = \\begin{bmatrix} R_k &amp; 0 \\\\ 0 &amp; P_{k|k-1} \\end{bmatrix}\\).</p>","tags":["bilibili","estimation"]},{"location":"2025/12/04/kalman-filter-in-3-ways/#map-solution","title":"MAP Solution","text":"<p>Weighted Least Squares Form:</p> \\[ M = \\begin{bmatrix} C_k \\\\ I \\end{bmatrix}, \\quad n = \\begin{bmatrix} y_k \\\\ \\hat{x}_{k|k-1} \\end{bmatrix}, \\quad \\Sigma = \\begin{bmatrix} R_k &amp; 0 \\\\ 0 &amp; P_{k|k-1} \\end{bmatrix} \\] <p>MAP Estimate:</p> \\[ \\begin{align*} \\hat{x}_{k|k} &amp;= \\left( M^\\top \\Sigma^{-1} M \\right)^{-1} M^\\top \\Sigma^{-1} n \\\\ &amp;= \\left( C_k^\\top R_k^{-1} C_k + P_{k|k-1}^{-1} \\right)^{-1} \\left( C_k^\\top R_k^{-1} y_k + P_{k|k-1}^{-1} \\hat{x}_{k|k-1} \\right) \\end{align*} \\]","tags":["bilibili","estimation"]},{"location":"2025/12/04/kalman-filter-in-3-ways/#equivalence-proof","title":"Equivalence Proof","text":"<p>Using Matrix Inversion Lemma:</p> \\[ \\begin{align*} \\hat{x}_{k|k} &amp;= \\left( C_k^\\top R_k^{-1} C_k + P_{k|k-1}^{-1} \\right)^{-1} \\left( C_k^\\top R_k^{-1} y_k + P_{k|k-1}^{-1} \\hat{x}_{k|k-1} \\right) \\\\ &amp;= \\hat{x}_{k|k-1} + P_{k|k-1} C_k^\\top (C_k P_{k|k-1} C_k^\\top + R_k)^{-1} (y_k - C_k \\hat{x}_{k|k-1}) \\end{align*} \\] <p>Proof:</p> \\[ \\begin{align*} \\left( C_k^\\top R_k^{-1} C_k + P_{k|k-1}^{-1} \\right)^{-1} C_k^\\top R_k^{-1} = P_{k|k-1} C_k^\\top (C_k P_{k|k-1} C_k^\\top + R_k)^{-1} \\end{align*} \\] <p>This shows the equivalence between the MAP solution and the Kalman update.</p>","tags":["bilibili","estimation"]},{"location":"2025/12/04/kalman-filter-in-3-ways/#conclusion","title":"Conclusion","text":"","tags":["bilibili","estimation"]},{"location":"2025/12/04/kalman-filter-in-3-ways/#theoretical-insights-and-extensions","title":"Theoretical Insights and Extensions","text":"<p>Key Insights:</p> <ul> <li>Geometric: Reveals orthogonality principle and innovation process</li> <li>Probabilistic: Shows optimality under Gaussian assumptions</li> <li>Optimization: Connects to weighted least squares and regularization</li> </ul> <p>Unified Algorithm: All approaches yield the same recursive equations:</p> \\[ \\begin{align*} \\text{time update } &amp; \\begin{cases} \\hat{x}_{k|k-1} = A_{k-1} \\hat{x}_{k-1|k-1} \\\\ P_{k|k-1} = A_{k-1} P_{k-1|k-1} A_{k-1}^\\top + Q \\end{cases} \\\\ \\text{measurement update } &amp; \\begin{cases} K_k = P_{k|k-1} C_k^\\top (C_k P_{k|k-1} C_k^\\top + R)^{-1} \\\\ \\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k (y_k - C_k\\hat{x}_{k|k-1}) \\\\ P_{k|k} = (I - K_k C_k) P_{k|k-1} \\end{cases} \\end{align*} \\] <p>Extensions:</p> <ul> <li>Nonlinear systems: EKF, UKF, particle filters</li> <li>Non-Gaussian noise: robust Kalman filters</li> </ul>","tags":["bilibili","estimation"]},{"location":"2025/12/04/kkt-conditions-and-optimization-techniques/","title":"KKT Conditions and Optimization Techniques","text":"","tags":["bilibili","optimization"]},{"location":"2025/12/04/kkt-conditions-and-optimization-techniques/#kkt-conditions-and-optimization-techniques","title":"KKT Conditions and Optimization Techniques","text":"<p>Slide: This lecture begins with an introduction to the Karush-Kuhn-Tucker (KKT) conditions, which are fundamental in constrained optimization. We will explore their geometric interpretation, derive the necessary conditions for optimality, and discuss their applications in solving optimization problems.</p> <p></p>","tags":["bilibili","optimization"]},{"location":"2025/12/04/kkt-conditions-and-optimization-techniques/#lagrangian-multiplier","title":"Lagrangian Multiplier","text":"","tags":["bilibili","optimization"]},{"location":"2025/12/04/kkt-conditions-and-optimization-techniques/#optimization-with-equality-constraints","title":"Optimization with Equality Constraints","text":"<p>Consider an optimization problem with equality constraints:</p> \\[ \\begin{aligned} \\min_{ x}f( x)\\quad\\text{subject to}\\quad h_j( x)=0 \\end{aligned} \\] <p>how to characterize the necessary conditions for the optimal solution \\(x^*\\) ?</p>","tags":["bilibili","optimization"]},{"location":"2025/12/04/kkt-conditions-and-optimization-techniques/#geometric-insight","title":"Geometric Insight","text":"<p>Consider a quadratic objective function with one linear equality constraint:</p> \\[ \\min_{x,y}x^2+y^2\\quad\\text{subject to}\\quad x+2y=5 \\] <p></p> <p>\\(\\nabla f( x^*)\\) must lie within the linear subspace spanned by \\(\\{\\nabla h_j({x}^*)\\}\\); otherwise, the function value could be further decreased by moving along a feasible direction. This means there exist multipliers \\(\\{\\nu_j^*\\}\\) such that:</p> \\[ \\nabla f({x}^*) + \\sum_j \\nu_j^* \\nabla h_j({x}^*) = 0 \\]","tags":["bilibili","optimization"]},{"location":"2025/12/04/kkt-conditions-and-optimization-techniques/#lagrangian-function-and-optimality-conditions","title":"Lagrangian Function and Optimality Conditions","text":"<p>We introduce the\u00a0Lagrangian function\u00a0as a tool to characterize optimality:</p> \\[ \\mathcal{L}(x,\\nu)=f(x)+\\sum_j\\nu_jh_j(x) \\] <p>The necessary conditions for optimality can be expressed as stationarity of the Lagrangian:</p> \\[ \\nabla\\mathcal{L}( x^*,\\nu^*) = 0 \\quad\\Longleftrightarrow\\quad\\begin{cases} \\nabla_{ x}\\mathcal{L} = \\nabla f( x^*) + \\sum_j\\nu_j^*\\nabla h_j( x^*) = 0 \\\\ \\nabla_{\\nu}\\mathcal{L} = [\\dots,h_j( x^*),\\dots]^\\top = 0 \\end{cases} \\]","tags":["bilibili","optimization"]},{"location":"2025/12/04/kkt-conditions-and-optimization-techniques/#lagrangian-relaxation","title":"Lagrangian Relaxation","text":"","tags":["bilibili","optimization"]},{"location":"2025/12/04/kkt-conditions-and-optimization-techniques/#a-min-max-interpretation","title":"A Min-Max Interpretation","text":"<p>The Lagrangian function also leads to a powerful dual interpretation:</p> \\[ \\max_\\nu\\mathcal{L}(x,\\nu) = \\begin{cases} f(x),&amp;h_j(x)=0 \\\\ \\infty,&amp;\\text{otherwise} \\end{cases} \\] <p>The original constrained problem is equivalent to the following\u00a0min-max problem:</p> \\[ \\min_xf(x),\\;\\text{s.t.}\\; h_j(x)=0 \\quad\\Longleftrightarrow\\quad \\min_x\\max_\\nu\\mathcal{L}(x,\\nu) \\]","tags":["bilibili","optimization"]},{"location":"2025/12/04/kkt-conditions-and-optimization-techniques/#the-dual-problem-and-weak-duality","title":"The Dual Problem and Weak Duality","text":"<p>Solution for \\(\\min_x\\max_\\nu\\mathcal{L}(x,\\nu)\\) may be non-continuous, but solution for \\(\\max_\\nu\\min_x\\mathcal{L}(x,\\nu)\\) is easy if \\(\\mathcal{L}(x,\\nu)\\) is tractable. We can form the dual problem by swapping the order of the min and the max:</p> \\[ \\max_\\nu d(\\nu) = \\max_\\nu\\min_x\\mathcal{L}(x,\\nu)\\le\\min_x\\max_\\nu\\mathcal{L}(x,\\nu) \\] <p>Under some conditions, equality holds, which means strong duality holds.</p>","tags":["bilibili","optimization"]},{"location":"2025/12/04/kkt-conditions-and-optimization-techniques/#uzawas-method-dual-ascent","title":"Uzawa's Method (Dual Ascent)","text":"\\[ \\nabla d(\\nu) = h[x^*(\\nu)]\\quad\\text{where}\\quad x^*(\\nu) = \\arg\\min_x\\mathcal{L}(x,\\nu) \\] <p>This leads to Uzawa's Method:</p> <ol> <li>Minimization (\\(x\\)-step):    $$    x<sup>{k+1}=\\arg\\min_x\\mathcal{L}(x,\\nu</sup>k)    $$</li> <li>Ascent (\\(\\nu\\)-step):    $$    \\nu^{k+1} = \\nu<sup>k+\\alpha</sup>kh(x^{k+1})    $$    where \\(\\alpha^k&gt;0\\) is the step size.</li> </ol>","tags":["bilibili","optimization"]},{"location":"2025/12/04/kkt-conditions-and-optimization-techniques/#kkt-conditions","title":"KKT Conditions","text":"","tags":["bilibili","optimization"]},{"location":"2025/12/04/kkt-conditions-and-optimization-techniques/#general-constrained-optimization","title":"General Constrained Optimization","text":"<p>Consider a general optimization problem with equality and inequality constraints:</p> \\[ \\begin{aligned} \\min_{ x}&amp;f( x)\\\\\\text{s.t. }&amp; g_i( x)\\le0\\\\ &amp;h_j( x)=0 \\end{aligned} \\] <p>the question does not change: how to characterize the necessary conditions for the optimal solution \\(x^*\\) ?</p>","tags":["bilibili","optimization"]},{"location":"2025/12/04/kkt-conditions-and-optimization-techniques/#geometric-insight_1","title":"Geometric Insight","text":"<p>Challenge:</p> <ol> <li>Directionality: On the boundary, \\(\\nabla g_i\\) points towards the exterior of the feasible region. To prevent \\(f\\) from pushing the point into an infeasible area, \\(\\nabla f\\) must have a component opposite to \\(\\nabla g_i\\) \\(\\Longrightarrow\\) \\(\\mu_i\\ge0\\).</li> <li>Activity Identification: The optimum may lie in the interior of the region with \\(g_i &lt; 0\\) or on the boundary with \\(g_i = 0\\) \\(\\Longrightarrow\\) \\(\\mu_ig_i=0\\).</li> </ol>","tags":["bilibili","optimization"]},{"location":"2025/12/04/kkt-conditions-and-optimization-techniques/#summary","title":"Summary","text":"<ol> <li>Stationarity: \\(0\\in\\partial_{ x}[f( x)+\\sum_i\\mu_ig_i(x)+\\sum_j\\nu_jh_j(x)]\\)</li> <li>Complementary Slackness: \\(\\mu_ig_i( x)=0\\)</li> <li>Primal Feasibility: \\(g_i(x)\\le0,h_j(x)=0\\)</li> <li>Dual Feasibility: \\(\\mu_i\\ge0\\)</li> </ol>","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/","title":"Limit-memory BFGS Method","text":"","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#limit-memory-bfgs-method","title":"Limit-memory BFGS Method","text":"<p>Slide: Starting with the classical Newton's method, we will examine its limitations and the improvements introduced by quasi-Newton methods. Finally, we will delve into the L-BFGS algorithm, which is particularly suited for large-scale problems. Inspired by Dr. Zhepei Wang's Lecture \"Numerical Optimization for Robotics\".</p> <p></p>","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#introduction","title":"Introduction","text":"","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#unconstrained-optimization","title":"Unconstrained Optimization","text":"<p>Consider a smooth and twice-differentiable unconstrained optimization problem:</p> \\[ \\min_{\\mathbf x} f(\\mathbf x) \\] <p>Descent methods provide an iterative solution:</p> \\[ \\mathbf x^{k+1} = \\mathbf x^k + \\alpha^k\\cdot \\mathbf d^k \\] <p>where \\(\\mathbf d^k\\) is the direction, and \\(\\alpha^k\\) is the step size.</p>","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#newtons-method","title":"Newton's Method","text":"<p>By second-order Taylor expansion,</p> \\[ f(\\mathbf{x})\\approx f(\\mathbf{x}^k) + \\nabla f(\\mathbf{x}^k)^\\top(\\mathbf{x}-\\mathbf{x}^k)+\\frac12(\\mathbf{x}-\\mathbf{x}^k)^\\top\\nabla^2f(\\mathbf{x}^k)(\\mathbf{x}-\\mathbf{x}^k) \\] <p>Minimizing quadratic approximation</p> \\[ \\nabla^2f(\\mathbf{x}^k)(\\mathbf{x}-\\mathbf{x}^k)+\\nabla f(\\mathbf{x}^k) = 0 \\] <p>For \\(\\nabla^2f(\\mathbf{x}^k)\\succ0\\)</p> \\[ \\mathbf{x}^{k+1} = \\mathbf{x}^k - [\\nabla^2f(\\mathbf{x}^k)]^{-1}\\nabla f(\\mathbf{x}^k) \\] <p></p>","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#damped-newton-method","title":"Damped Newton Method","text":"<p>For \\(\\nabla^2f(\\mathbf{x}^k) \\nsucc 0\\), the direction \\(\\mathbf{d}^k\\) cannot be directly solved from \\(\\nabla^2f(\\mathbf{x}^k) \\mathbf{d}^k = -\\nabla f(\\mathbf{x}^k)\\). In such cases, a PD matrix \\(\\mathbf{M}^k\\) must be constructed to approximate the Hessian.</p> <p>If the function is convex, \\(\\nabla^2f(\\mathbf{x}^k)\\) may be singular. Adding a regularization term ensures positive definiteness:</p> \\[ \\mathbf{M}^k = \\nabla^2f(\\mathbf{x}^k) + \\lambda \\mathbf{I} \\] <p>\\(\\lambda &gt; 0\\) starts small and grows until Cholesky decomposition works.</p> <p>If the function is nonconvex, \\(\\nabla^2f(\\mathbf{x}^k)\\) may be indefinite. To handle this, the Bunch-Kaufman decomposition is applied to obtain \\(\\mathbf{L}\\mathbf{D}\\mathbf{L}^\\top\\) and \\(\\tilde{\\mathbf{D}}\\).</p> \\[ \\mathbf{M}^k = \\mathbf{L}\\tilde{\\mathbf{D}}\\mathbf{L}^\\top \\] <p>Finally, direction is solved from \\(\\mathbf{M}^k\\mathbf{d}^k=-\\nabla f(\\mathbf{x}^k)\\).</p>","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#practical-newton-method","title":"Practical Newton Method","text":"<p>Moreover, we can select \\(\\alpha^k\\) by backtracking line search to ensure sufficient decrease in the objective function, satisfying the Armijo condition:</p> \\[ f(\\mathbf{x}^k + \\alpha^k \\mathbf{d}^k) \\leq f(\\mathbf{x}^k) + c_1\\cdot \\alpha^k \\nabla f(\\mathbf{x}^k)^\\top \\mathbf{d}^k \\] <p>where \\(c_1 \\in (0, 1)\\) is a small constant.</p> <p></p>","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#quasi-newton-methods","title":"Quasi-Newton Methods","text":"","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#newtons-method-limitations","title":"Newton's Method: Limitations","text":"<ul> <li>High Cost: Computing the Hessian and its inverse requires \\(\\mathcal{O}(n^3)\\) operations, impractical for large problems.</li> <li>Indefinite Hessian: In nonconvex cases, the Hessian may lead to steps toward saddle points.</li> <li>Ill-Conditioning: Poorly conditioned Hessians amplify errors and hinder convergence.</li> <li>Inaccurate Model: Local quadratic approximations may fail for complex functions, causing inefficiency or divergence.</li> </ul>","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#quasi-newton-approximation","title":"Quasi-Newton Approximation","text":"<p>Newton approximation:</p> \\[ f(\\mathbf{x}) \\approx f(\\mathbf{x}^k)+ (\\mathbf{x}-\\mathbf{x}^k)^\\top \\mathbf{g}^k+\\frac12(\\mathbf{x}-\\mathbf{x}^k)^\\top \\mathbf{H}^k(\\mathbf{x}-\\mathbf{x}^k) \\] \\[ \\mathbf{H}^k\\mathbf{d}^k=-\\mathbf{g}^k \\] <p>Quasi-Newton approximation:</p> \\[ f(\\mathbf{x}) \\approx f(\\mathbf{x}^k) + (\\mathbf{x}-\\mathbf{x}^k)^\\top \\mathbf{g}^k + \\frac{1}{2}(\\mathbf{x}-\\mathbf{x}^k)^\\top \\mathbf{B}^k (\\mathbf{x}-\\mathbf{x}^k) \\] \\[ \\mathbf{B}^k\\mathbf{d}^k=-\\mathbf{g}^k \\] <p>The matrix \\(\\mathbf{B}^k\\) should:</p> <ul> <li>Avoid full second-order derivatives.</li> <li>Have a closed-form solution for linear equations.</li> <li>Retain first-order curvature information.</li> <li>Preserve the descent direction.</li> </ul>","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#bfgs-method","title":"BFGS Method","text":"","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#descent-direction","title":"Descent Direction","text":"<p>Search direction \\(d^k\\) should make acute angle with negative gradient.</p> \\[ (\\mathbf{g}^k)^\\top\\mathbf{d}^k = -(\\mathbf{g}^k)^\\top(\\mathbf{B}^k)^{-1}\\mathbf{g}^k &lt; 0 \\] <p>\\(\\mathbf{B}^k\\) must be PD since for all non-negative \\(\\mathbf{g}^k\\), \\((\\mathbf{g}^k)^\\top(\\mathbf{B}^k)^{-1}\\mathbf{g}^k&gt;0\\).</p> <p></p>","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#curvature-information","title":"Curvature Information","text":"<p>At the point \\(\\mathbf{x}^{k+1}\\), the gradient is \\(\\mathbf{g}^{k+1}\\). We want \\(\\mathbf{B}^{k+1}\\) to satisfy:</p> \\[ \\mathbf{B}^{k+1}(\\mathbf{x}^{k+1}-\\mathbf{x}^k)\\approx\\mathbf{g}^{k+1}-\\mathbf{g}^k \\] \\[ \\mathbf{B}^{k+1}\\mathbf{s}^{k}=\\mathbf{y}^{k} \\] <p></p>","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#the-optimal-mathbfbk1","title":"The Optimal \\(\\mathbf{B}^{k+1}\\)?","text":"<p>Infinitely many \\(\\mathbf{B}^{k+1}\\) satisfy the secant condition, how to choose the best one. We define the following weighted least square problem</p> \\[ \\min_{\\mathbf{B}}\\lVert\\mathbf{B}-\\mathbf{B}^k\\rVert_\\mathbf{W}^2\\quad\\text{subject to}\\quad\\mathbf{B}=\\mathbf{B}^\\top,\\mathbf{B}\\mathbf{s}^{k}=\\mathbf{y}^{k} \\] <p>In BFGS, weight matrix is select to be:</p> \\[ \\mathbf{W}=\\int_0^1\\nabla^2f[(1-\\tau)\\mathbf{x}^k+\\tau\\mathbf{x}^{k+1}]{\\rm d}\\tau \\]","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#closed-form-solution-for-mathbfbk1","title":"Closed-form Solution for \\(\\mathbf{B}^{k+1}\\)","text":"<p>To derive the optimal \\(\\mathbf{B}^{k+1}\\), we construct the Lagrangian function:</p> \\[ \\mathcal{L}(\\mathbf{B}, \\boldsymbol{\\Lambda}) = \\frac{1}{2} \\lVert \\mathbf{B} - \\mathbf{B}^k \\rVert_\\mathbf{W}^2 + \\text{tr}\\left[\\boldsymbol{\\Lambda}^\\top \\left(\\mathbf{B}\\mathbf{s}^{k} - \\mathbf{y}^{k}\\right)\\right] \\] <p>Taking the derivative of the Lagrangian with respect to \\(\\mathbf{B}\\) and setting it to zero gives:</p> \\[ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{B}} = \\mathbf{W}(\\mathbf{B} - \\mathbf{B}^k)\\mathbf{W} + \\boldsymbol{\\Lambda}(\\mathbf{s}^{k})^\\top = 0 \\] <p>Rearranging the terms, we express \\(\\mathbf{B}\\) as:</p> \\[ \\mathbf{B} = \\mathbf{B}^k - \\mathbf{W}^{-1} \\boldsymbol{\\Lambda} (\\mathbf{s}^{k})^\\top\\mathbf{W}^{-1} \\] <p>Substituting this expression into the secant condition \\(\\mathbf{B}\\mathbf{s}^{k} = \\mathbf{y}^{k}\\), we obtain:</p> \\[ \\left(\\mathbf{B}^k - \\mathbf{W}^{-1} \\boldsymbol{\\Lambda}(\\mathbf{s}^{k})^\\top\\right)\\mathbf{s}^{k} = \\mathbf{y}^{k} \\] <p>Solving for \\(\\boldsymbol{\\Lambda}\\), we find:</p> \\[ \\boldsymbol{\\Lambda} = \\mathbf{W} \\left(\\mathbf{y}^{k} - \\mathbf{B}^k \\mathbf{s}^{k}\\right) \\left((\\mathbf{s}^{k})^\\top \\mathbf{W}^{-1} \\mathbf{s}^{k}\\right)^{-1} \\] <p>Finally, substituting \\(\\boldsymbol{\\Lambda}\\) back, the closed-form solution for \\(\\mathbf{B}^{k+1}\\) is:</p> \\[ \\mathbf{B}^{k+1} = \\mathbf{B}^k + \\frac{\\mathbf{y}^{k}(\\mathbf{y}^{k})^\\top}{(\\mathbf{s}^{k})^\\top \\mathbf{y}^{k}} - \\frac{\\mathbf{B}^k \\mathbf{s}^{k} (\\mathbf{B}^k \\mathbf{s}^{k})^\\top}{(\\mathbf{s}^{k})^\\top \\mathbf{B}^k \\mathbf{s}^{k}} \\]","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#broyden-fletcher-goldfarb-shanno-bfgs","title":"Broyden-Fletcher-Goldfarb-Shanno (BFGS)","text":"<p>Given the initial value \\(\\mathbf{B}^0=\\mathbf{I}\\), the updates are performed iteratively:</p> \\[ \\mathbf{B}^{k+1} = \\mathbf{B}^k + \\frac{\\mathbf{y}^{k}(\\mathbf{y}^{k})^\\top}{(\\mathbf{s}^{k})^\\top \\mathbf{y}^{k}} - \\frac{\\mathbf{B}^k \\mathbf{s}^{k} (\\mathbf{B}^k \\mathbf{s}^{k})^\\top}{(\\mathbf{s}^{k})^\\top \\mathbf{B}^k \\mathbf{s}^{k}} \\] <p>with \\(\\mathbf{s}^{k} = \\mathbf{x}^{k+1} - \\mathbf{x}^k\\) and \\(\\mathbf{y}^{k} = \\mathbf{g}^{k+1} - \\mathbf{g}^k\\).</p> <p>For computational efficiency, we often work with the inverse of \\(\\mathbf{B}^k\\) directly. The iterative update for \\((\\mathbf{B}^k)^{-1}\\) is given by:</p> \\[ \\mathbf{C}^{k+1} = \\left( I - \\frac{\\mathbf{s}^{k}(\\mathbf{y}^{k})^\\top}{(\\mathbf{s}^{k})^\\top \\mathbf{y}^{k}} \\right)\\mathbf{C}^k\\left( I - \\frac{\\mathbf{y}^{k}(\\mathbf{s}^{k})^\\top}{(\\mathbf{s}^{k})^\\top \\mathbf{y}^{k}} \\right) + \\frac{\\mathbf{s}^{k}(\\mathbf{s}^{k})^\\top}{(\\mathbf{s}^{k})^\\top \\mathbf{y}^{k}} \\]","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#guaranteeing-pd-of-mathbfbk1","title":"Guaranteeing PD of \\(\\mathbf{B}^{k+1}\\)","text":"<p>To ensure that \\(\\mathbf{B}^{k+1}\\) remains positive definite (PD), the following curvature condition must hold:</p> \\[ (\\mathbf{y}^{k})^\\top\\mathbf{s}^{k} &gt; 0 \\] <p>For any nonzero vector \\(\\mathbf{z}\\), using the Cauchy-Schwarz inequality:</p> \\[ \\begin{aligned} \\mathbf{z}^\\top\\mathbf{B}^{k+1}\\mathbf{z} &amp;= \\mathbf{z}^\\top\\mathbf{B}^k\\mathbf{z} + \\frac{(\\mathbf{z}^\\top\\mathbf{y}^{k})^2}{(\\mathbf{y}^{k})^\\top\\mathbf{s}^{k}} - \\frac{(\\mathbf{z}^\\top\\mathbf{B}^k\\mathbf{s}^{k})^2}{(\\mathbf{s}^{k})^\\top\\mathbf{B}^k\\mathbf{s}^{k}} \\\\ &amp;\\geq \\frac{\\mathbf{z}^\\top\\mathbf{B}^k\\mathbf{z}(\\mathbf{s}^{k})^\\top\\mathbf{B}^k\\mathbf{s}^{k} - (\\mathbf{z}^\\top\\mathbf{B}^k\\mathbf{s}^{k})^2}{(\\mathbf{s}^{k})^\\top\\mathbf{B}^k\\mathbf{s}^{k}} \\geq 0 \\end{aligned} \\] <p>Equalities hold only when \\(\\mathbf{z}^\\top\\mathbf{y}^{k} = 0\\) and \\(\\mathbf{z} \\parallel \\mathbf{s}^{k}\\). Given that \\((\\mathbf{y}^{k})^\\top\\mathbf{s}^{k} &gt; 0\\), these conditions cannot hold simultaneously. Therefore, if \\(\\mathbf{B}^k \\succ 0\\), it follows that \\(\\mathbf{B}^{k+1} \\succ 0\\).</p>","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#guranteeing-mathbfyktopmathbfsk-0","title":"Guranteeing \\((\\mathbf{y}^{k})^\\top\\mathbf{s}^{k} &gt; 0\\)","text":"<p>Armijo Condition (AC) cannot gurantee curvature, we need curvature condition (CC):</p> \\[ (\\mathbf{d}^k)^\\top\\nabla f(\\mathbf{x}^k+\\alpha^k\\mathbf{d}^k)\\ge c_2\\cdot(\\mathbf{d}^k)^\\top\\nabla f(\\mathbf{x}^k) \\] <p>Typically, \\(c_1=10^{-4}, c_2=0.9\\).</p> <p></p>","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#l-bfgs-method","title":"L-BFGS Method","text":"","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#lewis-overton-line-search","title":"Lewis-Overton Line Search","text":"<p>The Lewis-Overton line search is a sophisticated backtracking line search designed specifically for quasi-Newton methods:</p> <ol> <li>Given search direction \\(\\mathbf{d}^k\\), current point \\(\\mathbf{x}^k\\) and gradient \\(\\mathbf{g}^k\\)</li> <li>Initialize trial step \\(\\alpha:=1\\), \\(\\alpha_l:=0\\), \\(\\alpha_r:=+\\infty\\)</li> <li>Repeat</li> <li>Update bounds:<ul> <li>If \\(\\text{AC}(\\alpha)\\) fails, set \\(\\alpha_r := \\alpha\\)</li> <li>Else if \\(\\text{CC}(\\alpha)\\) fails, set \\(\\alpha_l := \\alpha\\)</li> <li>Otherwise, return \\(\\alpha\\)</li> </ul> </li> <li>Update \\(\\alpha\\):<ul> <li>If \\(\\alpha_r &lt; +\\infty\\), set \\(\\alpha := \\text{CubicInterpolate}(\\alpha_l, \\alpha_r)\\) or \\(\\alpha := (\\alpha_l + \\alpha_r) / 2\\)</li> <li>Otherwise, set \\(\\alpha := \\text{CubicExtrapolate}(\\alpha_l, \\alpha_r)\\) or \\(\\alpha := 2\\alpha_l\\)</li> </ul> </li> <li>Ensure \\(\\alpha \\in [\\alpha_{\\min}, \\alpha_{\\max}]\\)</li> </ol>","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#cautious-update","title":"Cautious Update","text":"<p>Sometimes, when line search is inexact or the function is poorly conditioned, \\((\\mathbf{y}^{k})^\\top \\mathbf{s}^{k} &gt; 0\\) cannot gurantee. To ensure numerical stability and maintain the PD Hessian approximation, L-BFGS employs a cautious update strategy:</p> <p>Skip update condition: If the curvature condition \\((\\mathbf{y}^k)^\\top\\mathbf{s}^k &gt; \\epsilon |\\mathbf{s}^k|^2\\) is not satisfied, where \\(\\epsilon\\) is a small positive constant (e.g., \\(10^{-6}\\)), skip the update for this iteration: \\(\\mathbf{B}^{k+1} = \\mathbf{B}^k\\).</p> <p>Powell's Damping: If the curvature condition \\((\\mathbf{y}^k)^\\top \\mathbf{s}^k \\geq \\eta (\\mathbf{s}^k)^\\top \\mathbf{B}^k \\mathbf{s}^k\\) is not satisfied, where \\(\\eta\\) is a small positive constant (e.g., \\(0.2\\) or \\(0.25\\)).</p> \\[ \\quad\\tilde{\\mathbf{y}}^k=\\theta\\mathbf{y}^k+(1-\\theta)\\mathbf{B}^k\\mathbf{s}^k,\\qquad\\theta=\\frac{(1-\\eta)\\cdot(\\mathbf{s}^k)^\\top \\mathbf{B}^k \\mathbf{s}^k}{(\\mathbf{s}^k)^\\top \\mathbf{B}^k \\mathbf{s}^k - (\\mathbf{y}^k)^\\top \\mathbf{s}^k} \\] <p>Cautious updates guaranteed to have its iterates converge to a critical point if the function has bounded sublevel sets and a Lipschitz continuous gradient.</p>","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#two-loop-recursion","title":"Two-Loop Recursion","text":"<p>L-BFGS uses a two-loop recursion to compute the search direction without explicitly forming the Hessian approximation. The algorithm maintains a history of the most recent \\(m\\) pairs \\({(\\mathbf{s}^i, \\mathbf{y}^i)}_{i=k-m}^{k-1}\\), where typically \\(m\\) is between 5 and 20.</p> <ol> <li>Initialize an empty array \\(\\mathcal{A}\\) of length \\(m\\), \\(\\mathbf{d}=\\mathbf{g}^k\\)</li> <li>For \\(i=k-1,k-2,\\dots,k-m\\):</li> <li>\\(\\mathcal{A}^{i+m-k} := \\langle\\mathbf{s}^i,\\mathbf{d}\\rangle/\\langle \\mathbf{s}^i,\\mathbf{y}^i\\rangle\\)</li> <li>\\(\\mathbf{d}:=\\mathbf{d}-\\mathbf{L}^{i+m-k}\\mathbf{y}^i\\)</li> <li>\\(\\mathbf{d}:=\\mathbf{d}\\cdot\\langle \\mathbf{s}^{k-1},\\mathbf{y}^{k-1}\\rangle/\\langle \\mathbf{y}^{k-1},\\mathbf{y}^{k-1}\\rangle\\)</li> <li>For \\(i=k-m,k-m+1,\\dots,k-1\\):</li> <li>\\(a:=\\langle \\mathbf{y}^i,\\mathbf{d}\\rangle/\\langle \\mathbf{s}^i,\\mathbf{y}^i\\rangle\\)</li> <li>\\(\\mathbf{d}:=\\mathbf{d}+\\mathbf{s}^i(\\mathcal{A}^{i+m-k}-a)\\)</li> <li>Return \\(d\\)</li> </ol> <p>This approach reduces the storage requirement from \\(\\mathcal{O}(n^2)\\) to \\(\\mathcal{O}(mn)\\) and the computational cost per iteration from \\(\\mathcal{O}(n^2)\\) to \\(\\mathcal{O}(mn)\\).</p>","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#algorithm-summary","title":"Algorithm Summary","text":"<p>The complete L-BFGS algorithm with cautious update and Lewis-Overton line search:</p> <ol> <li>Initialize \\(\\mathbf{x}^0,\\mathbf{g}^0:=\\nabla f(\\mathbf{x}^0)\\), choose \\(m\\)</li> <li>For \\(k = 0, 1, 2, \\ldots\\) until convergence:</li> <li>Compute search direction: \\(\\mathbf{d}^k\\) using L-BFGS two-loop recursion</li> <li>Find step size \\(\\alpha^k\\) using Lewis-Overton line search</li> <li>Update: \\(\\mathbf{x}^{k+1} = \\mathbf{x}^k + \\alpha^k \\mathbf{d}^k\\)</li> <li>Compute \\(\\mathbf{s}^k = \\mathbf{x}^{k+1} - \\mathbf{x}^k\\), \\(\\mathbf{y}^k = \\nabla f(\\mathbf{x}^{k+1}) - \\nabla f(\\mathbf{x}^k)\\)</li> <li>Apply cautious update to \\((\\{\\mathbf{s}^k\\}, \\{\\mathbf{y}^k\\})\\)</li> </ol>","tags":["bilibili","optimization"]},{"location":"2025/12/04/limit-memory-bfgs-method/#open-source-implementation","title":"Open Source Implementation","text":"<ul> <li>https://github.com/chokkan/liblbfgs</li> <li>https://github.com/hjmshi/PyTorch-LBFGS</li> <li>https://github.com/ZJU-FAST-Lab/LBFGS-Lite</li> <li>https://github.com/yixuan/LBFGSpp</li> </ul>","tags":["bilibili","optimization"]},{"location":"2025/12/04/the-duality-and-the-failure-of-lqg-control/","title":"The Duality and the Failure of LQG Control","text":"","tags":["bilibili","control"]},{"location":"2025/12/04/the-duality-and-the-failure-of-lqg-control/#the-duality-and-the-failure-of-lqg-control","title":"The Duality and the Failure of LQG Control","text":"<p>Slide: Explore the duality between state observers and feedback controllers, focusing on KF and LQR. Understand why combining the \"optimal observer\" with the \"optimal controller\" might fail. Inspired by Dominikus Noll's page \"A generalization of the Linear Quadratic Gaussian Loop Transfer Recovery procedure (LQG/LTR)\".</p> <p></p>","tags":["bilibili","control"]},{"location":"2025/12/04/the-duality-and-the-failure-of-lqg-control/#introduction","title":"Introduction","text":"","tags":["bilibili","control"]},{"location":"2025/12/04/the-duality-and-the-failure-of-lqg-control/#system-model","title":"System Model","text":"<p>Consider a \\(n\\)-th order linear time-invariant (LTI) discrete-time dynamic system with \\(m\\)-dimensional input and \\(p\\)-dimensional output:</p> \\[ \\begin{aligned} x_{k+1} &amp;= A x_{k} + B u_{k} + \\omega_{k}, &amp;\\omega_{k}\\sim\\mathcal{N}(0, W_{k}) \\\\ y_{k} &amp;= C x_{k} + \\nu_{k} ,&amp;\\nu_{k}\\sim\\mathcal{N}(0, V_{k}) \\end{aligned} \\] <ul> <li>\\(x_k\\in\\mathbb{R}^n\\): state vector at time step \\(k\\)</li> <li>\\(u_k\\in\\mathbb{R}^m\\): control input vector at time step \\(k\\)</li> <li>\\(y_k\\in\\mathbb{R}^p\\): measurement vector at time step \\(k\\)</li> <li>\\(A\\in\\mathbb{R}^{n\\times n}\\): state transition matrix</li> <li>\\(B\\in\\mathbb{R}^{n\\times m}\\): control input matrix</li> <li>\\(C\\in\\mathbb{R}^{p\\times n}\\): observation matrix</li> </ul>","tags":["bilibili","control"]},{"location":"2025/12/04/the-duality-and-the-failure-of-lqg-control/#controllability","title":"Controllability","text":"<p>A LTI system is said to be controllable if,</p> \\[ \\forall x_0,x^*,\\exists k&gt;0,\\mathbf u_k=[u_{k-1},\\cdots,u_1,u_0],\\quad\\text{such that}\\quad x_k=x^*. \\] <p>This is equivalent to \\(\\text{rank}(M_c)=n\\), where \\(M_c = [B, AB, A^2B, \\ldots, A^{n-1}B]\\in\\mathbb{R}^{n\\times nm}\\) is the controllability matrix.</p> \\[ \\begin{aligned} x_n &amp;= Ax_{n-1} + Bu_{n-1} \\\\ &amp;= A(Ax_{n-2} + Bu_{n-2}) + Bu_{n-1} \\\\ &amp;= A^2x_{n-2} + ABu_{n-2} + Bu_{n-1} \\\\ &amp;= A^nx_0 + A^{n-1}Bu_0 + \\cdots + ABu_{n-2} + Bu_{n-1} \\\\ &amp;= A^nx_0 + M_c\\mathbf u_n \\end{aligned} \\] \\[ \\mathbf u_n = M_c^\\top(M_cM_c^\\top)^{-1}(x^*-A^nx_0) \\]","tags":["bilibili","control"]},{"location":"2025/12/04/the-duality-and-the-failure-of-lqg-control/#observability","title":"Observability","text":"<p>A LTI system is said to be observable if,</p> \\[ \\forall x_0\\in\\mathbb{R}^n\\exists k&gt;0, \\mathbf{y_k}=[y_0,y_1,\\ldots,y_{k-1}]^\\top \\Rightarrow x_0. \\] <p>This is equivalent to \\(\\text{rank}(M_o)=n\\), where \\(M_o = [C^\\top, (CA)^\\top, (CA^2)^\\top, \\ldots, (CA^{n-1})^\\top]^\\top\\in\\mathbb{R}^{np\\times n}\\) is the observability matrix.</p> \\[ \\begin{aligned} y_0 &amp;= Cx_0 \\\\ y_1 &amp;= Cx_1 = CAx_0 \\\\ &amp;\\ \\ \\vdots \\\\ y_{n-1} &amp;= CA^{n-1}x_0 \\end{aligned}\\quad\\Rightarrow\\quad \\mathbf y_n=\\begin{bmatrix}y_0\\\\y_1\\\\\\vdots\\\\y_{n-1}\\end{bmatrix}=\\begin{bmatrix}C\\\\CA\\\\\\vdots\\\\CA^{n-1}\\end{bmatrix}x_0=M_ox_0 \\] \\[ x_0 = (M_o^\\top M_o)^{-1}M_o^\\top \\mathbf y_n \\]","tags":["bilibili","control"]},{"location":"2025/12/04/the-duality-and-the-failure-of-lqg-control/#the-optimality","title":"The Optimality","text":"","tags":["bilibili","control"]},{"location":"2025/12/04/the-duality-and-the-failure-of-lqg-control/#optimal-estimator-kalman-filter","title":"Optimal Estimator: Kalman Filter","text":"<p>Goal:</p> \\[ \\min_{\\hat{x}_{k|k}} \\mathbb{E}[(x_k - \\hat{x}_{k|k})(x_k - \\hat{x}_{k|k})^\\top\\mid y_1, \\ldots, y_k] \\] <p>Solution:</p> \\[ \\begin{aligned} \\hat x_{k\\vert k-1}&amp;=A\\hat x_{k-1\\vert k-1}+Bu_{k-1} \\\\ \\hat P_{k\\vert k-1} &amp;= A\\hat P_{k-1\\vert k-1}A^\\top+W_{k-1} \\\\ K_k &amp;= \\hat P_{k\\vert k-1}C^\\top(C\\hat P_{k\\vert k-1}C^\\top+V_k)^{-1} \\\\ \\hat x_{k\\vert k} &amp;= \\hat x_{k\\vert k-1} + K_k(y_k-C\\hat x_{k\\vert k-1}) \\\\ \\hat P_{k\\vert k} &amp;= \\hat P_{k\\vert k-1}-K_kC\\hat P_{k\\vert k-1} = (\\hat P_{k\\vert k-1}^{-1}+C^\\top V_k^{-1}C)^{-1}\\\\ \\end{aligned} \\]","tags":["bilibili","control"]},{"location":"2025/12/04/the-duality-and-the-failure-of-lqg-control/#optimal-regulator-lqr","title":"Optimal Regulator: LQR","text":"<p>Goal:</p> \\[ \\min_{\\{u_k\\}} \\mathbb{E}\\left[x_N^\\top Q_N x_N + \\sum_{k=0}^{N-1}(x_k^\\top Q_k x_k + u_k^\\top R_k u_k)\\right] \\] <p>Solution:</p> \\[ \\begin{aligned} S_N &amp;= Q_N \\\\ L_k &amp;= (R_k+B_k^\\top S_{k+1}B_k)^{-1}B_k^\\top S_{k+1}A_k \\\\ S_k &amp;= Q_k + A_k^\\top S_{k+1}(A_k-B_kL_k) \\\\ u_k &amp;= -L_k x_k \\end{aligned} \\]","tags":["bilibili","control"]},{"location":"2025/12/04/the-duality-and-the-failure-of-lqg-control/#linear-quadratic-gaussian-lqg","title":"Linear Quadratic Gaussian (LQG)","text":"<p>The separation principle states that the design of the optimal controller and the optimal observer can be separated. The optimal control law is given by:</p> \\[ u_k = -L_k \\hat x_{k|k} \\] <p>where \\(\\hat x_{k|k}\\) is the state estimate provided by the Kalman filter.</p> <p></p>","tags":["bilibili","control"]},{"location":"2025/12/04/the-duality-and-the-failure-of-lqg-control/#the-duality","title":"The Duality","text":"","tags":["bilibili","control"]},{"location":"2025/12/04/the-duality-and-the-failure-of-lqg-control/#the-duality-in-control-theory","title":"The Duality in Control Theory","text":"<p>Controllability vs Observability For the original system \\(\\Sigma=(A,B,C)\\), the dual system is defined as \\(\\Sigma^*=(A^\\top,C^\\top,B^\\top)\\).</p> <ul> <li>\\(\\Sigma\\) is controllable \\(\\Leftrightarrow\\) \\(\\Sigma^*\\) is observable</li> <li>\\(\\Sigma\\) is observable \\(\\Leftrightarrow\\) \\(\\Sigma^*\\) is controllable</li> </ul> <p>Controller vs Observer</p> <ul> <li>Feedback controller \\(u_k = -L_k x_k\\) \"suppresses\" the state deviation \\(x_k\\) through inputs</li> <li>State observer \\(\\hat x_{k|k} = \\hat x_{k|k-1} + K_k(y_k - C\\hat x_{k|k-1})\\) \"corrects\" the state estimate \\(\\hat x_{k|k}\\) through measurements</li> <li>The design of \\(L_k\\) and \\(K_k\\) are dual problems</li> </ul>","tags":["bilibili","control"]},{"location":"2025/12/04/the-duality-and-the-failure-of-lqg-control/#the-duality-in-lqr-and-kalman-filter-optimization","title":"The Duality in LQR and Kalman Filter (Optimization)","text":"<p>Optimization formulation of LQR:</p> \\[ \\min_{x_{1:N},u_{1:N-1}}x_N^\\top Q_N x_N + \\sum_{k=0}^{N-1}\\bigg[x_k^\\top Q_k x_k + u_k^\\top R_k u_k\\bigg] \\] <p>Optimization formulation of Kalman Filter:</p> \\[ \\min_{x_{1:N},\\omega_{1:N-1}} (x_0-\\hat x_{0|0})^\\top P_0^{-1}(x_0-\\hat x_{0|0}) + \\sum_{k=0}^{N-1}\\bigg[(y_k - Cx_k)^\\top V_k^{-1}(y_k - Cx_k) + \\omega_k^\\top W_k^{-1}\\omega_k\\bigg] \\] <p>subject to \\(x_{k+1} = A x_k + Bu_k + \\omega_k\\).</p> <p>Duality:</p> \\[ A \\leftrightarrow A^\\top, \\quad B \\leftrightarrow C^\\top, \\quad Q \\leftrightarrow W, \\quad R \\leftrightarrow V \\]","tags":["bilibili","control"]},{"location":"2025/12/04/the-duality-and-the-failure-of-lqg-control/#the-duality-in-lqr-and-kalman-filter-riccati","title":"The Duality in LQR and Kalman Filter (Riccati)","text":"<p>Riccati Equation in LQR:</p> \\[ \\begin{cases} L_k = (R_k+B_k^\\top S_{k+1}B_k)^{-1}B_k^\\top S_{k+1}A_k \\\\ S_k = Q_k + A_k^\\top S_{k+1}(A_k-B_kL_k) \\end{cases} \\] \\[ S=A^\\top SA+Q-A^\\top SB(B^\\top SB+R)^{-1}B^\\top SA \\] <p>Riccati Equation in Kalman Filter:</p> \\[ \\begin{cases} \\hat P_{k\\vert k-1} = A\\hat P_{k-1\\vert k-1}A^\\top+W_{k-1} \\\\ K_k = \\hat P_{k\\vert k-1}C^\\top(C\\hat P_{k\\vert k-1}C^\\top+V_k)^{-1} \\\\ \\hat P_{k\\vert k} = \\hat P_{k\\vert k-1}-K_kC\\hat P_{k\\vert k-1} = (\\hat P_{k\\vert k-1}^{-1}+C^\\top V_k^{-1}C)^{-1} \\end{cases} \\] \\[ P=A P A^\\top + W - A P C^\\top (C P C^\\top + V)^{-1} C P A^\\top \\] <p>Duality:</p> \\[ A \\leftrightarrow A^\\top, \\quad B \\leftrightarrow C^\\top, \\quad Q \\leftrightarrow W, \\quad R \\leftrightarrow V, \\quad S \\leftrightarrow P \\]","tags":["bilibili","control"]},{"location":"2025/12/04/the-duality-and-the-failure-of-lqg-control/#the-failure","title":"The Failure","text":"","tags":["bilibili","control"]},{"location":"2025/12/04/the-duality-and-the-failure-of-lqg-control/#the-paradox-of-optimality","title":"The Paradox of Optimality","text":"<p>LQR Robustness (SISO systems):</p> <ul> <li>\\(\\geq 60 \\deg\\) Phase Margin</li> <li>\\(\\geq 6 \\text{ dB}\\) Gain Margin</li> <li>Infinite gain reduction margin</li> </ul> <p>Kalman Filter Robustness:</p> <ul> <li>Dual robustness properties at sensor output</li> <li>Excellent margins against sensor errors</li> </ul> <p></p>","tags":["bilibili","control"]},{"location":"2025/12/04/the-duality-and-the-failure-of-lqg-control/#the-fundamental-trade-off","title":"The Fundamental Trade-Off","text":"<p>LQR's Need for High-Gain Feedback:</p> <ul> <li>Large Q &amp; Small R</li> <li>Excellent stability margins</li> </ul> <p>KF's Need for High-Gain Feedback:</p> <ul> <li>Large W &amp; Small V</li> <li>Prompt response to new measurements</li> </ul> <p>Optimizing for individual robustness leads to a fragile combined LQG system.</p> <p>The Destructive Feedback Loop:</p> <ol> <li>High-gain L reacts aggressively to state deviations</li> <li>High-gain K amplifies sensor noise</li> <li>This creates a positive feedback loop</li> <li>Resulting in potential instability of the system</li> </ol> <p>No stability guarantee for imperfect models, leading to the development of \\(H_\\infty\\) Control</p>","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/","title":"Linear Quadratic Regulator in 3 Ways","text":"","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/#linear-quadratic-regulator-in-3-ways","title":"Linear Quadratic Regulator in 3 Ways","text":"<p>Slide: Explore the Linear Quadratic Regulator (LQR) through the Indirect Shooting Method (Pontryagin's Principle), the Optimization Approach (Quadratic Programming), and the Recursive Solution (Riccati Equation). Inspired by Zachary Manchester's Spring 2024-25 lecture \"Optimal Control and Reinforcement Learning\".</p> <p></p>","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/#introduction","title":"Introduction","text":"","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/#problem-formulation","title":"Problem Formulation","text":"<p>Consider a discrete-time linear system:</p> \\[ x_{n+1} = A_nx_n+B_nu_n \\] <p>Quadratic cost function:</p> \\[ \\min_{x_{1:N},u_{1:N-1}}J= \\sum_{n=1}^{N-1}\\underbrace{\\left[ \\frac12x_n^\\top Q_nx_n+\\frac12u_n^\\top R_nu_n\\right]}_\\text{running cost}  + \\underbrace{\\frac12x_N^\\top Q_Nx_N}_\\text{terminal cost} \\] <p>Assumptions:</p> <ul> <li>\\((A_n,B_n)\\) is controllable and \\((A_n,C_n)\\) is observable</li> <li>\\(Q_n\\succeq0,R_n\\succeq0,Q_N\\succeq0\\)</li> </ul>","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/#indirect-shooting-pmp-perspective","title":"Indirect Shooting: PMP Perspective","text":"","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/#problem-formulation-and-optimality-conditions","title":"Problem Formulation and Optimality Conditions","text":"<p>Consider the deterministic discrete-time optimal control problem:</p> \\[ \\begin{aligned} \\min_{x_{1:N},u_{1:N-1}} &amp; \\sum_{n=1}^{N-1} l(x_n,u_n) + l_F(x_N) \\\\ \\text{s.t. }\\ &amp; x_{n+1} = f(x_n,u_n) \\\\ &amp; u_n \\in\\mathcal{U} \\end{aligned} \\] <p>The first-order necessary conditions for optimality can be derived using:</p> <ul> <li>The Lagrangian framework (special case of KKT conditions)</li> <li>Pontryagin's Minimum Principle (PMP)</li> </ul>","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/#lagrangian-formulation","title":"Lagrangian Formulation","text":"<p>Form the Lagrangian:</p> \\[ L = \\sum_{n=1}^{N-1} l(x_n,u_n) + \\lambda_{n+1}^\\top (f(x_n,u_n) - x_{n+1}) + l_F(x_N) \\] <p>Define the Hamiltonian:</p> \\[ H(x_n,u_n,\\lambda_{n+1}) = l(x_n,u_n) + \\lambda_{n+1}^\\top f(x_n,u_n) \\] <p>Rewrite the Lagrangian using the Hamiltonian:</p> \\[ L = H(x_1,u_1,\\lambda_2) + \\left[ \\sum_{n=2}^{N-1} H(x_n,u_n,\\lambda_{n+1}) - \\lambda_n^\\top x_n \\right] + l_F(x_N) - \\lambda_N^\\top x_N \\]","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/#optimality-conditions","title":"Optimality Conditions","text":"<p>Take derivatives with respect to \\(x\\) and \\(\\lambda\\):</p> \\[ \\begin{aligned} \\frac{\\partial L}{\\partial \\lambda_n} &amp;= \\frac{\\partial H}{\\partial \\lambda_n} - x_{n+1} = f(x_n,u_n) - x_{n+1} = 0 \\\\ \\frac{\\partial L}{\\partial x_n} &amp;= \\frac{\\partial H}{\\partial x_n} - \\lambda_n^\\top = \\frac{\\partial l}{\\partial x_n} + \\lambda_{n+1}^\\top \\frac{\\partial f}{\\partial x_n} - \\lambda_n^\\top = 0 \\\\ \\frac{\\partial L}{\\partial x_N} &amp;= \\frac{\\partial l_F}{\\partial x_N} - \\lambda_N^\\top = 0 \\end{aligned} \\] <p>For \\(u\\), we write the minimization explicitly to handle constraints:</p> \\[ \\begin{aligned} u_n = &amp; \\arg\\min_u H(x_n,u,\\lambda_{n+1}) \\\\ &amp; \\text{s.t. } u \\in \\mathcal{U} \\end{aligned} \\]","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/#summary-of-necessary-conditions","title":"Summary of Necessary Conditions","text":"<p>The first-order necessary conditions can be summarized as:</p> \\[ \\begin{aligned} x_{n+1} &amp;= \\nabla_\\lambda H(x_n,u_n,\\lambda_{n+1}) \\\\ \\lambda_n &amp;= \\nabla_x H(x_n,u_n,\\lambda_{n+1}) \\\\ u_n &amp;= \\arg\\min_u H(x_n,u,\\lambda_{n+1}),\\quad \\text{s.t. } u \\in \\mathcal{U} \\\\ \\lambda_N &amp;= \\frac{\\partial l_F}{\\partial x_N} \\end{aligned} \\] <p>In continuous time, these become:</p> \\[ \\begin{aligned} \\dot{x} &amp;= \\nabla_\\lambda H(x,u,\\lambda) \\\\ -\\dot{\\lambda} &amp;= \\nabla_x H(x,u,\\lambda) \\\\ u &amp;= \\arg\\min_{\\tilde{u}} H(x,\\tilde{u},\\lambda),\\quad \\text{s.t. } \\tilde{u} \\in \\mathcal{U} \\\\ \\lambda(t_F) &amp;= \\frac{\\partial l_F}{\\partial x} \\end{aligned} \\]","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/#application-to-lqr-problems","title":"Application to LQR Problems","text":"<p>For LQR problems with quadratic cost and linear dynamics:</p> \\[ \\begin{aligned} l(x_n,u_n) &amp;= \\frac{1}{2}(x_n^\\top Q_n x_n + u_n^\\top R_n u_n) \\\\ l_F(x_N) &amp;= \\frac{1}{2}x_N^\\top Q_N x_N \\\\ f(x_n,u_n) &amp;= A_n x_n + B_n u_n \\end{aligned} \\] <p>The necessary conditions simplify to:</p> \\[ \\begin{aligned} x_{n+1} &amp;= A_n x_n + B_n u_n \\\\ \\lambda_n &amp;= Q_n x_n + A_n^\\top \\lambda_{n+1} \\\\ \\lambda_N &amp;= Q_N x_N \\\\ u_n &amp;= -R_n^{-1} B_n^\\top \\lambda_{n+1} \\end{aligned} \\] <p>This forms a linear two-point boundary value problem.</p>","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/#indirect-shooting-algorithm-for-lqr","title":"Indirect Shooting Algorithm for LQR","text":"<p>Procedure:</p> <ol> <li>Make initial guess for control sequence \\(u_{1:N-1}\\)</li> <li>Forward pass: Simulate dynamics to get state trajectory \\(x_{1:N}\\)</li> <li>Backward pass:</li> <li>Set terminal costate: \\(\\lambda_N = Q_N x_N\\)</li> <li>Compute costate trajectory: \\(\\lambda_n = Q_n x_n + A_n^\\top \\lambda_{n+1}\\)</li> <li>Compute control adjustment: \\(\\Delta u_n = -R_n^{-1} B_n^\\top \\lambda_{n+1} - u_n\\)</li> <li>Line search: Update controls \\(u_n \\leftarrow u_n + \\alpha \\Delta u_n\\)</li> <li>Iterate until convergence</li> </ol>","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/#direct-approach-qp-perspective","title":"Direct Approach: QP Perspective","text":"","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/#lqr-as-quadratic-programming-problem","title":"LQR as Quadratic Programming Problem","text":"<p>Assume \\(x_1\\) is given, define the decision variable vector and the block-diagonal matrix:</p> \\[ z=\\begin{bmatrix}u_1\\\\x_2\\\\u_2\\\\\\vdots\\\\x_N\\end{bmatrix},\\qquad H=\\begin{bmatrix}R_1\\\\&amp;Q_2\\\\&amp;&amp;R_2\\\\&amp;&amp;&amp;\\ddots\\\\&amp;&amp;&amp;&amp;Q_N\\end{bmatrix} \\] <p>The dynamics constraints can be expressed as</p> \\[ \\underbrace{\\begin{bmatrix} B_1 &amp; -I \\\\ &amp; A_2 &amp; B_2 &amp; -I \\\\ &amp; &amp; &amp; \\ddots \\\\ &amp; &amp; &amp; A_{N-1} &amp; B_{N-1} &amp; -I \\end{bmatrix}}_{C}\\begin{bmatrix}u_1\\\\x_2\\\\\\vdots\\\\x_N\\end{bmatrix} = \\underbrace{\\begin{bmatrix} -A_1x_1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}}_{d} \\]","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/#qp-formulation-and-kkt-conditions","title":"QP Formulation and KKT Conditions","text":"<p>The LQR problem becomes the QP:</p> \\[ \\min_zJ=\\frac12z^\\top Hz\\quad\\text{subject to}\\quad Cz=d \\] <p>The Lagrangian of this QP is:</p> \\[ \\mathcal{L}(z,\\lambda)=\\frac12z^\\top Hz+\\lambda^\\top(Cz-d) \\] <p>The KKT conditions are:</p> \\[ \\begin{align*} \\nabla_z\\mathcal{L} &amp;= Hz + C^\\top\\lambda=0 \\\\ \\nabla_\\lambda\\mathcal{L} &amp;= Cz-d=0 \\end{align*} \\] <p>This leads to the linear system:</p> \\[ \\begin{bmatrix} H &amp; C^\\top \\\\ C &amp; 0 \\end{bmatrix} \\begin{bmatrix} z \\\\ \\lambda \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ d \\end{bmatrix} \\] <p>We get the exact solution by solving one linear system!</p>","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/#riccati-equation-solution","title":"Riccati Equation Solution","text":"","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/#kkt-system-structure-for-lqr","title":"KKT System Structure for LQR","text":"<p>The KKT system for LQR has a highly structured sparse form, consider an \\(N=4\\) case:</p> \\[ \\begin{bmatrix} R_1 &amp;  &amp;  &amp;  &amp;  &amp;  &amp; B_1^T \\\\ &amp; Q_2 &amp;  &amp;  &amp;  &amp;  &amp; I &amp; A_2^T \\\\ &amp;  &amp; R_2 &amp;  &amp;  &amp;  &amp;  &amp; B_2^T \\\\ &amp;  &amp;  &amp; \\green{Q_3} &amp;  &amp;  &amp; \\green{-I} &amp; \\green{A_3^T} \\\\ &amp;  &amp;  &amp;  &amp; \\red{R_3} &amp;  &amp;  &amp; \\red{B_3^T} \\\\ &amp;  &amp;  &amp;  &amp;  &amp; \\blue{Q_4} &amp;  &amp; \\blue{-I} \\\\ B_1 &amp; -I &amp;  &amp;  &amp;  &amp;  &amp;  \\\\ &amp; A_2 &amp; B_2 &amp; -I &amp;  &amp;  &amp;  \\\\ &amp;  &amp; A_3 &amp; B_3 &amp; -I &amp;  &amp; \\end{bmatrix} \\begin{bmatrix} u_1 \\\\ x_2 \\\\ u_2 \\\\ x_3 \\\\ u_3 \\\\ x_4 \\\\ \\lambda_2 \\\\ \\lambda_3 \\\\ \\lambda_4 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\green{0} \\\\ \\red{0} \\\\ \\blue{0} \\\\ -A_1x_1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\]","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/#deriving-the-riccati-recursion","title":"Deriving the Riccati Recursion","text":"<p>Start from the terminal condition (blue equation):</p> \\[ \\blue{Q_4 x_4 - \\lambda_4 = 0 \\Rightarrow \\lambda_4 = Q_4 x_4} \\] <p>Move to the previous equation (red equation):</p> \\[ \\red{R_3 u_3 + B_3^\\top \\lambda_4 = R_3 u_3 + B_3^\\top Q_4 x_4 = 0} \\] <p>Substitute \\(x_4 = A_3 x_3 + B_3 u_3\\):</p> \\[ R_3 u_3 + B_3^\\top Q_4 (A_3 x_3 + B_3 u_3) = 0 \\] <p>Solve for \\(u_3\\):</p> \\[ u_3 = -\\underbrace{(R_3 + B_3^\\top Q_4 B_3)^{-1} B_3^\\top Q_4 A_3}_{K_3} x_3 \\]","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/#deriving-the-riccati-recursion-contd","title":"Deriving the Riccati Recursion (Cont'd)","text":"<p>Now consider the green equation:</p> \\[ \\green{Q_3 x_3 - \\lambda_3 + A_3^\\top \\lambda_4 = 0} \\] <p>Substitute \\(\\lambda_4 = Q_4 x_4\\) and \\(x_4 = A_3 x_3 + B_3 u_3\\):</p> \\[ Q_3 x_3 - \\lambda_3 + A_3^\\top Q_4 (A_3 x_3 + B_3 u_3) = 0 \\] <p>Substitute \\(u_3 = -K_3 x_3\\):</p> \\[ Q_3 x_3 - \\lambda_3 + A_3^\\top Q_4 (A_3 x_3 - B_3 K_3 x_3) = 0 \\] <p>Solve for \\(\\lambda_3\\):</p> \\[ \\lambda_3 = \\underbrace{(Q_3 + A_3^\\top Q_4 (A_3 - B_3 K_3))}_{P_3} x_3 \\]","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/#riccati-recursion-formula","title":"Riccati Recursion Formula","text":"<p>We now have a recursive relationship. Generalizing:</p> \\[ \\begin{aligned} P_N &amp;= Q_N \\\\ K_k &amp;= (R_k + B_k^\\top P_{k+1} B_k)^{-1} B_k^\\top P_{k+1} A_k \\\\ P_k &amp;= Q_k + A_k^\\top P_{k+1} (A_k - B_k K_k) \\end{aligned} \\] <p>This is the celebrated Riccati equation.</p> <p>The solution process involves:</p> <ol> <li>A backward Riccati pass to compute \\(P_k\\) and \\(K_k\\) for \\(k = N-1, \\ldots, 1\\)</li> <li>A forward rollout to compute \\(x_{1:N}\\) and \\(u_{1:N-1}\\) using \\(u_k = -K_k x_k\\)</li> </ol>","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/#computational-complexity","title":"Computational Complexity","text":"<p>Naive QP Solution: Treats problem as one big least-squares.</p> <ul> <li>Computational cost: \\(\\mathbf{O[N^3(n+m)^3]}\\)</li> <li>Must be re-solved from scratch for any change.</li> </ul> <p>Riccati Recursion: Exploits the temporal structure.</p> <ul> <li>Computational cost: \\(\\mathbf{O[N(n+m)^3]}\\)</li> <li>Exponentially faster for long horizons (large \\(N\\)).</li> </ul> <p>The Riccati Solution is More Than Just Fast:</p> <ul> <li>It provides a ready-to-use feedback policy: \\(\\mathbf{u_k = -K_k x_k}\\)</li> <li>This policy is adaptive: optimal for any initial state \\(x_1\\), not just a single one.</li> <li>It enables real-time control by naturally rejecting disturbances.</li> <li>And it delivers the exact same optimal solution as the QP.</li> </ul>","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/#conclusion","title":"Conclusion","text":"","tags":["bilibili","control"]},{"location":"2025/12/04/linear-quadratic-regulator-in-3-ways/#summary","title":"Summary","text":"<p>Finite-Horizon Problems</p> <ul> <li>Use Riccati recursion backward in time</li> <li>Store gain matrices \\(K_n\\)</li> <li>Apply time-varying feedback</li> </ul> <p>Infinite-Horizon Problems</p> <ul> <li>Solve algebraic Riccati equation offline</li> <li>Use constant gain matrix \\(K_\\infty\\)</li> <li>Implement simple state feedback</li> <li>Algebraic Riccati Equation (ARE):   $$   P_\\infty=Q + A^\\top P_\\infty A - A^\\top P_\\infty B(R + B^\\top P_\\infty B)<sup>{-1}B</sup>\\top P_\\infty A   $$</li> </ul>","tags":["bilibili","control"]},{"location":"2025/12/04/rl-as-an-adaptive-optimal-control/","title":"RL as an Adaptive Optimal Control","text":"","tags":["bilibili","rl"]},{"location":"2025/12/04/rl-as-an-adaptive-optimal-control/#rl-as-an-adaptive-optimal-control","title":"RL as an Adaptive Optimal Control","text":"<p>Slide: Explore reinforcement learning (RL) by comparing the Markov Chain and the Markov Decision Process (MDP). Understand how RL functions as a direct adaptive optimal control method through the example of Q-Learning. Inspired by Sutton's paper \"Reinforcement Learning is Direct Adaptive Optimal Control\" and Pieter Abbeel's lecture \"Foundations of Deep RL\".</p> <p></p>","tags":["bilibili","rl"]},{"location":"2025/12/04/rl-as-an-adaptive-optimal-control/#recap","title":"Recap","text":"","tags":["bilibili","rl"]},{"location":"2025/12/04/rl-as-an-adaptive-optimal-control/#problem-formulation","title":"Problem Formulation","text":"<p>Consider the deterministic discrete-time optimal control problem:</p> \\[ \\begin{aligned} \\min_{x_{1:N},u_{1:N-1}} &amp; \\sum_{n=1}^{N-1} l(x_n,u_n) + l_F(x_N) \\\\ \\text{s.t. }\\ &amp; x_{n+1} = f(x_n,u_n) \\\\ &amp; u_n \\in\\mathcal{U} \\end{aligned} \\] <p>The first-order necessary conditions for optimality can be derived using:</p> <ul> <li>The Lagrangian framework (special case of KKT conditions)</li> <li>Pontryagin's Minimum Principle (PMP)</li> </ul>","tags":["bilibili","rl"]},{"location":"2025/12/04/rl-as-an-adaptive-optimal-control/#lagrangian-formulation","title":"Lagrangian Formulation","text":"<p>Form the Lagrangian:</p> \\[ L = \\sum_{n=1}^{N-1} l(x_n,u_n) + \\lambda_{n+1}^\\top (f(x_n,u_n) - x_{n+1}) + l_F(x_N) \\] <p>Define the Hamiltonian:</p> \\[ H(x_n,u_n,\\lambda_{n+1}) = l(x_n,u_n) + \\lambda_{n+1}^\\top f(x_n,u_n) \\] <p>Rewrite the Lagrangian using the Hamiltonian:</p> \\[ L = H(x_1,u_1,\\lambda_2) + \\left[ \\sum_{n=2}^{N-1} H(x_n,u_n,\\lambda_{n+1}) - \\lambda_n^\\top x_n \\right] + l_F(x_N) - \\lambda_N^\\top x_N \\]","tags":["bilibili","rl"]},{"location":"2025/12/04/rl-as-an-adaptive-optimal-control/#optimality-conditions","title":"Optimality Conditions","text":"<p>Take derivatives with respect to \\(x\\) and \\(\\lambda\\):</p> \\[ \\begin{aligned} \\frac{\\partial L}{\\partial \\lambda_n} &amp;= \\frac{\\partial H}{\\partial \\lambda_n} - x_{n+1} = f(x_n,u_n) - x_{n+1} = 0 \\\\ \\frac{\\partial L}{\\partial x_n} &amp;= \\frac{\\partial H}{\\partial x_n} - \\lambda_n^\\top = \\frac{\\partial l}{\\partial x_n} + \\lambda_{n+1}^\\top \\frac{\\partial f}{\\partial x_n} - \\lambda_n^\\top = 0 \\\\ \\frac{\\partial L}{\\partial x_N} &amp;= \\frac{\\partial l_F}{\\partial x_N} - \\lambda_N^\\top = 0 \\end{aligned} \\] <p>For \\(u\\), we write the minimization explicitly to handle constraints:</p> \\[ \\begin{aligned} u_n = &amp; \\arg\\min_u H(x_n,u,\\lambda_{n+1}) \\\\ &amp; \\text{s.t. } u \\in \\mathcal{U} \\end{aligned} \\]","tags":["bilibili","rl"]},{"location":"2025/12/04/rl-as-an-adaptive-optimal-control/#summary-of-necessary-conditions","title":"Summary of Necessary Conditions","text":"<p>The first-order necessary conditions can be summarized as:</p> \\[ \\begin{aligned} x_{n+1} &amp;= \\nabla_\\lambda H(x_n,u_n,\\lambda_{n+1}) \\\\ \\lambda_n &amp;= \\nabla_x H(x_n,u_n,\\lambda_{n+1}) \\\\ u_n &amp;= \\arg\\min_u H(x_n,u,\\lambda_{n+1}),\\quad \\text{s.t. } u \\in \\mathcal{U} \\\\ \\lambda_N &amp;= \\frac{\\partial l_F}{\\partial x_N} \\end{aligned} \\] <p>In continuous time, these become:</p> \\[ \\begin{aligned} \\dot{x} &amp;= \\nabla_\\lambda H(x,u,\\lambda) \\\\ -\\dot{\\lambda} &amp;= \\nabla_x H(x,u,\\lambda) \\\\ u &amp;= \\arg\\min_{\\tilde{u}} H(x,\\tilde{u},\\lambda),\\quad \\text{s.t. } \\tilde{u} \\in \\mathcal{U} \\\\ \\lambda(t_F) &amp;= \\frac{\\partial l_F}{\\partial x} \\end{aligned} \\]","tags":["bilibili","rl"]},{"location":"2025/12/04/rl-as-an-adaptive-optimal-control/#application-to-lqr-problems","title":"Application to LQR Problems","text":"<p>For LQR problems with quadratic cost and linear dynamics:</p> \\[ \\begin{aligned} l(x_n,u_n) &amp;= \\frac{1}{2}(x_n^\\top Q_n x_n + u_n^\\top R_n u_n) \\\\ l_F(x_N) &amp;= \\frac{1}{2}x_N^\\top Q_N x_N \\\\ f(x_n,u_n) &amp;= A_n x_n + B_n u_n \\end{aligned} \\] <p>The necessary conditions simplify to:</p> \\[ \\begin{aligned} x_{n+1} &amp;= A_n x_n + B_n u_n \\\\ \\lambda_n &amp;= Q_n x_n + A_n^\\top \\lambda_{n+1} \\\\ \\lambda_N &amp;= Q_N x_N \\\\ u_n &amp;= -R_n^{-1} B_n^\\top \\lambda_{n+1} \\end{aligned} \\] <p>This forms a linear two-point boundary value problem.</p>","tags":["bilibili","rl"]},{"location":"2025/12/04/rl-as-an-adaptive-optimal-control/#mdp-rl","title":"MDP &amp; RL","text":"","tags":["bilibili","rl"]},{"location":"2025/12/04/rl-as-an-adaptive-optimal-control/#bridging-optimal-control-and-rl","title":"Bridging Optimal Control and RL","text":"<p>Markov Chains</p> <ul> <li>State space \\(\\mathcal{X}\\)</li> <li>Action space \\(\\mathcal{U}\\)</li> <li>System dynamics \\(f(x_n,u_n)\\)</li> <li>Cost function \\(l(x,u)\\) and \\(l_F(x)\\)</li> </ul> <p>Find feedback \\(\\boldsymbol{u=K(x)}\\) to minimize \\(J(x_0,u)\\)</p> \\[ J(x_0,u) = \\mathbb{E}\\left[\\sum_{n=0}^{N-1} l(x_n,u_n) + l_F(x_N)\\right] \\] <p>subject to \\(x_{n+1} \\sim f(x_n,u_n)\\).</p> <p>Markov (Decision) Process</p> <ul> <li>State space \\(\\mathcal{S}\\)</li> <li>Action space \\(\\mathcal{A}\\)</li> <li>Transition dynamics \\(P(s'|s,a)\\)</li> <li>Reward function \\(R(s,a,s')\\)</li> </ul> <p>Find policy \\(\\boldsymbol{\\pi(a|s)}\\) to maximize \\(V(s_0,\\pi)\\)</p> \\[ V(s_0,\\pi) = \\mathbb{E}\\left[\\sum_{n=0}^{H} \\gamma^n R(s_n,a_n,s_{n+1})\\right] \\] <p>subject to \\(s_{n+1} \\sim P(\\cdot|s_n,a_n)\\), \\(a_n \\sim \\pi(\\cdot|s_n)\\), where \\(\\gamma\\in[0,1)\\) is the discount factor.</p> <p>RL is an adaptive method to solve MDP in the absence of model knowledge.</p>","tags":["bilibili","rl"]},{"location":"2025/12/04/rl-as-an-adaptive-optimal-control/#value-function-and-action-value-function","title":"Value Function and Action-Value Function","text":"<p>Optimal Control:</p> <ul> <li>Value Function:   $$   V(x) = \\min_u \\mathbb{E}\\left[\\sum_{n=0}^{N-1} l(x_n,u_n) + l_F(x_N) \\;\\middle|\\; x_0 = x\\right]   $$</li> <li>Action-Value Function:   $$   Q(x,u) = \\mathbb{E}\\left[l(x,u) + V(x')\\middle| x'\\sim f(x,u)\\right]   $$</li> </ul> <p>Reinforcement Learning:</p> <ul> <li>Value Function:   $$   V<sup>\\pi(s)=\\mathbb{E}\\left[\\sum_{n=0}</sup>) \\;\\middle|\\; s_0=s, a_n\\sim \\pi(\\cdot|s_n)\\right]   $$} \\gamma^n R(s_n,a_n,s_{n+1</li> <li>Action-Value Function:   $$   Q^\\pi(s,a)=\\mathbb{E}\\left[R(s,a,s') + \\gamma V^\\pi(s') \\;\\middle|\\; s'\\sim P(\\cdot|s,a)\\right]   $$</li> </ul>","tags":["bilibili","rl"]},{"location":"2025/12/04/rl-as-an-adaptive-optimal-control/#q-learning","title":"Q-Learning","text":"","tags":["bilibili","rl"]},{"location":"2025/12/04/rl-as-an-adaptive-optimal-control/#the-scalability-challenge","title":"The Scalability Challenge","text":"<p>For discrete, low-dimensional problems with a known model, Optimal Control and Model-based RL can be solved exactly using Dynamic Programming (DP). But what if...</p> <ul> <li>The model \\(f(x,u)\\), \\(l(x,u)\\) or \\(P(s'|s,a), R(s,a,s')\\) is unknown?</li> <li>The state or action space is too large or continuous making DP loops intractable?</li> <li>The system is too complex to model accurately?</li> </ul> <p>We need model-free, stochastic, and approximate methods.</p>","tags":["bilibili","rl"]},{"location":"2025/12/04/rl-as-an-adaptive-optimal-control/#tabular-q-learning","title":"(Tabular) Q-Learning","text":"<p>(Tabular) Q-Learning replace expectation by samples:</p> <ul> <li>For an state-action pair \\((s,a)\\) , receive \\(s'\\sim P(s'|s,a)\\)</li> <li>Consider old estimate \\(Q_k(s,a)\\)</li> <li>Consider new sample estimate:   \\(\\text{target}(s')=R(s,a,s')+\\gamma\\max_{a'}Q_k(s',a')\\)</li> <li>Incorporate the new estimate into a running average:   $$   Q_{k+1}(s,a)\\leftarrow(1-\\alpha)Q_k(s,a)+\\alpha[\\text{target}(s')]   $$</li> </ul> <p>Q-learning converges to optimal policy even if you're acting suboptimally and is called off-policy learning. Requires sufficient exploration and a learning rate \\(\\alpha\\) that decays appropriately:</p> \\[ \\sum_{t=0}^\\infty\\alpha_t(s,a)=\\infty\\quad\\sum_{t=0}^\\infty\\alpha_t^2(s,a)&lt;\\infty \\]","tags":["bilibili","rl"]},{"location":"2025/12/04/rl-as-an-adaptive-optimal-control/#approximate-q-learning","title":"Approximate Q-Learning","text":"<p>Instead of a table, we use a parametrized Q function \\(Q_\\theta(s,a)\\) to approximate:</p> <ul> <li>Learning rule:   $$   \\begin{align}   \\text{target}(s') &amp;= R(s,a,s')+\\gamma\\max_{a'}Q_{\\theta_k}(s',a') \\   \\theta_{k+1} &amp;\\leftarrow \\theta_k - \\alpha \\nabla_\\theta \\left[ \\frac{1}{2}(Q_\\theta(s,a) - \\text{target}(s'))^2 \\right] \\Big|_{\\theta=\\theta_k}   \\end{align}   $$</li> <li>Practical details:</li> <li>Use Huber loss instead of squared loss on Bellman error:     $$     L_\\delta(a) = \\begin{cases}     \\frac{1}{2}a^2 &amp; \\text{for } |a| \\leq \\delta \\     \\delta(|a| - \\frac{1}{2}\\delta) &amp; \\text{otherwise}     \\end{cases}     $$</li> <li>Use RMSProp instead of vanilla SGD.</li> <li>It is beneficial to anneal the exploration rate over time.</li> </ul>","tags":["bilibili","rl"]},{"location":"2025/12/04/rl-as-an-adaptive-optimal-control/#conclusion","title":"Conclusion","text":"","tags":["bilibili","rl"]},{"location":"2025/12/04/rl-as-an-adaptive-optimal-control/#rl-as-an-adaptive-optimal-control_1","title":"RL as an Adaptive Optimal Control","text":"<p>Common Core:</p> <ul> <li>Value Function</li> <li>Bellman Equation</li> <li>Sequential Decision Making</li> </ul> <p>The Evolution:</p> <ul> <li>Expectation \\(\\rightarrow\\) Samples</li> <li>Table \\(\\rightarrow\\) Function Approximation</li> <li>Exact Solution \\(\\rightarrow\\) Stochastic Optimization</li> </ul> <p>A powerful and adaptive optimal control framework.</p>","tags":["bilibili","rl"]},{"location":"2020/12/20/congrats-to-8011-for-winning-_championship_-at-2020-we-robostar-league/","title":"Congrats to 8011 for winning Championship at 2020 WE RoboStar League","text":"","tags":["FRC"]},{"location":"2020/12/20/congrats-to-8011-for-winning-_championship_-at-2020-we-robostar-league/#congrats-to-8011-for-winning-championship-at-2020-we-robostar-league","title":"Congrats to 8011 for winning Championship at 2020 WE RoboStar League","text":"","tags":["FRC"]},{"location":"2021/05/30/congrats-to-8011-for-winning-_rookie-game-changer-award_-at-2021-infinite-recharge-at-home-challenge/","title":"Congrats to 8011 for winning Rookie Game Changer Award at 2021 INFINITE RECHARGE At Home Challenge","text":"","tags":["FRC"]},{"location":"2021/05/30/congrats-to-8011-for-winning-_rookie-game-changer-award_-at-2021-infinite-recharge-at-home-challenge/#congrats-to-8011-for-winning-rookie-game-changer-award-at-2021-infinite-recharge-at-home-challenge","title":"Congrats to 8011 for winning Rookie Game Changer Award at 2021 INFINITE RECHARGE At Home Challenge","text":"","tags":["FRC"]},{"location":"2023/08/04/congrats-to-8811-for-winning-_3rd-prize_-at-2023-indiemicro-robotics-competition-exchange-event/","title":"Congrats to 8811 for winning 3rd Prize at 2023 Indiemicro Robotics Competition Exchange Event","text":"","tags":["FRC"]},{"location":"2023/08/04/congrats-to-8811-for-winning-_3rd-prize_-at-2023-indiemicro-robotics-competition-exchange-event/#congrats-to-8811-for-winning-3rd-prize-at-2023-indiemicro-robotics-competition-exchange-event","title":"Congrats to 8811 for winning 3rd Prize at 2023 Indiemicro Robotics Competition Exchange Event","text":"<p>8811 is a student-led team organized by students from Guangzhou No. 2 High School, with all members and mentors currently enrolled in middle or high school.</p> <p></p> <p></p> <p></p> <p></p>","tags":["FRC"]},{"location":"2024/08/23/congrats-to-6399-for-winning-_engineering-inspiration-award_-at-2024-world-robot-contest-championships-beijing/","title":"Congrats to 6399 for winning Engineering Inspiration Award at 2024 World Robot Contest Championships Beijing","text":"","tags":["FRC"]},{"location":"2024/08/23/congrats-to-6399-for-winning-_engineering-inspiration-award_-at-2024-world-robot-contest-championships-beijing/#congrats-to-6399-for-winning-engineering-inspiration-award-at-2024-world-robot-contest-championships-beijing","title":"Congrats to 6399 for winning Engineering Inspiration Award at 2024 World Robot Contest Championships Beijing","text":"","tags":["FRC"]},{"location":"2025/04/20/congrats-to-both-9635-and-8214-for-_advancing-to-2025-first-championship_/","title":"Congrats to both 9635 and 8214 for advancing to 2025 FIRST Championship","text":"","tags":["FRC"]},{"location":"2025/04/20/congrats-to-both-9635-and-8214-for-_advancing-to-2025-first-championship_/#congrats-to-both-9635-and-8214-for-advancing-to-2025-first-championship","title":"Congrats to both 9635 and 8214 for advancing to 2025 FIRST Championship","text":"","tags":["FRC"]},{"location":"2025/04/24/congrats-to-8214-for-being-_division-finalists-captain_-and-winning-_industrial-design-award_-at-2025-galileo-division/","title":"Congrats to 8214 for being Division Finalists Captain and winning Industrial Design Award at 2025 Galileo Division","text":"","tags":["FRC"]},{"location":"2025/04/24/congrats-to-8214-for-being-_division-finalists-captain_-and-winning-_industrial-design-award_-at-2025-galileo-division/#congrats-to-8214-for-being-division-finalists-captain-and-winning-industrial-design-award-at-2025-galileo-division","title":"Congrats to 8214 for being Division Finalists Captain and winning Industrial Design Award at 2025 Galileo Division","text":"","tags":["FRC"]},{"location":"archive/2025/","title":"2025","text":""},{"location":"archive/2025/#2025","title":"2025","text":""},{"location":"archive/2024/","title":"2024","text":""},{"location":"archive/2024/#2024","title":"2024","text":""},{"location":"archive/2023/","title":"2023","text":""},{"location":"archive/2023/#2023","title":"2023","text":""},{"location":"archive/2021/","title":"2021","text":""},{"location":"archive/2021/#2021","title":"2021","text":""},{"location":"archive/2020/","title":"2020","text":""},{"location":"archive/2020/#2020","title":"2020","text":""},{"location":"category/news/","title":"News","text":""},{"location":"category/news/#news","title":"News","text":""},{"location":"category/blog/","title":"Blog","text":""},{"location":"category/blog/#blog","title":"Blog","text":""},{"location":"category/paper/","title":"Paper","text":""},{"location":"category/paper/#paper","title":"Paper","text":""},{"location":"category/notes/","title":"Notes","text":""},{"location":"category/notes/#notes","title":"Notes","text":""},{"location":"page/2/","title":"Posts","text":""},{"location":"page/2/#posts","title":"Posts","text":""},{"location":"page/3/","title":"Posts","text":""},{"location":"page/3/#posts","title":"Posts","text":""},{"location":"page/4/","title":"Posts","text":""},{"location":"page/4/#posts","title":"Posts","text":""},{"location":"archive/2025/page/2/","title":"2025","text":""},{"location":"archive/2025/page/2/#2025","title":"2025","text":""},{"location":"archive/2025/page/3/","title":"2025","text":""},{"location":"archive/2025/page/3/#2025","title":"2025","text":""},{"location":"category/blog/page/2/","title":"Blog","text":""},{"location":"category/blog/page/2/#blog","title":"Blog","text":""}]}